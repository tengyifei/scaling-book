<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Sharded Matrices and How to Multiply Them | How To Scale Your Model </title> <meta name="author" content=" "> <meta name="description" content="Here we'll explain how sharding works, how TPUs communicate with each other (emphasizing 4 core communication primitives) and how communication is performed by our hardware."> <meta name="keywords" content="scaling, jax, llms, transformers, tpus, google, deepmind, parallelism, pallas"> <link rel="stylesheet" href="/scaling-book/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/scaling-book/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/scaling-book/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/scaling-book/assets/css/jekyll-pygments-themes-vs.css?4ee1a2facd1a8a76347f4bd43a740500" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/scaling-book/assets/img/favicon.png?fddbd8c2ec231ba2060e67c85de32a55"> <link rel="stylesheet" href="/scaling-book/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jax-ml.github.io/scaling-book/sharding/"> <script src="/scaling-book/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/scaling-book/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/scaling-book/assets/js/distillpub/template.v2.js"></script> <script src="/scaling-book/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">{{page._styles}}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Sharded Matrices and How to Multiply Them",
            "description": "Here we'll explain how sharding works, how TPUs communicate with each other (emphasizing 4 core communication primitives) and how communication is performed by our hardware.",
            "published": "February 04, 2025",
            "authors": [
              
              {
                "author": "Jacob Austin",
                "authorURL": "https://www.jacobaustin.org/",
                "affiliations": [
                  {
                    "name": "Google DeepMind",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Sholto Douglas",
                "authorURL": "https://x.com/_sholtodouglas",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Roy Frostig",
                "authorURL": "https://cs.stanford.edu/~rfrostig/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Anselm Levskaya",
                "authorURL": "https://anselmlevskaya.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Charlie Chen",
                "authorURL": "https://x.com/charliexychen",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Sharad Vikram",
                "authorURL": "https://sharadvikram.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Federico Lebron",
                "authorURL": "https://fedelebron.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Peter Choy",
                "authorURL": "https://x.com/pchoy95",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Vinay Ramasesh",
                "authorURL": "https://x.com/vinayramasesh",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Albert Webson",
                "authorURL": "https://representation.ai/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Reiner Pope<sup>*</sup>",
                "authorURL": "https://x.com/reinerpope",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <script>
    function goToTop() {
      document.body.scrollTop = 0; // For Safari
      document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
    }

    // When the user scrolls down 20px from the top of the document, show the button
    window.onscroll = function() {scrollFunction()};

    function scrollFunction() {
      // Get the button:
      let mybutton = document.getElementById("top-button");

      if (document.body.scrollTop > 40 || document.documentElement.scrollTop > 40) {
        mybutton.style.display = "block";
      } else {
        mybutton.style.display = "none";
      }
  }
  </script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/scaling-book"> How To Scale Your Model </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="left-button section-button"><a href="../tpus"><svg viewbox="-78.5 0 512 512"><path d="M257 64L291 98 128 262 291 426 257 460 61 262 257 64Z"></path></svg></a></div> <div class="right-button section-button"><a href="../transformers"><svg viewbox="-78.5 0 512 512"><path d="M98 460L64 426 227 262 64 98 98 64 294 262 98 460Z"></path></svg></a></div> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/scaling-book/"> </a> </li> <li class="nav-item nav-hidden"><a class="nav-link" onclick="goToTop()" id="top-button" style="display: none;">Back to Top</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../tpus">Previous Part</a></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../transformers">Next Part</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Sections </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/scaling-book/index">Part 0. Introduction</a> <a class="dropdown-item " href="/scaling-book/roofline">Part 1. Intro to Rooflines</a> <a class="dropdown-item " href="/scaling-book/tpus">Part 2. All About TPUs</a> <a class="dropdown-item " href="/scaling-book/sharding">Part 3. Sharded Matmuls</a> <a class="dropdown-item " href="/scaling-book/transformers">Part 4. Transformers</a> <a class="dropdown-item " href="/scaling-book/training">Part 5. Training</a> <a class="dropdown-item " href="/scaling-book/applied-training">Part 6. Training LLaMA</a> <a class="dropdown-item " href="/scaling-book/inference">Part 7. Inference</a> <a class="dropdown-item " href="/scaling-book/applied-inference">Part 8. Serving LLaMA</a> <a class="dropdown-item " href="/scaling-book/profiling">Part 9. Profiling</a> <a class="dropdown-item " href="/scaling-book/jax-stuff">Part 10. All About JAX</a> <a class="dropdown-item " href="/scaling-book/conclusion">Part 11. Conclusions</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Sharded Matrices and How to Multiply Them</h1> <p>Part 3 of <a href="/scaling-book">How To Scale Your Model</a> (<a href="../tpus">Part 2: TPUs</a> | <a href="../transformers">Part 4: Transformer Math</a>)</p> <p>Here we'll explain how sharding works, how TPUs communicate with each other (emphasizing 4 core communication primitives) and how communication is performed by our hardware.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#partitioning-notation-and-collective-operations">Partitioning Notation and Collective Operations</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#a-unified-notation-for-sharding">A unified notation for sharding</a> </li> <li> <a href="#a-quick-aside-how-would-we-describe-this-in-code">A quick aside: how would we describe this in code?</a> </li> </ul> <div> <a href="#computation-with-sharded-arrays">Computation With Sharded Arrays</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#case-1-neither-multiplicand-has-a-sharded-contracting-dimension">Case 1: neither multiplicand has a sharded contracting dimension</a> </li> <li> <a href="#case-2-one-multiplicand-has-a-sharded-contracting-dimension">Case 2: one multiplicand has a sharded contracting dimension</a> </li> <li> <a href="#case-3-both-multiplicands-have-sharded-contracting-dimensions">Case 3: both multiplicands have sharded contracting dimensions</a> </li> <li> <a href="#case-4-both-multiplicands-have-a-non-contracting-dimension-sharded-along-the-same-axis">Case 4: both multiplicands have a non-contracting dimension sharded along the same axis</a> </li> </ul> <div> <a href="#a-deeper-dive-into-tpu-communicaton-primitives">A Deeper Dive into TPU Communicaton Primitives</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#our-final-communication-primitive-the-alltoall">Our final communication primitive: the AllToAll</a> </li> <li> <a href="#more-about-the-reducescatter">More about the ReduceScatter</a> </li> </ul> <div> <a href="#what-have-we-learned">What Have We Learned?</a> </div> <div> <a href="#some-problems-to-work">Some Problems to Work</a> </div> </nav> </d-contents> <h2 id="partitioning-notation-and-collective-operations">Partitioning Notation and Collective Operations</h2> <p>When we train an LLM on ten thousand TPUs, we’re still doing abstractly the same computation as when we’re training on one. The difference is that <strong>our arrays don’t fit in the HBM of a single TPU</strong>, so we have to split them up.<d-footnote>It's worth noting that we may also choose to parallelize for speed. Even if we could fit on a smaller number of chips, scaling to more simply gives us more FLOPs/s. During inference, for instance, we can sometimes fit on smaller topologies but choose to scale to larger ones in order to reduce latency. Likewise, during training we often scale to more chips to reduce the step time.</d-footnote> We call this “<em>sharding</em>” or “<em>partitioning</em>” our arrays.</p> <p>Here’s an example 2D array <strong>A</strong> sharded across 4 TPUs:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/sharding-example-480.webp 480w,/scaling-book/assets/img/sharding-example-800.webp 800w,/scaling-book/assets/img/sharding-example-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/sharding-example.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure:</b> an example array of shape <b>A</b>[I, J] gets sharded across 4 devices. Both dimensions are evenly sharded across 2 devices with a sharding <b>A</b>[I<sub>X</sub>, J<sub>Y</sub>]. Each TPU holds 1/4 of the total memory.</figcaption> </figure> <p>Note how the sharded array still has the same <em>global</em> or <em>logical shape</em> as unsharded array, say <code class="language-plaintext highlighter-rouge">(4, 128)</code>, but it also has a <em>device local shape</em>, like <code class="language-plaintext highlighter-rouge">(2, 64)</code>, which gives us the actual size in bytes that each TPU is holding (in the figure above, each TPU holds ¼ of the total array). Now we’ll generalize this to arbitrary arrays.</p> <h3 id="a-unified-notation-for-sharding">A unified notation for sharding</h3> <p>We use a variant of <em>named-axis notation</em> to describe <em>how</em> the tensor is sharded in blocks across the devices: we assume the existence of a 2D or 3D grid of devices called the <strong>device mesh</strong> where each axis has been given <strong>mesh axis names</strong> <strong>e.g. X</strong>, <strong>Y, and Z.</strong> We can then specify how the matrix data is laid out across the device mesh by describing how each named dimension of the array is partitioned across the physical mesh axes. We call this assignment a <strong>sharding</strong>.</p> <p><strong>Example (the diagram above)</strong>: For the above diagram, we have:</p> <ul> <li> <strong>Sharding:</strong> $A[I_X, J_Y]$, which tells us to shard the first axis, $I$, along the mesh axis $X$, and the second axis, $J$, along the mesh axis $Y$. This sharding tells us that each shard holds $1 / (\lvert X\rvert \cdot \lvert Y\rvert)$ of the array.</li> <li> <strong>Mesh:</strong> the device mesh above <code class="language-plaintext highlighter-rouge">Mesh(devices=((0, 1), (2, 3)), axis_names=(‘X', ‘Y'))</code>, which tells us we have 4 TPUs in a 2x2 grid, with axis names $X$ and $Y$.</li> </ul> <p>Taken together, we know that the local shape of the array (the size of the shard that an individual device holds) is $(\lvert I\rvert / 2, \lvert J\rvert / 2)$, where \(\lvert I\rvert\) is the size of A’s first dimension and \(\lvert J\rvert\) is the size of A’s second dimension.</p> <p><strong>Example (2D sharding across 1 axis)</strong>: $A[I_{XY}, J]$ shards the first dimension (I) along both the X and Y hardware axes. The number of bytes per device is the same as the previous sharding but the local shape is different. It is now $(\lvert I\rvert /(\lvert X\rvert \cdot \lvert Y\rvert), \lvert J\rvert)$.</p> <p><strong>Visualizing these shardings:</strong> Let’s try to visualize these shardings by looking at a 2D array of data split over 4 devices:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/sharding-colored1-480.webp 480w,/scaling-book/assets/img/sharding-colored1-800.webp 800w,/scaling-book/assets/img/sharding-colored1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/sharding-colored1.png" class="img-fluid img-small" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>We write the <em>fully-replicated</em> form of the matrix simply as $A[I, J]$ with no sharding assignment. This means that <em>each</em> device contains a full copy of the entire matrix.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/sharding-colored2-480.webp 480w,/scaling-book/assets/img/sharding-colored2-800.webp 800w,/scaling-book/assets/img/sharding-colored2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/sharding-colored2.png" class="img-fluid img-small" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>When we wish to indicate that one of these dimensions has been partitioned across a mesh axis, then we indicate so using a mesh-axis subscript. For instance $A[I_X, J]$ would mean that the <strong>I</strong> logical axis has been partitioned across the <strong>X</strong> mesh dimension, but that the <strong>J</strong> dimension is <em>not</em> partitioned, and the blocks remain <em>partially-replicated</em> across the <strong>Y</strong> mesh axis.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/sharding-colored3-480.webp 480w,/scaling-book/assets/img/sharding-colored3-800.webp 800w,/scaling-book/assets/img/sharding-colored3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/sharding-colored3.png" class="img-fluid img-small" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>$A[I_X, J_Y]$ means that the <strong>I</strong> logical axis has been partitioned across the <strong>X</strong> mesh axis, and that the <strong>J</strong> dimension has been partitioned across the <strong>Y</strong> mesh axis.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/sharding-colored4-480.webp 480w,/scaling-book/assets/img/sharding-colored4-800.webp 800w,/scaling-book/assets/img/sharding-colored4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/sharding-colored4.png" class="img-fluid img-small" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>We illustrate the other possibilities in the figure below:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/sharding-colored5-480.webp 480w,/scaling-book/assets/img/sharding-colored5-800.webp 800w,/scaling-book/assets/img/sharding-colored5-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/sharding-colored5.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Here $A[I_{XY}, J]$ means that we treat the <strong>X</strong> and <strong>Y</strong> mesh axes as a larger flattened dimension and partition the <strong>I</strong> named axis across all the devices. The order of the multiple mesh-axis subscripts matters, as it specifies the traversal order of the partitioning across the grid.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/sharding-colored6-480.webp 480w,/scaling-book/assets/img/sharding-colored6-800.webp 800w,/scaling-book/assets/img/sharding-colored6-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/sharding-colored6.png" class="img-fluid img-small" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Lastly, note that we <em>cannot</em> have multiple named axes sharded along the <em>same</em> mesh dimension. e.g. $A[I_X, J_X]$ is a nonsensical, forbidden sharding. Once a mesh dimension has been used to shard one dimension of an array, it is in a sense “spent”.</p> <p><b style="color: #57cf57;">Pop Quiz:</b> Let <strong>A</strong> be an array with shape <code class="language-plaintext highlighter-rouge">int8[128, 2048]</code>, sharding $A[I_{XY}, J]$, and mesh <code class="language-plaintext highlighter-rouge">Mesh({‘X': 2, ‘Y': 8, ‘Z': 2})</code> (so 32 devices total). How much memory does <strong>A</strong> use per device? How much total memory does <strong>A</strong> use across all devices?</p> <details><summary>Click here for the answer.</summary> <p><strong>Answer:</strong> Our array <strong>A</strong> is sharded over X and and Y and replicated over Z, so per device it has shape <code class="language-plaintext highlighter-rouge">int8[128 / (2 * 8), 2048] = int8[8, 2048]</code>, with size <code class="language-plaintext highlighter-rouge">8 * 2048 = 16,384</code> bytes. Because it’s replicated over Z, while within a Z-plane it’s fully sharded over X and Y, there’s one copy of it per Z-plane, and 2 such planes, so the total size (across all devices) is <code class="language-plaintext highlighter-rouge">128 * 2048 * 2 = 512kiB</code> total.</p> </details> <h3 id="a-quick-aside-how-would-we-describe-this-in-code">A quick aside: how would we describe this in code?</h3> <p>JAX uses a named sharding syntax that very closely matches the abstract syntax we describe above. We’ll talk more about this in <a href="../jax-stuff">Section 10</a>, but here’s a quick preview. You can play with this in a Google Colab <a href="https://colab.research.google.com/drive/15cxw66eABwZPG-V4QFmbLfiykPFf_gaP?usp=sharing" rel="external nofollow noopener" target="_blank">here</a> and profile the result to see how JAX handles different shardings. This snippet does 3 things:</p> <ol> <li>Creates a <strong>jax.Mesh</strong> that maps our 8 TPUs into a 4x2 grid with names ‘X’ and ‘Y’ assigned to the two axes.</li> <li>Creates matrices A and B where A is sharded along both its dimensions and B is sharded along the output dimension.</li> <li>Compiles and performs a simple matrix multiplication that returns a sharded array.</li> </ol> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">import</span> <span class="n">jax.sharding</span> <span class="k">as</span> <span class="n">shd</span>

<span class="c1"># Create our mesh! We're running on a TPU v2-8 4x2 slice with names 'X' and 'Y'.
</span><span class="k">assert</span> <span class="nf">len</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="nf">devices</span><span class="p">())</span> <span class="o">==</span> <span class="mi">8</span>
<span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">make_mesh</span><span class="p">(</span><span class="n">axis_shapes</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">axis_names</span><span class="o">=</span><span class="p">(</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">))</span>

<span class="c1"># A little utility function to help define our sharding. A PartitionSpec is our
# sharding (a mapping from axes to names).
</span><span class="k">def</span> <span class="nf">P</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">shd</span><span class="p">.</span><span class="nc">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">shd</span><span class="p">.</span><span class="nc">PartitionSpec</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">))</span>

<span class="c1"># We shard both A and B over the non-contracting dimension and A over the contracting dim.
</span><span class="n">A</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2048</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="nc">P</span><span class="p">(</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">))</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">2048</span><span class="p">,</span> <span class="mi">8192</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="nc">P</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">))</span>

<span class="c1"># We can perform a matmul on these sharded arrays! out_shardings tells us how we want
# the output to be sharded. JAX/XLA handles the rest of the sharding for us.
</span><span class="n">compiled</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">jit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">BD,DF-&gt;BF</span><span class="sh">'</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">),</span> <span class="n">out_shardings</span><span class="o">=</span><span class="nc">P</span><span class="p">(</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">)).</span><span class="nf">lower</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">).</span><span class="nf">compile</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="nf">compiled</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
</code></pre></div></div> <p>The cool thing about JAX is that these arrays behave as if they’re unsharded! <code class="language-plaintext highlighter-rouge">B.shape</code> will tell us the global or logical shape (2048, 8192). We have to actually look at <code class="language-plaintext highlighter-rouge">B.addressable_shards</code> to see how it’s locally sharded. We can perform operations on these arrays and JAX will attempt to figure out how to broadcast or reshape them to perform the operations. For instance, in the above example, the local shape of <strong>A</strong> is <code class="language-plaintext highlighter-rouge">[2, 1024]</code> and for <strong>B</strong> is <code class="language-plaintext highlighter-rouge">[2048, 4096]</code>. JAX/XLA will automatically add communication across these arrays as necessary to perform the final multiplication.</p> <h2 id="computation-with-sharded-arrays">Computation With Sharded Arrays</h2> <p>If you have an array of data that’s distributed across many devices and wish to perform mathematical operations on it, what are the overheads associated with sharding both the data and the computation?</p> <p>Obviously, this depends on the computation involved.</p> <ul> <li>For <em>elementwise</em> operations, there is <strong>no overhead</strong> for operating on a distributed array.</li> <li>When we wish to perform operations across elements resident on many devices, things get complicated. Thankfully, for most machine learning nearly all computation takes place in the form of matrix multiplications, and they are relatively simple to analyze.</li> </ul> <p>The rest of this section will deal with how to multiply sharded matrices. To a first approximation, this involves moving chunks of a matrix around so you can fully multiply or sum each chunk. <strong>Each sharding will involve different communication.</strong> For example, $A[I_X, J] \cdot B[J, K] \to C[I_X, K]$ can be multiplied without any communication because the <em>contracting dimension</em> (J, the one we’re actually summing over) is unsharded. However, if we wanted the output unsharded (i.e. $A[I_X, J] \cdot B[J, K] \to C[I_X, K]$), we would need to copy $A$ or $C$ to every device. These two choices have different communication costs, so we need to calculate this cost and pick the lowest one.</p> <details><summary>You can think of this in terms of “block matrix multiplcation”.</summary> <p>First let’s recall the concept of a “block matrix”, or a nested matrix of matrices:</p> \[\begin{equation} \begin{pmatrix} a_{00} &amp; a_{01} &amp; a_{02} &amp; a_{03} \\ a_{10} &amp; a_{11} &amp; a_{12} &amp; a_{13} \\ a_{20} &amp; a_{21} &amp; a_{22} &amp; a_{23} \\ a_{30} &amp; a_{31} &amp; a_{32} &amp; a_{33} \end{pmatrix} = \left( \begin{matrix} \begin{bmatrix} a_{00} &amp; a_{01} \\ a_{10} &amp; a_{11} \end{bmatrix} \\ \begin{bmatrix} a_{20} &amp; a_{21} \\ a_{30} &amp; a_{31} \end{bmatrix} \end{matrix} \begin{matrix} \begin{bmatrix} a_{02} &amp; a_{03} \\ a_{12} &amp; a_{13} \end{bmatrix} \\ \begin{bmatrix} a_{22} &amp; a_{23} \\ a_{32} &amp; a_{33} \end{bmatrix} \end{matrix} \right) = \begin{pmatrix} \mathbf{A_{00}} &amp; \mathbf{A_{01}} \\ \mathbf{A_{10}} &amp; \mathbf{A_{11}} \end{pmatrix} \end{equation}\] <p>Matrix multiplication has the nice property that when the matrix multiplicands are written in terms of blocks, the product can be written in terms of block matmuls following the standard rule:</p> \[\begin{equation} \begin{pmatrix} A_{00} &amp; A_{01} \\ A_{10} &amp; A_{11} \end{pmatrix} \cdot \begin{pmatrix} B_{00} &amp; B_{01} \\ B_{10} &amp; B_{11} \end{pmatrix} = \begin{pmatrix} A_{00}B_{00} + A_{01}B_{10} &amp; A_{00}B_{01} + A_{01}B_{11} \\ A_{10}B_{00} + A_{11}B_{10} &amp; A_{10}B_{01} + A_{11}B_{11} \end{pmatrix} \end{equation}\] <p>What this means is that implementing distributed matrix multiplications reduces down to moving these sharded blocks over the network, performing <em>local</em> matrix multiplications on the blocks, and summing their results. <strong>The question then is what communication to add, and how expensive it is.</strong></p> </details> <p>Conveniently, we can boil down all possible shardings into roughly 4 cases we need to consider, each of which has a rule for what communication we need to add</p> <ol> <li> <strong><a href="#case-1-neither-multiplicand-has-a-sharded-contracting-dimension">Case 1</a>:</strong> neither input is sharded along the contracting dimension. <em>We can multiply local shards without any communication.</em> </li> <li> <strong><a href="#case-2-one-multiplicand-has-a-sharded-contracting-dimension">Case 2</a>:</strong> one input has a sharded contracting dimension. <em>We typically “AllGather” the sharded input along the contracting dimension.</em> </li> <li> <strong><a href="#case-3-both-multiplicands-have-sharded-contracting-dimensions">Case 3</a>:</strong> both inputs are sharded along the contracting dimension. <em>We can multiply the local shards, then “AllReduce” the result.</em> </li> <li> <strong><a href="#case-4-both-multiplicands-have-a-non-contracting-dimension-sharded-along-the-same-axis">Case 4</a>:</strong> both inputs have a non-contracting dimension sharded along the same axis. We cannot proceed without AllGathering one of the two inputs first.</li> </ol> <p>You can think of these as rules that simply need to be followed, but it’s also valuable to understand why these rules hold and how expensive they are. We’ll go through each one of these in detail now.</p> <h3 id="case-1-neither-multiplicand-has-a-sharded-contracting-dimension">Case 1: neither multiplicand has a sharded contracting dimension</h3> <p><strong>Lemma:</strong> when multiplying partitioned tensors, the computation is valid and the output follows the sharding of the inputs <em>unless</em> the contracting dimension is sharded or both tensors have a non-contracting dimension sharded along the same axis. For example, this works fine</p> \[\begin{equation*} \mathbf{A}[I_X, J] \cdot \mathbf{B}[J, K_Y] \rightarrow \mathbf{C}[I_X, K_Y] \end{equation*}\] <p>with no communication whatsoever, and results in a tensor sharded across both the X and Y hardware dimensions. Try to think about why this is. Basically, the computation is <em>independent</em> of the sharding, since each batch entry has some local chunk of the axis being contracted that it can multiply and reduce. Any of these cases work fine and follow this rule:</p> \[\begin{align*} \mathbf{A}[I, J] \cdot \mathbf{B}[J, K] \rightarrow &amp;\ \mathbf{C}[I, K] \\ \mathbf{A}[I_X, J] \cdot \mathbf{B}[J, K] \rightarrow &amp;\ \mathbf{C}[I_X, K]\\ \mathbf{A}[I, J] \cdot \mathbf{B}[J, K_Y] \rightarrow &amp;\ \mathbf{C}[I, K_Y]\\ \mathbf{A}[I_X, J] \cdot \mathbf{B}[J, K_Y] \rightarrow &amp;\ \mathbf{C}[I_X, K_Y] \end{align*}\] <p>Because neither <strong>A</strong> nor <strong>B</strong> has a sharded contracting dimension <strong>J</strong>, we can simply perform the local block matrix multiplies of the inputs and the results will <em>already</em> be sharded according to the desired output shardings. When both multiplicands have non-contracting dimensions sharded along the same axis, this is no longer true (see the <a href="#case-4-both-multiplicands-have-a-non-contracting-dimension-sharded-along-the-same-axis">invalid shardings</a> section for details).</p> <h3 id="case-2-one-multiplicand-has-a-sharded-contracting-dimension">Case 2: one multiplicand has a sharded contracting dimension</h3> <p>Let us consider the simple case of the distributed matrix multiply of <strong>A</strong> sharded in the contracting <strong>J</strong> dimension against a fully replicated <strong>B:</strong></p> \[\mathbf{A}[I, J_X] \cdot \mathbf{B}[J, K] \rightarrow \mathbf{C}[I, K]\] <p>We cannot simply perform local matrix multiplies of the local <strong>A</strong>, <strong>B</strong> blocks against one another as we’re missing the full data from the contracting axis of <strong>A</strong>. Typically, we first “<strong>AllGather</strong>” the shards of <strong>A</strong> together locally, and only then multiply against <strong>B:</strong></p> \[\textbf{AllGather}_X[I, J_X] \rightarrow \mathbf{A}[I, J]\] \[\mathbf{A}[I, J] \cdot \mathbf{B}[J, K] \rightarrow \mathbf{C}[I, K]\] <p>AllGathers <em>remove sharding</em> along an axis and reassembles the shards spread across devices onto <em>each</em> device along that axis. Using the notation above, an AllGather removes a subscript from a set of axes, e.g.</p> \[\textbf{AllGather}_{XY}(A[I_{XY}, J]) \rightarrow A[I, J]\] <p>We also don’t have to remove all subscripts for a given dimension, e.g. \(A[I_{XY}, J] \rightarrow A[I_Y, J]\) is also an AllGather, just over only a single axis.</p> <p>Note that we may also wish to use an AllGather to remove <em>non-contracting</em> dimension sharding, for instance the matrix multiply:</p> \[A[I_X, J] \cdot B[J, K] \rightarrow C[I, K]\] <p>We would similarly AllGather along <strong>X</strong> to remove the output sharding, however in this case we have the freedom of doing so before or after the matrix multiply, unlike in the case of AllGathering the contracting dimension, where we are forced to do so before performing the matrix multiply.</p> <p><strong>How is an AllGather actually performed?</strong> To perform an AllGather along a single axis, we need to pass all the shards around the axis until every device has a copy. Figure 1 shows an example. Each of the 8 devices starts with 1 / 8th of the array and ends up with all copies. One efficient way to do this is to have each device pass its shard around the sharding dimension ring, either in one direction or both directions. If we do one direction, it takes \(N - 1\) hops of size \(\text{total size} / N\) per-link, otherwise we have $\lceil \frac{N}{2} \rceil$ hops of size \(2 \cdot \text{total size} / N\) per link.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/all-gather-480.webp 480w,/scaling-book/assets/img/all-gather-800.webp 800w,/scaling-book/assets/img/all-gather-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/all-gather.gif" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>How long does this take?</strong> Let’s take the bidirectional AllGather and calculate how long it takes. Let \(V\) be the number of bytes in the array, and \(\lvert X\rvert\) be the number of shards on the contracting dimension. Then from the above diagram, each hop sends $V / \lvert X\rvert$ bytes in each direction, so each hop takes</p> \[T_{hop} = \frac{2 \cdot V}{|X| \cdot W_{ICI}}\] <p>where \(W_\text{ICI}\) is the <strong>bidirectional</strong> ICI bandwidth. We need to send a total of $\lvert X\rvert / 2$ hops to reach every TPU<d-footnote>technically, $\lceil | X | / 2 \rceil$</d-footnote>, so the total reduction takes</p> \[T_{total} = \frac{2 \cdot V \cdot |X|}{2 \cdot |X| \cdot W_\text{ICI}}\] \[T_{total} = \frac{V}{W_\text{ICI}}\] <p>Note that this <strong>doesn’t depend on \(\lvert X\rvert\)!</strong> That’s kind of striking, because it means even though our TPUs are only locally connected, the locality of the connections doesn’t matter. We’re just bottlenecked by the speed of each link.</p> <p class="takeaway"><strong>Takeaway:</strong> when performing an AllGather (or a ReduceScatter or AllReduce) in a throughput-bound regime, the actual communication time depends only on the size of the array and the available bandwidth, not the number of devices over which our array is sharded!</p> <p><strong>A note on ICI latency:</strong> Each hop over an ICI link has some intrinsic overhead regardless of the data volume. This is typically around 1us. This means when our array \(A\) is very small and each hop takes less than 1us, we can enter a “latency-bound” regime where the calculation <em>does</em> depend on \(\lvert X \rvert\).</p> <details><summary>For the full details, click here.</summary> <p>Let \(T_\text{min}\) be the minimum time for a single hop. Then</p> \[T_{hop} = \max \left[ T_{min}, \frac{2 \cdot V}{|X| \cdot W_\text{ICI}} \right]\] \[T_{total} = \max \left[ \frac{T_{min} \cdot |X|}{2}, \frac{V}{W_\text{ICI}} \right]\] <p>since we perform \(\lvert X \rvert / 2\) hops. For large reductions or gathers, we’re solidly bandwidth bound. We’re sending so much data that the overhead of each hop is essentially negligible. But for small arrays (e.g. when sampling from a model), this isn’t negligible, and the ICI bandwidth isn’t relevant. We’re bound purely by latency. Another way to put this is that given a particular TPU, e.g. TPU v5e with <code class="language-plaintext highlighter-rouge">4.5e10</code> unidirectional ICI bandwidth, sending any buffer under <code class="language-plaintext highlighter-rouge">4.5e10 * 1e-6 = 45kB</code> will be latency bound.</p> </details> <p><strong>What happens when we AllGather over multiple axes?</strong> When we gather over multiple axes, we have multiple dimensions of ICI over which to perform the gather. For instance, AllGather<sub>XY</sub>([B, D<sub>XY</sub>]) operates over two hardware mesh axes. This increases the available bandwidth by a factor of \(n_\text{axes}\).</p> <details><summary>For the full details, click here.</summary> <p>In general we have</p> \[T_{total} = \max \left[ \frac{T_{min} \cdot \sum_{i} |X_i|}{2}, \frac{V}{W_\text{ICI} \cdot n_\text{axes}} \right]\] <p>where \(\sum_i \lvert X_i \rvert / 2\) is the length of the longest path in the TPU mesh.</p> </details> <p><b style="color:rgb(144, 92, 255);">Pop Quiz 2 [AllGather time]:</b> Using the numbers from <a href="../tpus">Part 2</a>, how long does it take to perform the AllGather<sub>Y</sub>([E<sub>Y</sub>, F]) → [E, F] on a TPUv5e with a 2D mesh <code class="language-plaintext highlighter-rouge">{'X': 8, 'Y': 4}</code>, \(E = 2048\), \(F = 8192\) in bfloat16? What about with \(E=256, F=256\)?</p> <details><summary>Click here for the answer.</summary> <p><strong>Answer:</strong> Let’s start by calculating some basic quantities:</p> <p>1) TPU v5e has 4.5e10 bytes/s of unidirectional ICI bandwidth for each of its 2 axes.<br> 2) In bfloat16 for (a), we have $A[E_Y, F]$ so each device holds an array of shape bfloat16[512, 8192] which has 512 * 8192 * 2 = 8.4MB. The total array has size 2048 * 8192 * 2 = 34MB.</p> <p><em>For part (1)</em>, we can use the formula above. Since we’re performing the AllGather over one axis, we have $T_{\text{comms}} = 34e6 / 9e10 = 377\mu s$. To check that we’re not latency-bound, we know over an axis of size 4, we’ll have at most 3 hops, so our latency bound is something like 3us, so we’re not close. However, TPU v5e only has a wraparound connection when one axis has size 16, so here <em>we actually can’t do a fully bidirectional AllGather</em>. We have to do 3 hops for data from the edges to reach the other edge, so in theory we have more like $T_{\text{comms}} = 3 * 8.4e6 / 4.5e10 = 560\mu s$. <a href="https://imgur.com/a/RkvpRGQ" rel="external nofollow noopener" target="_blank"><strong>Here’s</strong></a> <strong>an actual profile</strong> from <a href="https://colab.research.google.com/drive/15tDZMfNqm2vJjvSzw5VC9qtSwc5td-oV?usp=sharing" rel="external nofollow noopener" target="_blank">this Colab</a>, which shows $680 \mu s$, which is reasonable since we’re likely not getting 100% of the theoretical bandwidth! <em>For part (2)</em> each shard has size <code class="language-plaintext highlighter-rouge">64 * 256 * 2 = 32kB. 32e3 / 4.5e10 = 0.7us</code>, so we’re latency bound. Since we have 3 hops, this will take roughly 3 * 1us = 3us. <a href="https://imgur.com/a/HZLQmYs" rel="external nofollow noopener" target="_blank">In practice, it’s closer to 8us.</a></p> </details> <h3 id="case-3-both-multiplicands-have-sharded-contracting-dimensions">Case 3: both multiplicands have sharded contracting dimensions</h3> <p>The third fundamental case is when both multiplicands are sharded on their contracting dimensions, along the same mesh axis:</p> \[\textbf{A}[I, J_X] \cdot \textbf{B}[J_X, K] \rightarrow C[I, K]\] <p>In this case the <em>local</em> sharded block matrix multiplies are at least <em>possible</em> to perform, since they will share the same sets of contracting indices. But each product will only represent a <em>partial sum</em> of the full desired product, and each device along the <strong>X</strong> dimension will be left with different <em>partial sums</em> of this final desired product. This is so common that we extend our notation to explicitly mark this condition:</p> \[\textbf{A}[I, J_X] \cdot_\text{LOCAL} \textbf{B}[J_X, K] \rightarrow C[I, K] \{\ U_X \}\] <p>The notation <strong>{ U<sub>X</sub> }</strong> reads “<strong>unreduced</strong> along X mesh axis” and refers to this status of the operation being “incomplete” in a sense, in that it will only be finished pending a final sum. The $\cdot_\text{LOCAL}$ syntax means we perform the local sum but leave the result unreduced.</p> <p>This can be seen as the following result about matrix multiplications and outer products:</p> \[A \cdot B = \sum_{i=1}^{P} \underbrace{A_{:,i} \otimes B_{i,:}}_{\in \mathbb{R}^{n \times m}}\] <p>where ⊗ is the outer product. Thus, if TPU <strong>i</strong> on axis <strong>X</strong> has the <strong>i</strong>th column of <strong>A</strong>, and the <strong>i</strong>th row of <strong>B</strong>, we can do a local matrix multiplication to obtain \(A_{:,i} \otimes B_{i,:} \in \mathbb{R}_{n\times m}\). This matrix has, in each entry, the <strong>i</strong>th term of the sum that <strong>A • B</strong> has at that entry. We still need to perform that sum over <strong>P</strong>, which we sharded over mesh axis <strong>X</strong>, to obtain the full <strong>A • B</strong>. This works the same way if we write <strong>A</strong> and <strong>B</strong> by blocks (i.e. shards), and then sum over each resulting shard of the result.</p> <p>We can perform this summation using a full <strong>AllReduce</strong> across the <strong>X</strong> axis to remedy this:</p> \[\begin{align*} A[I, J_X] \cdot_\text{LOCAL} B[J_X, K] \rightarrow &amp;\ C[I, K] \{ U_X \} \\ \textbf{AllReduce}_X C[I, K] \{ U_X \} \rightarrow &amp;\ C[I, K] \end{align*}\] <p>AllReduce removes partial sums, resulting in <em>each</em> device along the axis having the same fully-summed value. AllReduce is the second of several key communications we’ll discuss in this section, the first being the AllGather, and the others being ReduceScatter and AllToAll. An AllReduce takes an array with an unreduced (partially summed) axis and performs the sum by passing those shards around the unreduced axis and accumulating the result. The signature is</p> \[\textbf{AllReduce}_Y A[I_X, J] \{U_Y\} \rightarrow A[I_X, J]\] <p>This means it simply removes the $\{U_Y\}$ suffix but otherwise leaves the result unchanged.</p> <p><strong>How expensive is an AllReduce?</strong> One mental model for how an AllReduce is performed is that every device sends its shard to its neighbors, and sums up all the shards that it receives. Clearly, this is more expensive than an AllGather because each “shard” has the same shape as the full array. Generally, <strong>an AllReduce is twice as expensive as an AllGather.</strong> One way to see this is to note that an <strong>AllReduce</strong> can be expressed as a composition of two other primitives: a <strong>ReduceScatter</strong> and an <strong>AllGather</strong>. Like an AllReduce, a ReduceScatter resolves partial sums on an array but results in an output ‘scattered’ or partitioned along a given dimension. AllGather collects all those pieces and ‘unpartitions/unshards/replicates’ the logical axis along that physical axis.</p> \[\begin{align*} \textbf{ReduceScatter}_{Y,J} : A[I_X,J] \{U_Y\} \rightarrow &amp;\ A[I_X, J_Y] \\ \textbf{AllGather}_Y : A[I_X, J_Y] \rightarrow &amp;\ A[I_X, J] \end{align*}\] <p><strong>What about a ReduceScatter?</strong> Just as the AllReduce removes a subscript ($F_Y \to F$ above), a ReduceScatter sums an unreduced/partially summed array and then scatters (shards) a different logical axis along the same mesh axis. $[F]\{U_Y\} \to [F_Y]$. The animation shows how this is done: note that it’s very similar to an AllGather but instead of retaining each shard, we sum them together. Thus, its latency is roughly the same, excluding the time taken to perform the reduction.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/reduce-scatter-480.webp 480w,/scaling-book/assets/img/reduce-scatter-800.webp 800w,/scaling-book/assets/img/reduce-scatter-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/reduce-scatter.gif" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The communication time for each hop is simply the per-shard bytes \(V\) divided by the bandwidth, as it was for an AllGather, so we have</p> \[T_{\text{comms per AllGather or ReduceScatter}} = \frac{V}{W_\text{ICI}}\] \[T_{\text{comms per AllReduce}} = 2 \cdot \frac{V}{W_\text{ICI}}\] <p>where \(W_\text{ICI}\) is the bidirectional bandwidth, so long as we have a full ring to reduce over.</p> <h3 id="case-4-both-multiplicands-have-a-non-contracting-dimension-sharded-along-the-same-axis">Case 4: both multiplicands have a non-contracting dimension sharded along the same axis</h3> <p>Each mesh dimension can appear at most once when sharding a tensor. Performing the above rules can sometimes lead to a situation where this rule is violated, such as:</p> \[A[I_X, J] \cdot B[J, K_X] \rightarrow C[I_X, K_X]\] <p>This is invalid because a given shard, say <strong>i</strong>, along dimension <strong>X</strong>, would have the <strong>(i, i)</strong>th shard of <strong>C</strong>, that is, a diagonal entry. There is not enough information among all shards, then, to recover anything but the diagonal entries of the result, so we cannot allow this sharding.</p> <p>The way to resolve this is to AllGather some of the dimensions. Here we have two choices:</p> \[\begin{align*} \textbf{AllGather}_X A[I_X, J] \rightarrow &amp;\ A[I, J] \\ A[I, J] \cdot B[J, K_X] \rightarrow &amp;\ C[I, K_X] \end{align*}\] <p>or</p> \[\begin{align*} \textbf{AllGather}_X B[J, K_X] \rightarrow &amp;\ B[J, K] \\ A[I_X, J] \cdot B[J, K] \rightarrow &amp;\ C[I_X, K] \end{align*}\] <p>In either case, the result will only mention <strong>X</strong> once in its shape. Which one we pick will be based on what sharding the following operations need.</p> <h2 id="a-deeper-dive-into-tpu-communicaton-primitives">A Deeper Dive into TPU Communicaton Primitives</h2> <p>The previous 4 cases have introduced several “core communication primitives” used to perform sharded matrix multiplications:</p> <ol> <li> <strong>AllGather:</strong> removes a subscript from a sharding, gathering the shards.</li> <li> <strong>ReduceScatter:</strong> removes an “un-reduced” suffix from an array by summing shards over that axis, leaving the array sharded over a second axis.</li> <li> <strong>AllReduce:</strong> removes an “un-reduced” suffix, leaving the array unsharded along that axis.</li> </ol> <p>There’s one more core communication primitive to mention that arises in the case of Mixture of Experts (MoE) models and other computations: the <strong>AllToAll</strong>.</p> <h3 id="our-final-communication-primitive-the-alltoall">Our final communication primitive: the AllToAll</h3> <p>A final fundamental collective which does not occur naturally when considering sharded matrix multiplies, but which comes up constantly in practice, is the <strong>AllToAll</strong> collective, or more precisely the special case of a <em>sharded transposition</em> or resharding operation. e.g.</p> \[\textbf{AllToAll}_{X, J} A[I_X, J] \rightarrow A[I, J_X]\] <p>AllToAlls are typically required to rearrange sharded layouts between different regions of a sharded computation that don’t have compatible layout schemes. They arise naturally when considering sharded mixture-of-experts models. <em>You can think of an AllToAll as moving a subscript from one axis to another.</em> Because an all to all doesn’t need to replicate all of the data of each shard across the ring, it’s actually <em>cheaper</em> than an allgather (by a factor of ¼).<d-footnote>For even-sized bidirectional rings, each device will send $(N/2 + (N/2-1) + … + 1)$ chunks right and $((N/2-1) + … + 1)$ chunks left $= 0.5 \cdot (N / 2) \cdot (N/2 + 1) + 0.5 \cdot (N / 2) \cdot (N/2 - 1) = N^2/4$. The size of each chunk (aka shard of a shard) is $\text{bytes} / N^2$ so the per-device cost is $(\text{bytes} / N^2) \cdot N^2 / 4 = \text{bytes} / 4$. This result scales across all devices as the total bandwidth scales with device number.</d-footnote></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/all-to-all-480.webp 480w,/scaling-book/assets/img/all-to-all-800.webp 800w,/scaling-book/assets/img/all-to-all-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/all-to-all.gif" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="more-about-the-reducescatter">More about the ReduceScatter</h3> <p>ReduceScatter is a more fundamental operation than it first appears, as it is actually the derivative of an AllGather, and vice versa. i.e. if in the forward pass we have:</p> \[\textbf{AllGather}_X A[I_X] \rightarrow A[I]\] <p>Then we ReduceScatter the reverse-mode derivatives <strong>A’</strong> (which will in general be different on each shard) to derive the sharded <strong>A’</strong>:</p> \[\textbf{ReduceScatter}_X A'[I] \{ U_X \} \rightarrow A'[I_X]\] <p>Likewise, \(\text{ReduceScatter}_X(A[I] \{U_X\}) \to A[I_X])\) in the forward pass implies \(\text{AllGather}_{X}(A'[I_X]) \to A'[I]\) in the backwards pass.</p> <p>Turning an AllReduce into an AllGather and ReduceScatter also has the convenient property that we can defer the final AllGather until some later moment. Very commonly we’d rather not pay the cost of reassembling the full matrix product replicated across the devices. Rather we’d like to preserve a sharded state even in this case of combining two multiplicands with sharded contracting dimensions:</p> \[A[I, J_X] \cdot B[J_X, K] \rightarrow C[I, K_X]\] <p>In this case, we can also perform a ReduceScatter instead of an AllReduce, and then optionally perform the AllGather at some later time, i.e.</p> \[\begin{align*} A[I, J_X] \cdot_{LOCAL} B[J_X, K] \rightarrow &amp;\ C[I, K] \{ U_X \} \\ \textbf{ReduceScatter}_{X,K} C[I, K] \{ U_X \} \rightarrow &amp;\ C[I, K_X] \end{align*}\] <p>Note that ReduceScatter <em>introduces</em> a sharded dimension, and so has a natural freedom to shard along either the <strong>I</strong> or <strong>K</strong> named dimensions in this case. We generally need to choose <em>which</em> named dimension to introduce a new sharding to when using a ReduceScatter (though the choice is usually forced by the larger modeling context). This is why we use the syntax <strong>ReduceScatter<sub>X,K</sub></strong> to specify the axis to shard.</p> <h2 id="what-have-we-learned">What Have We Learned?</h2> <ul> <li>The sharding of an array is specified by a <strong>Mesh</strong> that names the physical, hardware axes of our TPU mesh and a <strong>Sharding</strong> that assigns mesh axis names to the logical axes of the array. <ul> <li>For example, <strong>A</strong>[I<sub>XY</sub>, J] describes an abstract array <strong>A</strong> with its first dimension sharded along two mesh axes X and Y. Combined with Mesh(mesh_shape=(4, 8), axis_names=(‘X’, ‘Y’)) or the abbreviated Mesh({‘X’: 4, ‘Y’: 8}), this tells us our array is sharded 32 ways along the first dimension.</li> </ul> </li> <li> <p><strong>Arithmetic with sharded arrays works exactly like with unsharded arrays unless you perform a contraction along a sharded axis</strong>. In that case, we have to introduce some communication. We consider four cases:</p> <ol> <li> <em>Neither array is sharded along the contracting dimension</em>: no communication is needed.</li> <li> <em>One array is sharded along the contracting dimension</em> (or the contracting dimensions are sharded along different axes): we AllGather one of the inputs before performing the operation.</li> <li> <em>Both arrays are identically sharded along the contracting dimension:</em> we multiply the shards locally then perform an AllReduce or ReduceScatter.</li> <li> <em>Both arrays are sharded along the same mesh axis along a non-contracting dimension:</em> we AllGather one of the inputs first.</li> </ol> </li> <li>TPUs use roughly <strong>4 core communication primitives</strong>: <ol> <li>AllGather: $[A_X, B] \to [A, B]$</li> <li>ReduceScatter: $[A, B] \{U_X\} \to [A, B_X]$</li> <li>AllToAll: $[A, B_X] \to [A_X, B]$</li> <li>AllReduce: $[A_X, B]\{U_Y\} \to [A_X, B]$ (technically not a primitive since it combines a ReduceScatter + AllGather)</li> </ol> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/all-collectives-480.webp 480w,/scaling-book/assets/img/all-collectives-800.webp 800w,/scaling-book/assets/img/all-collectives-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/all-collectives.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>The cost and latency of each of these operations <strong>doesn’t depend on the size of the axis (as long as they’re bandwidth bound)</strong>, but only on the size of the input arrays and the bandwidth of the link. For a unidirectional AllGather/ReduceScatter:</li> </ul> \[T_{\text{comm per AllGather or ReduceScatter}} = \frac{\text{Data volume}}{\text{bandwidth}} \cdot \frac{\text{Axis} - 1}{\text{Axis}} \longrightarrow \frac{\text{Data volume}}{\text{bandwidth (bidirectional)}}\] <ul> <li>An AllReduce is composed of a ReduceScatter followed by an AllGather, and thus has 2x the above cost. An AllToAll only has to pass shards part-way around the ring and is thus ¼ the cost of an AllGather. Here’s a summary:</li> </ul> <table> <thead> <tr> <th style="text-align: left">Operation</th> <th style="text-align: left">Description</th> <th style="text-align: left">Syntax</th> <th style="text-align: left">Runtime</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>AllGather</strong></td> <td style="text-align: left">Gathers all the shards of a sharded array along an axis, removing a subscript.</td> <td style="text-align: left">$[A_X, B] \to [A, B]$</td> <td style="text-align: left">bytes / (bidirectional ICI bandwidth * num_axes)</td> </tr> <tr> <td style="text-align: left"><strong>ReduceScatter</strong></td> <td style="text-align: left">Sums a partially summed array along an axis and shards it along another axis (adding a subscript).</td> <td style="text-align: left">$[A, B] \{U_X\} \to [A_X, B]$</td> <td style="text-align: left">Same as AllGather</td> </tr> <tr> <td style="text-align: left"><strong>AllReduce</strong></td> <td style="text-align: left">Sums a partially summed array along an axis. Removes a { U<sub>x</sub> }. Combines an AllGather and ReduceScatter.</td> <td style="text-align: left">$[A_X, B]\{U_Y\} \to [A_X, B]$</td> <td style="text-align: left">2 * AllGather</td> </tr> <tr> <td style="text-align: left"><strong>AllToAll</strong></td> <td style="text-align: left">Gathers (replicates) an axis and shards a different dimension along the same axis.</td> <td style="text-align: left">$[A, B_X] \to [A_X, B]$</td> <td style="text-align: left">AllGather / 4 for a bidirectional ring</td> </tr> </tbody> </table> <h2 id="some-problems-to-work">Some Problems to Work</h2> <p><em>Here are some instructive problems based on content in this section. We won’t include all answers at the moment but we’ll write up more answers as we can.</em></p> <p><strong>Question 1 [replicated sharding]</strong>: An array is sharded $A[I_X, J, K, \ldots]$ (i.e., only sharded across $X$), with a mesh <code class="language-plaintext highlighter-rouge">Mesh({'X': 4, 'Y': 8, 'Z': 2})</code>. What is the ratio of the total number of bytes taken up by $A$ across all chips to the size of one copy of the array?</p> <details><summary>Click here for the answer.</summary> <p>Our array is only sharded along X, which has size 4, so effectively each shard has size $[I / 4, J, K, \ldots] = \text{sizeof}(A) / 4$. Since our array is replicated across Y and Z, the total size is $Y \cdot Z \cdot \text{sizeof}(A)$, so the ratio of total size to single chip size is $Y \cdot Z \cdot \text{sizeof}(A) / \text{sizeof}(A) = 16$.</p> </details> <p><strong>Question 2 [AllGather latency]</strong>: How long should $\text{AllGather}_X([B_X, D_Y])$ take on a TPUv4p 4x4x4 slice with mesh <code class="language-plaintext highlighter-rouge">Mesh({'X': 4, 'Y': 4, 'Z': 4})</code> if $B=1024$ and $D=4096$ in bfloat16? How about \(\text{AllGather}_{XY}([B_X, D_Y])\)? How about \(\text{AllReduce}_Z([B_X, D_Y] \{U_Z \})\)?</p> <details><summary>Click here for the answer.</summary> <p>We have a wraparound link on all axes because we have a full <code class="language-plaintext highlighter-rouge">4x4x4</code> cube, so we have 9e10 bidirectional bandwidth to work with.</p> <ol> <li> <p>Because we’re just gathering over one axis and the other is sharded, we’re effectively gathering $\frac{2BD}{Y}$ bytes over 1 axis. Since our ICI bandwidth for TPU v4p is 9e10 bytes/second bidirectional, this will take $\frac{2BD}{9e10 \cdot Y} = \frac{2 \cdot 1024 \cdot 4096}{9e10 \cdot 4} = 23 \mu s$.</p> </li> <li> <p>We have twice the bandwidth as before but we’re AllGathering the full array, so <code class="language-plaintext highlighter-rouge">T = 2BD / (2 * W) = 2*1024*4096 / (2 * 9e10) = 46us</code>. This is far from the latency bound of 4us (1us per hop), so we’re fine.</p> </li> <li> <p>The cost of an AllReduce is twice that of an AllGather, so the cost is about $4BD / W$, or roughly <code class="language-plaintext highlighter-rouge">4 * 1024 * 4096 / 9e10 = 190us</code>.</p> </li> </ol> </details> <p><strong>Question 3 [latency-bound AllGather]</strong>: Let’s say we’re performing an $\text{AllGather}_X([B_X])$ but $B$ is very small (say 128). How long should this take on a TPUv4p 4x4x4 slice with mesh <code class="language-plaintext highlighter-rouge">Mesh({'X': 4, 'Y': 4, 'Z': 4})</code> in bfloat16? <em>Hint: you’re probably latency bound.</em></p> <details><summary>Click here for the answer.</summary> <p>Our array in bfloat16 uses only 256 bytes total, and only 64 per device. Since we have an axis of size 4 on a TPU v4p, we have a wraparound link, so we can do the AllGather by sending half the bytes in each direction, meaning only 32 bytes in each direction. With <code class="language-plaintext highlighter-rouge">4.5e10</code> of unidirectional bandwidth, each hop would take roughly <code class="language-plaintext highlighter-rouge">32 / 4.5e10 ~ 0</code>, so we’re definitely latency bound. Counting the number of hops, we can do the full gather in only 2 hops, so roughly 2us a good estimate.</p> </details> <p><strong>Question 4 [matmul strategies]</strong>: To perform $A[B, D] \cdot_D B[D_X, F] \to C[B, F]$, in this section we tell you to perform $\text{AllGather}_X(B[D_X, F])$ and multiply the fully replicated matrices (Case 2, <em>Strategy 1</em>). Instead, you could multiply the local shards like $A[B, D_X] \cdot_D B[D_X, F] \to C[B, F] \{U_X\}$ (Case 4, <em>Strategy 2</em>), and then $\text{AllReduce}_X(C[B, F] \{ U_X\})$. How many FLOPs and comms does each of these perform? Which is better and why?</p> <details><summary>Click here for the answer.</summary> <p>Let’s start with our baseline (<em>Strategy 1</em>). As we’ve shown, the cost of the AllGather is $2DF / W_\text{ici}$. Once we have the fully replicated arrays, the total compute time is $2BDF / C$ (where $C$ is our accelerator FLOPs/s, since each TPU does the same FLOPs). So we have</p> \[T_\text{total (Strategy 1)} = \max\left(\frac{2BDF}{C}, \frac{2DF}{W_\text{ici}}\right)\] <p>By comparison, the new strategy (Strategy 2) does twice as many comms (for the AllReduce) and $1 / X$ fewer FLOPs since the computation is sharded. This means we do $2\cdot B\cdot D\cdot F / X$ FLOPs and the resulting AllReduce communicates \(2 \cdot 2 \cdot B \cdot F\) bytes in bfloat16. Thus, our total time for <em>Strategy 2</em> (no AllGather, just an AllReduce later on) is roughly</p> \[T_\text{total} = \max\left(\frac{2BDF}{X \cdot C}, \frac{4BF}{W_\text{ici}}\right)\] <p>The question is: <em>which of these is bigger?</em> Strategy (2) is compute bound when $D / (X \cdot C) &gt; 2 / W_\text{ici}$, or when $D / 2X &gt; C / W_\text{ici} \approx 2550 \rightarrow X &lt; D / (2 * 2550)$. We might reasonably expect $D \approx 8k$, so this would mean roughly $X &lt; 2$ which is unlikely. So we’re basically always comms bound in the first case. In the second case (baseline), we’re comms bound when \(F &lt; C / W_\text{ici} = 2550\) (which is rarely true), so we’re generally compute-bound. Thus, the question of whether strategy (1) is better becomes whether</p> \[T_\text{comms for Strategy 2} &lt; T_\text{math for Strategy 1} \Leftrightarrow \frac{4BF}{W_{ici}} &lt; \frac{2BDF}{C}\] <p>This is true when $2 / W_\text{ici} &lt; D / C$, or when $D &gt; 2 * 2550 = 5100$, which is usually true for large models. So this alternative strategy is typically better for large models.</p> <p><em>Why don’t we always do this?</em> Well, in practice we may do this sometimes, but it’s typically rare to have the contracting dimension of one of the inputs to a matmul sharded along a axis that the other input isn’t sharded over. For instance, if we’re doing FSDP (explained in <a href="../training">Section 5</a>), we’ll sharded our parameters over the data dimension but our activations will <em>also be sharded along data</em>. So in this sense this doesn’t show up much.</p> </details> <p><strong>Question 5 [minimum latency]</strong>: Let’s say I want to do a matmul $A[B, D] \cdot_D B[D, F] \to C[B, F]$ on a TPUv5p 4x4x4 with the lowest possible latency. How should my inputs be sharded? What is the total FLOPs and comms time?</p> <p><strong>Question 6:</strong> Let’s say we want to perform $A[I_X, J_Y] \cdot_J B[J_Y, K] \to C[I_X, K]$ on TPUv5e 4x4. What communication do we perform? How much time is spent on communication vs. computation?</p> <ul> <li>What about $A[I_X, J] \cdot_J B[J_X, K_Y] \to C[I_X, K_Y]$? This is the most standard setting for training where we combine data, tensor, and zero sharding.</li> <li>What about $A[I_X, J] \cdot_J B[J, K_Y] \to C[I_X, K_Y]$? This is standard for inference, where do pure tensor parallelism (+data).</li> </ul> <p><strong>Question 7:</strong> A typical Transformer block has two matrices $B[D, F]$ and $C[F, D]$ where $F \gg D$. With a batch size B, the whole block is \(C \cdot B \cdot x\) with \(x[B, D]\). Let’s pick \(D=8192\), \(F=32768\), and \(B=128\) and assume everything is in bfloat16. Assume we’re running on a TPUv5e 2x2 slice but assume each TPU only has 300MB of free memory. How should <strong>B, C, and the output be sharded to stay below the memory limit while minimizing overall time? How much time is spent on comms and FLOPs?</strong></p> <p><strong>Question 8 [challenge]</strong>: Using the short code snippet above as a template, allocate a sharded array and benchmark each of the 4 main communication primitives (AllGather, AllReduce, ReduceScatter, and AllToAll) using pmap or shard_map. You will want to use <code class="language-plaintext highlighter-rouge">jax.lax.all_gather</code>, <code class="language-plaintext highlighter-rouge">jax.lax.psum</code>, <code class="language-plaintext highlighter-rouge">jax.lax.psum_scatter</code>, and <code class="language-plaintext highlighter-rouge">jax.lax.all_to_all</code>. Do you understand the semantics of these functions? How long do they take?</p> <p><strong>Question 9 [another strategy for sharded matmuls?]</strong>: <a href="#case-2-one-multiplicand-has-a-sharded-contracting-dimension">Above</a> we claimed that when only one input to a matmul is sharded along its contracting dimension, we should AllGather the sharded matrix and perform the resulting contracting locally. Another strategy you might think of is to perform the sharded matmul and then AllReduce the result (as if both inputs were sharded along the contracting dimension), i.e. $A[I, J_X] *_J B[J, K] \to C[I, K]$ by way of</p> <ol> <li>$C[I, K] \{ U_X \} = A[I, J_X] \cdot B[J_X, K]$</li> <li>$C[I, K] = \text{AllReduce}(C[I, K] \{ U_X\})$</li> </ol> <p>Answer the following:</p> <ol> <li>Explicitly write out this algorithm for matrices $A[N, M]$ and $B[M, K]$, using indices to show exactly what computation is done on what device. Assume $A$ is sharded as $A[I, J_X]$ across ND devices, and you want your output to be replicated across all devices.</li> <li>Now suppose you are ok with the final result not being replicated on each device, but instead sharded (across either the N or K dimension). How would the algorithm above change?</li> <li>Looking purely at the communication cost of the strategy above (in part (b), not (a)), how does this communication cost compare to the communication cost of the algorithm in which we first AllGather A and then do the matmul?</li> </ol> <details><summary>Click here for the answer.</summary> <ol> <li>First compute the outer products, storing the result in \(O[N, K]: o_{kj} = \sum_i a_{ik} b_{ji}\). Note that the repeated index is not the one being contracted, as we are doing an outer product. Here the sum ranges across the set of i values stored on the particular device we are using. So, for example, if we have a contracting axis of size 16, and 4 devices, then on device 0, i would range from {0, 1, 2, 3}; on device 1, i would range from {4, 5, 6, 7}; on device 2, i would range from {8, 9, 10, 11}; and on device 3, i would range from {12, 13, 14, 15}. Then AllReduce the partial-sums of $O[N, K]$ which live on each device, to form the full O[N x K].</li> <li>Instead of doing an AllReduce in step 2, we could get away with a cheaper ReduceScatter, along either axis: $[N, K] \{ U_X \} \to [N_X, K]$ or $[N, K] \{ U_X \} \to [N, K_X]$.</li> <li>As described in the main text above, the cost of doing an AllGather (when we are throughput-bound) is the same as that of a ReduceScatter; it is simply given by the size of the full matrix we are processing. So in the gather-then-matmul algorithm, this scales as $NM$ (since we are $\text{AllGather}$-ing $A$); in the matmul-then-reduce-scatter algorithm, this scales as NK (since we are reduce-scattering $O$). So the communication cost ratio of the two algorithms is <code class="language-plaintext highlighter-rouge">M/K</code>.</li> </ol> </details> <p><strong>Question 10: Fun with AllToAll:</strong> In the table above, it was noted that the time to perform an AllToAll is a factor of 4 lower than the time to perform an AllGather or ReduceScatter (in the regime where we are throughput-bound). In this problem we will see where that factor of 4 comes from, and also see how this factor would change if we only had single-direction ICI links, rather than bidirectional ICI links.</p> <ol> <li>Let’s start with the single-direction case first. Imagine we have <em>D</em> devices in a ring topology, and If we are doing either an AllGather or a ReduceScatter, on an N x N matrix <em>A</em> which is sharded as $A[I_X, J]$ (say $D$ divides $N$ for simplicity). Describe the comms involved in these two collectives, and calculate the total number of scalars (floats or ints) which are transferred across <strong>a single</strong> ICI link during the entirety of this algorithm.</li> <li>Now let’s think about an AllToAll, still in the single-directional ICI case. How is the algorithm different in this case than the all-gather case? Calculate the number of scalars that are transferred across a single ICI link in this algorithm.</li> <li>You should have found that the ratio between your answers to part (a) and part (b) is a nice number. Explain where this factor comes from in simple terms.</li> <li>Now let’s add bidirectional communication. How does this affect the total time needed in the all-gather case?</li> <li>How does adding bidirectional communication affect the total time needed in the AllToAll case?</li> <li>Now simply explain the ratio between AllGather time and AllToAll time in a bidirectional ring.</li> </ol> <details><summary>Click here for the answer.</summary> <p>(1) <strong>Solution:</strong> The process is simple: in each step of the algorithm, each device will send a single-shard “strip” of the matrix (totalling \(\frac{N}{D} \times N\) elements in size) to its nearest neighbor. This occurs \(D-1\) times, since each shard needs to be communicated to all of the devices except the one it starts out on. So in total, \(\frac{N^2(D-1)}{D}\) scalars are transferred by each device, i.e. flow across a single ICI link.</p> <p><strong>Answer:</strong> \(N^2 (1-\frac{1}{D})\), or simply \(N^2\) when \(D &gt;&gt; 1\).</p> <p>(2) <strong>Solution:</strong> The key difference between an AllToAll and an AllGather, from the perspective of communications, is that in an AllToAll, the entirety of the shard that lives on a particular device does not need to be communicated to every other device. Imagine the shard stored on a particular device (call it device 0) is \([A, B, C, D]\) (here A,B,C,D are matrices and we are imagining a ring with 4 devices for illustration). Now the matrix \(A\) does not need to be communicated anywhere, the matrix \(B\) needs to end up on device 1; matrix \(C\) ends up on device 2; and matrix \(D\) ends up on device 3. So in the first step of the algorithm, we send \(B\), \(C\), and \(D\) to device 1; in the next step, device 1 sends \(C\) and \(D\) onwards to device 2; in the final step, device 2 sends just \(D\) on to device 3. The total number of parameters transferred in this case is \((\text{size of A/B/C/D}) * (3 + 2 + 1)\). The size of A/B/C/D is (in the general case now) \(\frac{N^2}{D^2}\), and again in the general case the \((3 + 2 + 1)\) term becomes \(((D-1) + (D-2) + … + 1)\), or \(\frac{(D)(D-1)}{2}\). So the total number of bytes transferred across a single ICI link is \(\frac{N^2(D-1)}{D \times 2}\).</p> <p><strong>Answer:</strong> \(\frac{N^2}{2}(1-\frac{1}{D})\), or simply \(\frac{N^2}{2}\) when \(D &gt;&gt; 1\).</p> <p>(3) <strong>Solution:</strong> The factor is simply \(\frac{1}{2}\), i.e. an AllToAll is half as costly as an all-gather/ReduceScatter on a unidirectional ring topology. Looking over the derivations above, this ultimately came from the fact that in the all-gather case, we are transferring the same sized block each of \((D-1)\) times, i.e. we’re doing the sum \(\text{tiny block size} * (D + D + D + … + D)\), whereas in the AllToAll case, we’re doing the sum \(\text{tiny block size} * (D + D-1 + D-2 + … + 1)\). The factor of two thus essentially comes from the fact that \(1 + 2 + \ldots + n = n(n+1)/2\).</p> <p>(4) <strong>Solution</strong>: The total number of scalars that any one link has to carry now reduces by a factor of 2, since in a bidirectional ring, each “sharded strip” can be sent two ways simultaneously.</p> <p>(5) <strong>Solution</strong>: In this case, we win a factor of 4 compared to the unidirectional case. This is easiest to see by considering the fate of each of the size-(N2/D2) blocks in a single sharded strip, say the one which originates on device 0. Instead of (as in the unidirectional case) sending one of these blocks a distance of D-1, another block a distance D - 2, etc. all the way to 1, we now divide the strip into blocks which move right or left, moving a maximum distance of ceil(D/2). So the corresponding sum now becomes \(D/2 + D/2 - 1 + D/2 - 2 + … = D/2 \cdot (D/2-1)/2\), or \(D^2/8\) in the limit of large \(D\). Compare this to \(D^2/2\) in the unidirectional case, and we see that we’ve won a factor of 4.</p> <p>(6) <strong>Solution:</strong> In a unidirectional ring, we saw that the AllToAll time was already twice as fast as the all-gather time; this comes from the fact that we don’t need to send our full strip to every single device. Then, when we added bidirectionality, we saw that it was a 4x win for AllToAll, and only a 2x win for all-gathers. Putting these ratios together, we get our sought after factor of 4.</p> </details> <h3 class="next-section">That’s it for Part 3! For Part 4 (about Transformer math), click <a href="../transformers">here</a>!</h3> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> <div class="base-grid appendix-entry"> <h3 style="grid-column: 0;">Miscellaneous</h3> <p class="author-footnote" style="grid-column: text;"><sup>*</sup>Work done at Google DeepMind, now at MatX.</p> </div> <div class="base-grid appendix-entry"> <h3 style="grid-column: 0;">Citation</h3> <p class="author-footnote">For attribution in academic contexts, please cite this work as:</p> <div class="author-footnote"> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c">Austin et al., "How to Scale Your Model", Google DeepMind, online, 2025.</span>
</code></pre></div></div> </div> <p class="author-footnote">or as a BibTeX entry:</p> <div class="author-footnote"> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="nc">@article</span><span class="p">{</span><span class="nl">scaling-book</span><span class="p">,</span>
      <span class="na">title</span> <span class="p">=</span> <span class="s">{How to Scale Your Model}</span><span class="p">,</span>
      <span class="na">author</span> <span class="p">=</span> <span class="s">{Austin, Jacob and Douglas, Sholto and Frostig, Roy and Levskaya, Anselm and Chen, Charlie and Vikram, Sharad
      and Lebron, Federico and Choy, Peter and Ramasesh, Vinay and Webson, Albert and Pope, Reiner}</span><span class="p">,</span>
      <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Google DeepMind}</span><span class="p">,</span>
      <span class="na">howpublished</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
      <span class="na">note</span> <span class="p">=</span> <span class="s">{Retrieved from https://jax-ml.github.io/scaling-book/}</span><span class="p">,</span>
      <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
    <span class="p">}</span>
</code></pre></div></div> </div> </div> </d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'jax-ml/scaling-book',
        'data-repo-id': '',
        'data-category': 'General',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '0',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-loading': '1',
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/scaling-book/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/scaling-book/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/scaling-book/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/scaling-book/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/scaling-book/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/scaling-book/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/scaling-book/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/scaling-book/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/scaling-book/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>