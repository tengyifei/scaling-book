<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Programming TPUs in JAX | How To Scale Your Model </title> <meta name="author" content=" "> <meta name="description" content="How to use JAX to program TPUs efficiently! Much of this section is taken from &lt;a href='https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html'&gt;here&lt;/a&gt;."> <meta name="keywords" content="scaling, jax, llms, transformers, tpus, google, deepmind, parallelism, pallas"> <link rel="stylesheet" href="/scaling-book/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/scaling-book/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/scaling-book/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/scaling-book/assets/css/jekyll-pygments-themes-vs.css?4ee1a2facd1a8a76347f4bd43a740500" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/scaling-book/assets/img/favicon.png?fddbd8c2ec231ba2060e67c85de32a55"> <link rel="stylesheet" href="/scaling-book/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jax-ml.github.io/scaling-book/jax-stuff/"> <script src="/scaling-book/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/scaling-book/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/scaling-book/assets/js/distillpub/template.v2.js"></script> <script src="/scaling-book/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">{{page._styles}}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Programming TPUs in JAX",
            "description": "How to use JAX to program TPUs efficiently! Much of this section is taken from <a href='https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html'>here</a>.",
            "published": "February 04, 2025",
            "authors": [
              
              {
                "author": "Jacob Austin",
                "authorURL": "https://www.jacobaustin.org/",
                "affiliations": [
                  {
                    "name": "Google DeepMind",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Sholto Douglas",
                "authorURL": "https://x.com/_sholtodouglas",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Roy Frostig",
                "authorURL": "https://cs.stanford.edu/~rfrostig/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Anselm Levskaya",
                "authorURL": "https://anselmlevskaya.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Charlie Chen",
                "authorURL": "https://x.com/charliexychen",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Sharad Vikram",
                "authorURL": "https://sharadvikram.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Federico Lebron",
                "authorURL": "https://fedelebron.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Peter Choy",
                "authorURL": "https://x.com/pchoy95",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Vinay Ramasesh",
                "authorURL": "https://x.com/vinayramasesh",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Albert Webson",
                "authorURL": "https://representation.ai/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Reiner Pope<sup>*</sup>",
                "authorURL": "https://x.com/reinerpope",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <script>
    function goToTop() {
      document.body.scrollTop = 0; // For Safari
      document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
    }

    // When the user scrolls down 20px from the top of the document, show the button
    window.onscroll = function() {scrollFunction()};

    function scrollFunction() {
      // Get the button:
      let mybutton = document.getElementById("top-button");

      if (document.body.scrollTop > 40 || document.documentElement.scrollTop > 40) {
        mybutton.style.display = "block";
      } else {
        mybutton.style.display = "none";
      }
  }
  </script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/scaling-book"> How To Scale Your Model </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="left-button section-button"><a href="../profiling"><svg viewbox="-78.5 0 512 512"><path d="M257 64L291 98 128 262 291 426 257 460 61 262 257 64Z"></path></svg></a></div> <div class="right-button section-button"><a href="../conclusion"><svg viewbox="-78.5 0 512 512"><path d="M98 460L64 426 227 262 64 98 98 64 294 262 98 460Z"></path></svg></a></div> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/scaling-book/"> </a> </li> <li class="nav-item nav-hidden"><a class="nav-link" onclick="goToTop()" id="top-button" style="display: none;">Back to Top</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../profiling">Previous Part</a></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../conclusion">Next Part</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Sections </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/scaling-book/index">Part 0. Introduction</a> <a class="dropdown-item " href="/scaling-book/roofline">Part 1. Intro to Rooflines</a> <a class="dropdown-item " href="/scaling-book/tpus">Part 2. All About TPUs</a> <a class="dropdown-item " href="/scaling-book/sharding">Part 3. Sharded Matmuls</a> <a class="dropdown-item " href="/scaling-book/transformers">Part 4. Transformers</a> <a class="dropdown-item " href="/scaling-book/training">Part 5. Training</a> <a class="dropdown-item " href="/scaling-book/applied-training">Part 6. Training LLaMA</a> <a class="dropdown-item " href="/scaling-book/inference">Part 7. Inference</a> <a class="dropdown-item " href="/scaling-book/applied-inference">Part 8. Serving LLaMA</a> <a class="dropdown-item " href="/scaling-book/profiling">Part 9. Profiling</a> <a class="dropdown-item " href="/scaling-book/jax-stuff">Part 10. All About JAX</a> <a class="dropdown-item " href="/scaling-book/conclusion">Part 11. Conclusions</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Programming TPUs in JAX</h1> <p>Part 10 of <a href="/scaling-book">How To Scale Your Model</a> (<a href="../profiling">Part 9: Profiling</a> | <a href="../conclusion">Part 11: Conclusions</a>)</p> <p>How to use JAX to program TPUs efficiently! Much of this section is taken from <a href="https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html" rel="external nofollow noopener" target="_blank">here</a>.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#how-does-parallelism-work-in-jax">How Does Parallelism Work in JAX?</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#jax-jit-the-automatic-parallelism-solution">jax.jit: the automatic parallelism solution</a> </li> <li> <a href="#shard-map-explicit-parallelism-control-over-a-program">shard_map: explicit parallelism control over a program</a> </li> </ul> <div> <a href="#worked-problems">Worked Problems</a> </div> </nav> </d-contents> <h2 id="how-does-parallelism-work-in-jax">How Does Parallelism Work in JAX?</h2> <p>JAX supports two schools of thought for multi-device programming:</p> <ol> <li> <strong>Compiler, take the wheel!</strong> Let the compiler automatically partition arrays and decide what communication to add to facilitate a given program. This lets you write a program on a single device and automatically run it on hundreds without changing anything.</li> <li> <strong>Just let me write what I mean, damnit!</strong> While compilers are nice, they sometimes do the wrong thing and add communication you don’t intend. Sometimes we want to be extremely explicit about what we’re doing.</li> </ol> <p>Correspondingly, JAX provides two APIs for each of these schools: <strong>jit</strong> (<code class="language-plaintext highlighter-rouge">jax.jit</code>) and <strong>shard_map</strong> (<code class="language-plaintext highlighter-rouge">jax.experimental.shard_map.shard_map</code>).</p> <ol> <li> <code class="language-plaintext highlighter-rouge">jax.jit</code> lets you specify the sharding of the inputs and outputs to a program (via <code class="language-plaintext highlighter-rouge">in_shardings</code> and <code class="language-plaintext highlighter-rouge">out_shardings</code>) and infers the rest using the <a href="https://arxiv.org/abs/2105.04663" rel="external nofollow noopener" target="_blank">GSPMD</a> compiler. While it isn’t perfect, it usually does a decent job at automatically scaling your program to any number of chips.</li> <li> <code class="language-plaintext highlighter-rouge">jax.experimental.shard_map.shard_map</code> is the more explicit counterpart. You get a device-local view of the program and have to write any communication you want explicitly. Have a sharded array and want the whole thing on each device? Add a <code class="language-plaintext highlighter-rouge">jax.lax.all_gather</code>. Want to sum an array across your devices? Add a <code class="language-plaintext highlighter-rouge">jax.lax.psum</code> (an AllReduce). Programming is harder but far less likely to do something you don’t want.</li> </ol> <h3 id="jax-jit-the-automatic-parallelism-solution">jax.jit: the automatic parallelism solution</h3> <p>jax.jit plays two roles inside JAX. As the name suggests, it “just-in-time” compiles a function from Python into bytecode (via XLA/HLO/LLO) so it runs faster. But if the input is sharded or the user specifies an <code class="language-plaintext highlighter-rouge">in_sharding</code> or <code class="language-plaintext highlighter-rouge">out_sharding</code>, it also lets XLA distribute the computation across multiple devices and add communication as needed. For example, here’s how you could write a sharded matmul using jax.jit:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">import</span> <span class="n">jax.sharding</span> <span class="k">as</span> <span class="n">shd</span>

<span class="c1"># Running on an TPU v5e 2x2. This assigns names to the two physical axes of the hardware.
</span><span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">make_mesh</span><span class="p">(</span><span class="n">axis_shapes</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">axis_names</span><span class="o">=</span><span class="p">(</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">P</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">shd</span><span class="p">.</span><span class="nc">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">shd</span><span class="p">.</span><span class="nc">PartitionSpec</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">))</span>

<span class="c1"># We create a matrix W and input activations In sharded across our devices.
</span><span class="n">In</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2048</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="nc">P</span><span class="p">(</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">))</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">2048</span><span class="p">,</span> <span class="mi">8192</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="nc">P</span><span class="p">(</span><span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">,</span> <span class="bp">None</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">matmul_square</span><span class="p">(</span><span class="n">In</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bd,df-&gt;bf</span><span class="sh">'</span><span class="p">,</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">In</span><span class="p">),</span> <span class="n">W</span><span class="p">)</span>

<span class="c1"># We can explicitly compile the sharded matmul function here. This adds all the
# necessary comms (e.g. an AllReduce after the matmul).
</span><span class="n">jit_matmul</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">jit</span><span class="p">(</span><span class="n">matmul_square</span><span class="p">,</span> <span class="n">out_shardings</span><span class="o">=</span><span class="nc">P</span><span class="p">(</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)).</span><span class="nf">lower</span><span class="p">(</span><span class="n">In</span><span class="p">,</span> <span class="n">W</span><span class="p">).</span><span class="nf">compile</span><span class="p">()</span>

<span class="n">out</span> <span class="o">=</span> <span class="nf">jit_matmul</span><span class="p">(</span><span class="n">In</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
</code></pre></div></div> <p>This will run automatically with any sharding and partition the computation across our devices. <strong>But what’s actually happening at the hardware level?</strong></p> <ol> <li>First we create In and W sharded across our devices<d-footnote>Notice how we did this. This is one way to create an array with a particular sharding (i.e. by adding the device argument to the creation function). Another one is to create an array normally with `jnp.array(....)` and then do e.g. `jax.device_put(..., P(‘x', ‘y'))`. Yet another is to write a function which creates the array you want, and jit-compile it with `out_shardings` being what you want.</d-footnote>. W is sharded 2 way along the contracting dimension, while In is sharded 4-ways (along both the contracting and output dimensions). This corresponds to a sharding W[D<sub>X</sub>, F] and In[B<sub>X</sub>, D<sub>Y</sub>], aka a kind of model and data parallelism.</li> <li>If we were running this locally (i.e. on one device), <code class="language-plaintext highlighter-rouge">matmul_square</code> would simply square the input and perform a simple matmul. But because we specify the <code class="language-plaintext highlighter-rouge">out_shardings</code> as <code class="language-plaintext highlighter-rouge">P(‘X', None)</code>, our output will be sharded along the batch but replicated across the model dimension and will require an AllReduce to compute.</li> </ol> <p>Using our notation from previous sections, this will likely do something like</p> <ol> <li>Out[B<sub>X</sub>, F] { U<sub>Y</sub> } = In[B<sub>X</sub>, D<sub>Y</sub>] *<sub>D</sub> W[D<sub>Y</sub>, F]</li> <li>Out[B<sub>X</sub>, F] = <strong>AllReduce</strong>(Out[B<sub>X</sub>, F] { U<sub>Y</sub> })</li> </ol> <p><code class="language-plaintext highlighter-rouge">jax.jit</code> will add this for us automatically! We can actually print the HLO with <code class="language-plaintext highlighter-rouge">jit_matmul.as_text()</code> and see the following HLO (abbreviated dramatically):</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This fusion is the actual matmul of the sharded inputs and matrix
</span><span class="o">%</span><span class="n">fusion</span> <span class="o">=</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">8192</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="nc">T</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">128</span><span class="p">)(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="nc">S</span><span class="p">(</span><span class="mi">1</span><span class="p">)}</span> <span class="nf">fusion</span><span class="p">(</span><span class="n">bf16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">1024</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="nc">T</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">128</span><span class="p">)(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)}</span> <span class="o">%</span><span class="n">param</span><span class="p">,</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">8192</span><span class="p">,</span><span class="mi">1024</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="nc">T</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">128</span><span class="p">)(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="nc">S</span><span class="p">(</span><span class="mi">1</span><span class="p">)}</span> <span class="o">%</span><span class="n">copy</span><span class="o">-</span><span class="n">done</span><span class="p">)</span>

<span class="c1"># We reduce the partially summed results across devices
</span><span class="n">ROOT</span> <span class="o">%</span><span class="n">AllReduce</span> <span class="o">=</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">8192</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="nc">T</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">128</span><span class="p">)(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)}</span> <span class="nc">AllReduce</span><span class="p">(</span><span class="n">bf16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">8192</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="nc">T</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">128</span><span class="p">)(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="nc">S</span><span class="p">(</span><span class="mi">1</span><span class="p">)}</span> <span class="o">%</span><span class="n">fusion</span><span class="p">)</span>
</code></pre></div></div> <p>We can see the matmul (the fusion) and the AllReduce above. Pay particular attention to the shapes. <code class="language-plaintext highlighter-rouge">bf16[4, 1024]</code> is a local view of the activations, since our <code class="language-plaintext highlighter-rouge">batch_size=8</code> is split across 2 devices and our <code class="language-plaintext highlighter-rouge">d_model=2048</code> is likewise split 2 ways.</p> <p><strong>This is pretty magical!</strong> No matter how complicated our program is, GSPMD and jit will attempt to find shardings for all the intermediate activations and add communication as needed. With that said, GSPMD has its flaws. It can make mistakes. Sometimes you’ll look at a profile and notice something has gone wrong. A giant AllGather takes up 80% of the profile, where it doesn’t need to. When this happens, we can try to correct the compiler by explicitly annotating intermediate tensors with <code class="language-plaintext highlighter-rouge">jax.lax.with_sharding_constraint</code>. For instance, with two matmuls I can force the intermediate activations to be sharded along the <code class="language-plaintext highlighter-rouge">y</code> dimension (not that this is a good idea) with the following:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>

<span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Win</span><span class="p">,</span> <span class="n">Wout</span><span class="p">):</span>
  <span class="n">hidden</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bd,df-&gt;bf</span><span class="sh">'</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">Win</span><span class="p">)</span>
  <span class="n">hidden</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">with_sharding_constraint</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="nc">P</span><span class="p">(</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bf,df-&gt;bd</span><span class="sh">'</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">Wout</span><span class="p">)</span>
</code></pre></div></div> <p>This makes up like 60% of JAX parallel programming in the jit world, since it’s our only way of intervening with the compiler. It’s worth playing around with <code class="language-plaintext highlighter-rouge">with_sharding_constraint</code> in a Colab and getting a sense for how it works. When we write LLMs using <code class="language-plaintext highlighter-rouge">jax.jit</code>, 90% of what we do to control shardings is changing the input and output shardings (via <code class="language-plaintext highlighter-rouge">in_shardings</code> and <code class="language-plaintext highlighter-rouge">out_shardings</code>) and annotating intermediate tensors with <code class="language-plaintext highlighter-rouge">with_sharding_constraint</code> to ensure the correct comms are happening. For more jax.jit examples, <a href="https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html" rel="external nofollow noopener" target="_blank">this is a great doc to read</a>.</p> <h3 id="shard-map-explicit-parallelism-control-over-a-program">shard_map: explicit parallelism control over a program</h3> <p>While GSPMD is the “compiler take the wheel” mode, jax <a href="https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html" rel="external nofollow noopener" target="_blank">shard_map</a> puts everything in your hands. You specify the sharding of the inputs, like in jax.jit, but then you write all communication explicitly. Whereas <code class="language-plaintext highlighter-rouge">jax.jit</code> leaves you with a global cross-device view of the program, <code class="language-plaintext highlighter-rouge">shard_map</code> gives you a local per-device view.</p> <p>Here’s an example. Try to reason about what this function does:<d-footnote>If you want to play with this yourself in a colab by emulating a mesh, you can do so using the following cell `import os; os.environ["XLA_FLAGS"] = '--xla_force_host_platform_device_count=8'`</d-footnote></p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">import</span> <span class="n">jax.lax</span>
<span class="kn">import</span> <span class="n">jax.sharding</span> <span class="k">as</span> <span class="n">shd</span>

<span class="kn">from</span> <span class="n">jax.experimental.shard_map</span> <span class="kn">import</span> <span class="n">shard_map</span> <span class="k">as</span> <span class="n">shmap</span>

<span class="n">P</span> <span class="o">=</span> <span class="n">shd</span><span class="p">.</span><span class="n">PartitionSpec</span>
<span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">make_mesh</span><span class="p">(</span><span class="n">axis_shapes</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">axis_names</span><span class="o">=</span><span class="p">(</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="nc">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="nc">P</span><span class="p">((</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">))))</span>

<span class="c1"># This function will operate on 1/8th of the array.
</span><span class="k">def</span> <span class="nf">slice_and_average</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">assert</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">512</span> <span class="o">//</span> <span class="mi">8</span><span class="p">,)</span>
  <span class="k">return</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">pmean</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="n">axis_name</span><span class="o">=</span><span class="p">(</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">))</span>

<span class="n">out</span> <span class="o">=</span> <span class="nf">shmap</span><span class="p">(</span><span class="n">slice_and_average</span><span class="p">,</span> <span class="n">mesh</span><span class="p">,</span> <span class="n">in_specs</span><span class="o">=</span><span class="nc">P</span><span class="p">((</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">)),</span> <span class="n">out_specs</span><span class="o">=</span><span class="nc">P</span><span class="p">(</span><span class="bp">None</span><span class="p">,))(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">out</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">4</span><span class="p">,)</span>
</code></pre></div></div> <p><strong>What does this do?</strong> <code class="language-plaintext highlighter-rouge">slice_and_average</code> is run on each TPU with 1/8th of the array, from which we slice the first 4 elements and average them across the full mesh. This means we’re effectively doing <code class="language-plaintext highlighter-rouge">mean(x[:4], x[64:68], x[128:132], …)</code>. This is pretty cool, because that’s not an easy operation to express in JAX otherwise.</p> <p><strong>Why do this instead of jax.jit?</strong> If we’d used <code class="language-plaintext highlighter-rouge">jax.jit</code>, <code class="language-plaintext highlighter-rouge">slice_and_average</code> would have seen a global view of the array (the full <code class="language-plaintext highlighter-rouge">[512,]</code> array). We’d have had to slice out this non-uniform slice and then perform an average which XLA would have had to interpret correctly. XLA might have added the wrong communication or gotten confused. Here we see the local view and write only the communication we need.</p> <p><strong>Example [Collective Matmul]:</strong> To take a more realistic example, say we to implement model parallelism where the activations are initially model sharded, i.e. A[B<sub>X</sub>, D<sub>Y</sub>] * W[D, F<sub>Y</sub>] -&gt; Out[B<sub>X</sub>, F<sub>Y</sub>]. Naively, we would do this by AllGathering A first followed by a local matrix multiplication:</p> <ol> <li>A[B<sub>X</sub>, D] = <strong>AllGather</strong><sub>Y</sub>(A[B<sub>X</sub>, D<sub>Y</sub>])</li> <li>Out[B<sub>X</sub>, F<sub>Y</sub>] = A[B<sub>X</sub>, D] *<sub>D</sub> W[D, F<sub>Y</sub>]</li> </ol> <p>Sadly, this is bad because it doesn’t allow us to overlap the communication with the computation. Overlapping them can be done with a “collective matmul”, as described in <a href="https://dl.acm.org/doi/pdf/10.1145/3567955.3567959" rel="external nofollow noopener" target="_blank">Wang et al. 2023</a>. The algorithm is basically as follows:</p> <ul> <li>For each Y shard, perform a matmul of the local chunk of A with the local chunk of W, producing a result of shape <code class="language-plaintext highlighter-rouge">[B / X, F / Y]</code>. Simultaneously, permute A so you get the next chunk locally, perform the matmul, and sum the result.</li> </ul> <p>We can implement that quite easily with shard_map:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">functools</span>

<span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">import</span> <span class="n">jax.sharding</span> <span class="k">as</span> <span class="n">shd</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="kn">from</span> <span class="n">jax.experimental.shard_map</span> <span class="kn">import</span> <span class="n">shard_map</span>

<span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">make_mesh</span><span class="p">(</span><span class="n">axis_shapes</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">axis_names</span><span class="o">=</span><span class="p">(</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">P</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">shd</span><span class="p">.</span><span class="nc">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">shd</span><span class="p">.</span><span class="nc">PartitionSpec</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">))</span>

<span class="n">B</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">F</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="mi">8192</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">prod</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">D</span><span class="p">))).</span><span class="nf">reshape</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">D</span><span class="p">))</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">prod</span><span class="p">((</span><span class="n">D</span><span class="p">,</span> <span class="n">F</span><span class="p">))).</span><span class="nf">reshape</span><span class="p">((</span><span class="n">D</span><span class="p">,</span> <span class="n">F</span><span class="p">))</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">device_put</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="nc">P</span><span class="p">(</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">))</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">device_put</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="nc">P</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">))</span>

<span class="nd">@functools.partial</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">out_shardings</span><span class="o">=</span><span class="nc">P</span><span class="p">(</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="n">lhs</span><span class="p">,</span> <span class="n">rhs</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">lhs</span> <span class="o">@</span> <span class="n">rhs</span>

<span class="k">def</span> <span class="nf">collective_matmul_allgather_lhs_contracting</span><span class="p">(</span><span class="n">lhs</span><span class="p">,</span> <span class="n">rhs</span><span class="p">):</span>
  <span class="c1"># lhs is the looped operand; rhs is the local operand
</span>  <span class="n">axis_size</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">psum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">)</span>  <span class="c1"># axis_size = 4 for this example
</span>  <span class="n">idx</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">axis_index</span><span class="p">(</span><span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">)</span>

  <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">lhs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="k">assert</span> <span class="n">rhs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="n">chunk_size</span> <span class="o">==</span> <span class="mi">0</span>

  <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">carrys</span><span class="p">):</span>
    <span class="n">accum</span><span class="p">,</span> <span class="n">lhs</span> <span class="o">=</span> <span class="n">carrys</span>
    <span class="n">rhs_chunk</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">dynamic_slice_in_dim</span><span class="p">(</span><span class="n">rhs</span><span class="p">,</span> <span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> <span class="o">%</span> <span class="n">axis_size</span> <span class="o">*</span> <span class="n">chunk_size</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">)</span>
    <span class="c1"># Matmul for a chunk
</span>    <span class="n">update</span> <span class="o">=</span> <span class="n">lhs</span> <span class="o">@</span> <span class="n">rhs_chunk</span>
    <span class="c1"># Circular shift to the left
</span>    <span class="n">lhs</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">ppermute</span><span class="p">(</span>
        <span class="n">lhs</span><span class="p">,</span>
        <span class="n">axis_name</span><span class="o">=</span><span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">,</span>
        <span class="n">perm</span><span class="o">=</span><span class="p">[(</span><span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">axis_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">axis_size</span><span class="p">)]</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">accum</span> <span class="o">+</span> <span class="n">update</span><span class="p">,</span> <span class="n">lhs</span>

  <span class="n">accum</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">lhs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">rhs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">lhs</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">accum</span><span class="p">,</span> <span class="n">lhs</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">fori_loop</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">axis_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">accum</span><span class="p">,</span> <span class="n">lhs</span><span class="p">),</span> <span class="n">unroll</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

  <span class="c1"># Compute the last chunk after the final permute to leave lhs in the state we found it
</span>  <span class="n">i</span> <span class="o">=</span> <span class="n">axis_size</span> <span class="o">-</span> <span class="mi">1</span>
  <span class="n">rhs_chunk</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">dynamic_slice_in_dim</span><span class="p">(</span><span class="n">rhs</span><span class="p">,</span> <span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> <span class="o">%</span> <span class="n">axis_size</span> <span class="o">*</span> <span class="n">chunk_size</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">)</span>
  <span class="n">update</span> <span class="o">=</span> <span class="n">lhs</span> <span class="o">@</span> <span class="n">rhs_chunk</span>
  <span class="k">return</span> <span class="n">accum</span> <span class="o">+</span> <span class="n">update</span>

<span class="n">jit_sharded_f</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">jit</span><span class="p">(</span><span class="nf">shard_map</span><span class="p">(</span>
  <span class="n">collective_matmul_allgather_lhs_contracting</span><span class="p">,</span> <span class="n">mesh</span><span class="p">,</span>
  <span class="n">in_specs</span><span class="o">=</span><span class="p">(</span><span class="n">shd</span><span class="p">.</span><span class="nc">PartitionSpec</span><span class="p">(</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">),</span> <span class="n">shd</span><span class="p">.</span><span class="nc">PartitionSpec</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">)),</span> <span class="n">out_specs</span><span class="o">=</span><span class="n">shd</span><span class="p">.</span><span class="nc">PartitionSpec</span><span class="p">(</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">)))</span>

<span class="n">shmapped_out</span> <span class="o">=</span> <span class="nf">jit_sharded_f</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
<span class="n">expected_out</span> <span class="o">=</span> <span class="nf">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>

<span class="n">np</span><span class="p">.</span><span class="n">testing</span><span class="p">.</span><span class="nf">assert_array_equal</span><span class="p">(</span><span class="n">shmapped_out</span><span class="p">,</span> <span class="n">expected_out</span><span class="p">)</span>
</code></pre></div></div> <p>This is pretty neat! We can benchmark this and see that it’s also a lot faster! <a href="https://imgur.com/a/e9I6SrM" rel="external nofollow noopener" target="_blank">Here’s</a> the profile with the default jit matmul which takes 311us with a big blocking AllGather at the beginning:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/not-overlapped-480.webp 480w,/scaling-book/assets/img/not-overlapped-800.webp 800w,/scaling-book/assets/img/not-overlapped-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/not-overlapped.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>And <a href="https://imgur.com/a/21iy0Sv" rel="external nofollow noopener" target="_blank">here’s</a> the version above that takes 244 us. You can see the profile doesn’t have the AllGather. It’s all useful work! Our FLOPs utilization is also a lot higher.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/overlapped-480.webp 480w,/scaling-book/assets/img/overlapped-800.webp 800w,/scaling-book/assets/img/overlapped-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/overlapped.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>It’s also worth noting that the matmul time with no sharding on the contracting dimension is <a href="https://imgur.com/a/i3gNKfq" rel="external nofollow noopener" target="_blank">224us</a>, so we’re remarkably close to the unsharded baseline here. This is a good example of the kind of performance engineering you might end up doing to improve TPU utilization. For more <code class="language-plaintext highlighter-rouge">shard_map</code> examples, <a href="https://jax.readthedocs.io/en/latest/notebooks/shard_map.html#example-1-all-gather-on-one-side" rel="external nofollow noopener" target="_blank">this note is great</a>.</p> <p>Now here are a couple of useful worked problems to try and implement using <code class="language-plaintext highlighter-rouge">jax.jit</code> or <code class="language-plaintext highlighter-rouge">shard_map</code>!</p> <h2 id="worked-problems">Worked Problems</h2> <p>Here are some random JAX-related problems. I’ll add some more later. For all of these, you’ll need some number of TPUs in a Colab. You can use a public Colab with TPUv2-8. From now on, we’ll assume you have N devices available.</p> <p><strong>Problem 1:</strong> For the next several parts, we’ll let <strong>A</strong> be an array of activations of shape float32[S<sub>X</sub>, D<sub>Y</sub>] with <code class="language-plaintext highlighter-rouge">X * Y = N</code>. Do the following:</p> <ol> <li> <p>Write a function in JAX that computes the average over each <code class="language-plaintext highlighter-rouge">X</code> shard, i.e. it returns an array of size [X, D<sub>Y</sub>] where <code class="language-plaintext highlighter-rouge">arr[i]</code> is the average over shard <code class="language-plaintext highlighter-rouge">i</code>. Do this with both <code class="language-plaintext highlighter-rouge">jax.jit</code> and <code class="language-plaintext highlighter-rouge">shard_map</code>. Profile each and see how long they took. Was there any communication added? <em>Hint: there shouldn’t be, but sometimes XLA adds it anyway.</em> <a href="https://pastecode.io/s/0v603d9o" rel="external nofollow noopener" target="_blank"><em>Here’s the answer.</em></a></p> </li> <li> <p>Write a function in JAX that returns roll(x, shift) - x for some shift <strong>within each shard X</strong>. I’m not enough of a masochist to make you do this in jax.jit, so just do this with <code class="language-plaintext highlighter-rouge">shard_map</code>.</p> </li> </ol> <p><strong>Problem 2:</strong> Here we’ll make a basic “mixture of experts” model together. Let <strong>W</strong>: float32[E<sub>X</sub>, D, F<sub>Y</sub>] be a set of E “expert” matrices. Let <strong>A</strong> be as above (our activations) and let <strong>B</strong> be a set of “routing assignments” where B[i] is an integer in the range <code class="language-plaintext highlighter-rouge">[0, E)</code> telling us which matrix we want to process that activation. We want to write a function in JAX that returns <code class="language-plaintext highlighter-rouge">Out[i] = W[B[i]] @ A[i]</code>.</p> <ol> <li> <p>Let’s start by ignoring sharding altogether. Make all of these tensors small enough so they fit in one device. Write a local implementation of this function. <em>Make sure you don’t materialize an array of shape <code class="language-plaintext highlighter-rouge">[S, D, F]</code>! Hint: try sorting the tokens into a new buffer of shape <code class="language-plaintext highlighter-rouge">[E, S, D]</code> with some attention to masking (why do we need the second dimension to have size S?).</em></p> </li> <li> <p>If you just <code class="language-plaintext highlighter-rouge">jax.jit</code> the above method, something will happen. Profile this and see what communication it decided to do. How long does it take?</p> </li> <li> <p>One problem you’ll notice with the above is that it likely gathers the full set of activations <strong>A</strong> locally, i.e. AllGather<sub>X</sub>([S<sub>X</sub>, D<sub>Y</sub>]), Not only is this expensive communication-wise, it’s also incredibly expensive memory-wise if we can’t fit the full set of activations locally. Implement the above using <code class="language-plaintext highlighter-rouge">shard_map</code> and explicit communication.</p> <ol> <li> <p>For a first pass, it might be easiest to use a <code class="language-plaintext highlighter-rouge">jax.lax.all_gather</code> and reorder as in (a).</p> </li> <li> <p>For a second pass, try to avoid materializing any array of size <code class="language-plaintext highlighter-rouge">[E, S, D]</code>, i.e. try to perform the computation in a ragged fashion using a <code class="language-plaintext highlighter-rouge">jax.lax.all_to_all</code> inside a <code class="language-plaintext highlighter-rouge">jax.lax.while_loop</code>. This way, you can avoid materializing the full activations and wasting compute on padding. How much faster is this than your original implementation?</p> </li> </ol> </li> <li> <p>Most MoEs route to multiple (k) experts and then average the result. Refactor the above to implement this. Let <strong>B</strong>: int32[S, k] in this case for the k experts to route to.</p> </li> </ol> <p><strong>Problem 3:</strong> The collective matmul example above is actually super relevant for real LLMs. Let’s tweak the example to do the full Transformer stack.</p> <ol> <li> <p>As an exercise, let’s start by implementing an AllReduce collective matmul, i.e. A[B<sub>X</sub>, D<sub>Y</sub>] *<sub>D</sub> W[D<sub>Y</sub>, F] -&gt; Out[B<sub>X</sub>, F]. Note that the output isn’t replicated. The naive algorithm is discussed above, basically just a local matmul followed by an AllReduce. Try to make a comms overlapped “collective” version of this operation. <em>Hint: tile over the output dimension and feel free to use <code class="language-plaintext highlighter-rouge">jax.lax.psum</code> (aka AllReduce).</em> <em>Note: due to the way XLA handles this, it may not actually be faster than the baseline.</em></p> </li> <li> <p>The complement to the AllReduce collective matmul above is a ReduceScatter collective matmul, as in Tmp[B<sub>X</sub>, F<sub>Y</sub>] *<sub>F</sub> W2[F<sub>Y</sub>, D] -&gt; Out[B<sub>X</sub>, D<sub>Y</sub>]. This occurs in the down-projection matrix in a Transformer. Implement a collective, overlapped version of this in JAX. Be careful about passing only the minimal amount of data you need. <em>Hint: try permuting the result as you accumulate it.</em></p> </li> <li> <p>Put these two together into an end-to-end Transformer block that performs In[B<sub>X</sub>, D<sub>Y</sub>] *<sub>D</sub> W<sub>in</sub>[D, F<sub>Y</sub>] *<sub>F</sub> W<sub>out</sub>[F<sub>Y</sub>, D] -&gt; Out[B<sub>X</sub>, D<sub>Y</sub>] with overlapped communication.<d-footnote>As before, we can't do $W_{in} \cdot W_{out}$ first because of a non-linearity we've omitted here.</d-footnote> How much faster is this than a <code class="language-plaintext highlighter-rouge">jax.jit</code> implementation?</p> </li> </ol> <p><strong>Problem 4:</strong> All of the collective matmuls implemented above are unidirectional: they only permute in one direction. Rewrite the collective AllReduce matmul and the collective ReduceScatter matmuls to use bidirectional communication. How much faster are these?</p> <h3 id="thats-all-for-part-10-thats-basically-it-for-final-conclusions-and-further-reading-click-here">That’s all for Part 10. That’s basically it! For final conclusions and further reading, click <a href="../conclusion">here</a>.</h3> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> <div class="base-grid appendix-entry"> <h3 style="grid-column: 0;">Miscellaneous</h3> <p class="author-footnote" style="grid-column: text;"><sup>*</sup>Work done at Google DeepMind, now at MatX.</p> </div> <div class="base-grid appendix-entry"> <h3 style="grid-column: 0;">Citation</h3> <p class="author-footnote">For attribution in academic contexts, please cite this work as:</p> <div class="author-footnote"> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c">Austin et al., "How to Scale Your Model", Google DeepMind, online, 2025.</span>
</code></pre></div></div> </div> <p class="author-footnote">or as a BibTeX entry:</p> <div class="author-footnote"> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="nc">@article</span><span class="p">{</span><span class="nl">scaling-book</span><span class="p">,</span>
      <span class="na">title</span> <span class="p">=</span> <span class="s">{How to Scale Your Model}</span><span class="p">,</span>
      <span class="na">author</span> <span class="p">=</span> <span class="s">{Austin, Jacob and Douglas, Sholto and Frostig, Roy and Levskaya, Anselm and Chen, Charlie and Vikram, Sharad
      and Lebron, Federico and Choy, Peter and Ramasesh, Vinay and Webson, Albert and Pope, Reiner}</span><span class="p">,</span>
      <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Google DeepMind}</span><span class="p">,</span>
      <span class="na">howpublished</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
      <span class="na">note</span> <span class="p">=</span> <span class="s">{Retrieved from https://jax-ml.github.io/scaling-book/}</span><span class="p">,</span>
      <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
    <span class="p">}</span>
</code></pre></div></div> </div> </div> </d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'jax-ml/scaling-book',
        'data-repo-id': '',
        'data-category': 'General',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '0',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-loading': '1',
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/scaling-book/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/scaling-book/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/scaling-book/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/scaling-book/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/scaling-book/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/scaling-book/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/scaling-book/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/scaling-book/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/scaling-book/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>