<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> All About Transformer Inference | How To Scale Your Model </title> <meta name="author" content=" "> <meta name="description" content="Performing inference on a Transformer can be very different from training. Partly this is because inference adds a new factor to consider: latency. In this section, we will go all the way from sampling a single new token from a model to efficiently scaling a large Transformer across many slices of accelerators as part of an inference engine."> <meta name="keywords" content="scaling, jax, llms, transformers, tpus, google, deepmind, parallelism, pallas"> <link rel="stylesheet" href="/scaling-book/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/scaling-book/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/scaling-book/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/scaling-book/assets/css/jekyll-pygments-themes-vs.css?4ee1a2facd1a8a76347f4bd43a740500" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/scaling-book/assets/img/favicon.png?fddbd8c2ec231ba2060e67c85de32a55"> <link rel="stylesheet" href="/scaling-book/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jax-ml.github.io/scaling-book/inference/"> <script src="/scaling-book/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/scaling-book/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/scaling-book/assets/js/distillpub/template.v2.js"></script> <script src="/scaling-book/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">{{page._styles}}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "All About Transformer Inference",
            "description": "Performing inference on a Transformer can be very different from training. Partly this is because inference adds a new factor to consider: latency. In this section, we will go all the way from sampling a single new token from a model to efficiently scaling a large Transformer across many slices of accelerators as part of an inference engine.",
            "published": "February 04, 2025",
            "authors": [
              
              {
                "author": "Jacob Austin",
                "authorURL": "https://www.jacobaustin.org/",
                "affiliations": [
                  {
                    "name": "Google DeepMind",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Sholto Douglas",
                "authorURL": "https://x.com/_sholtodouglas",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Roy Frostig",
                "authorURL": "https://cs.stanford.edu/~rfrostig/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Anselm Levskaya",
                "authorURL": "https://anselmlevskaya.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Charlie Chen",
                "authorURL": "https://x.com/charliexychen",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Sharad Vikram",
                "authorURL": "https://sharadvikram.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Federico Lebron",
                "authorURL": "https://fedelebron.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Peter Choy",
                "authorURL": "https://x.com/pchoy95",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Vinay Ramasesh",
                "authorURL": "https://x.com/vinayramasesh",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Albert Webson",
                "authorURL": "https://representation.ai/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Reiner Pope<sup>*</sup>",
                "authorURL": "https://x.com/reinerpope",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <script>
    function goToTop() {
      document.body.scrollTop = 0; // For Safari
      document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
    }

    // When the user scrolls down 20px from the top of the document, show the button
    window.onscroll = function() {scrollFunction()};

    function scrollFunction() {
      // Get the button:
      let mybutton = document.getElementById("top-button");

      if (document.body.scrollTop > 40 || document.documentElement.scrollTop > 40) {
        mybutton.style.display = "block";
      } else {
        mybutton.style.display = "none";
      }
  }
  </script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/scaling-book"> How To Scale Your Model </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="left-button section-button"><a href="../applied-training"><svg viewbox="-78.5 0 512 512"><path d="M257 64L291 98 128 262 291 426 257 460 61 262 257 64Z"></path></svg></a></div> <div class="right-button section-button"><a href="../applied-inference"><svg viewbox="-78.5 0 512 512"><path d="M98 460L64 426 227 262 64 98 98 64 294 262 98 460Z"></path></svg></a></div> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/scaling-book/"> </a> </li> <li class="nav-item nav-hidden"><a class="nav-link" onclick="goToTop()" id="top-button" style="display: none;">Back to Top</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../applied-training">Previous Part</a></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../applied-inference">Next Part</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Sections </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/scaling-book/index">Part 0. Introduction</a> <a class="dropdown-item " href="/scaling-book/roofline">Part 1. Intro to Rooflines</a> <a class="dropdown-item " href="/scaling-book/tpus">Part 2. All About TPUs</a> <a class="dropdown-item " href="/scaling-book/sharding">Part 3. Sharded Matmuls</a> <a class="dropdown-item " href="/scaling-book/transformers">Part 4. Transformers</a> <a class="dropdown-item " href="/scaling-book/training">Part 5. Training</a> <a class="dropdown-item " href="/scaling-book/applied-training">Part 6. Training LLaMA</a> <a class="dropdown-item " href="/scaling-book/inference">Part 7. Inference</a> <a class="dropdown-item " href="/scaling-book/applied-inference">Part 8. Serving LLaMA</a> <a class="dropdown-item " href="/scaling-book/profiling">Part 9. Profiling</a> <a class="dropdown-item " href="/scaling-book/jax-stuff">Part 10. All About JAX</a> <a class="dropdown-item " href="/scaling-book/conclusion">Part 11. Conclusions</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>All About Transformer Inference</h1> <p>Part 7 of <a href="/scaling-book">How To Scale Your Model</a> (<a href="../applied-training">Part 6: Training LLaMA</a> | <a href="../applied-inference">Part 8: Serving LLaMA</a>)</p> <p>Performing inference on a Transformer can be very different from training. Partly this is because inference adds a new factor to consider: latency. In this section, we will go all the way from sampling a single new token from a model to efficiently scaling a large Transformer across many slices of accelerators as part of an inference engine.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#the-basics-of-transformer-inference">The Basics of Transformer Inference</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#what-do-we-actually-want-to-optimize">What do we actually want to optimize?</a> </li> <li> <a href="#linear-operations-what-bottlenecks-us">Linear operations: what bottlenecks us?</a> </li> <li> <a href="#what-about-attention">What about attention?</a> </li> <li> <a href="#theoretical-estimates-for-llm-latency-and-throughput">Theoretical estimates for LLM latency and throughput</a> </li> <li> <a href="#what-about-memory">What about memory?</a> </li> <li> <a href="#modeling-throughput-and-latency-for-llama-2-13b">Modeling throughput and latency for LLaMA 2-13B</a> </li> </ul> <div> <a href="#tricks-for-improving-generation-throughput-and-latency">Tricks for Improving Generation Throughput and Latency</a> </div> <div> <a href="#distributing-inference-over-multiple-accelerators">Distributing Inference Over Multiple Accelerators</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#prefill">Prefill</a> </li> <li> <a href="#generation">Generation</a> </li> <li> <a href="#sharding-the-kv-cache">Sharding the KV cache</a> </li> </ul> <div> <a href="#designing-an-effective-inference-engine">Designing an Effective Inference Engine</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#continuous-batching">Continuous Batching</a> </li> <li> <a href="#prefix-caching">Prefix Caching</a> </li> <li> <a href="#let-s-look-at-an-implementation-jetstream">Let's look at an implementation: JetStream</a> </li> </ul> <div> <a href="#worked-problems">Worked Problems</a> </div> <div> <a href="#appendix">Appendix</a> </div> </nav> </d-contents> <h2 id="the-basics-of-transformer-inference">The Basics of Transformer Inference</h2> <p>So you’ve trained a Transformer, and you want to use it to generate some new sequences. <em>At the end of the day, benchmark scores going up and loss curves going down are only proxies for whether something interesting is going to happen once the rubber hits the road!</em><d-footnote>Historically, you can do a surprising amount of research on Transformers without ever touching inference — LLM loss, multiple choice benchmarks can be run efficiently without a proper KV cache or generation loop implementation. This meant, especially in research codebases, there's often a lot of low hanging fruits in the inference codepath.</d-footnote></p> <p>Sampling is conceptually simple. We put a sequence in and our favorite Transformer will spit out \(\log p(\text{next token}_i \vert \text{previous tokens})\), i.e. log-probabilities for all possible next tokens. We can sample from this distribution and obtain a new token. Append this token and repeat this process and we obtain a sequence of tokens which is a continuation of the prompt.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/naive-inference-480.webp 480w,/scaling-book/assets/img/naive-inference-800.webp 800w,/scaling-book/assets/img/naive-inference-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/naive-inference.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure:</b> naive sampling from a Transformer. The blue logits give us a distribution over the next token that we can sample from. Note that each step re-processes the entire prefix, leading to a $\Theta(n^2)$ runtime for the algorithm.</figcaption> </figure> <p>We have just described the naive implementation of Transformer sampling, and while it works, <strong>we never do it in practice</strong> because we are re-processing the entire sequence every time we generate a token. This algorithm is \(O(n^2)\) on the FFW and \(O(n^3)\) on the attention mechanism to generate \(n\) tokens!</p> <p><strong>How do we avoid this?</strong> Instead of doing the full forward pass every time, it turns out we can save some intermediate activations from each forward pass that let us avoid re-processing previous tokens. Specifically, since a given token only attends to previous tokens during dot-product attention, we can simply write each token’s key and value projections into a new data structure called a <strong>KV cache</strong>. Once we’ve saved these key/value projections for past tokens, future tokens can simply compute their \(q_i \cdot k_j\) products without performing any new FLOPs on the earlier tokens. Amazing!</p> <p>With this in mind, inference has two key parts:</p> <ul> <li> <b style="color: red;">Prefill</b>: Given a long prompt, we process all the tokens in the prompt at the same time and save the resulting activations (specifically, the key-value projections) in a <strong>“KV cache”</strong>. We also save the logits for the last token.</li> <li> <b style="color: blue;">Generation</b>: Given a KV cache and the previous logits, we incrementally sample one token from the logits, feed that token back into the Transformer, and produce a new set of logits for the next step. We also append the KV activations for that new token to the KV cache. We repeat this until we hit a special <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> token or reach some maximum length limit.</li> </ul> <p>Here’s a diagram of sampling with a KV cache:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/cached-inference-480.webp 480w,/scaling-book/assets/img/cached-inference-800.webp 800w,/scaling-book/assets/img/cached-inference-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/cached-inference.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure:</b> diagram of efficient Transformer sampling with a KV cache. <b style="color: red;">Prefill</b> processes our prompt and saves all the per-token key-value activations in a cache. <b style="color: blue;">Generation</b> takes this cache (and the last-token logits), samples a new token, and passes that new token through the model, attending to the KV cache and saving the new token's key-value projections back to the cache. This is an $O(n)$ algorithm in the MLP block.</figcaption> </figure> <p>By sampling with a KV cache, we’ve reduced our time complexity to generate $n$ tokens to \(O(n)\) on the FFW and \(O(n^2)\) on the attention, since we never reprocess a previous token. However, many forward passes are still needed to generate a sequence — that’s what’s happening when you query Gemini or ChatGPT and the result streams back to you. Every token is (usually) a separate (but partially cached) Transformer call to a massive model.</p> <p>We will soon see that <b style="color: red;">prefill</b> and <b style="color: blue;">generation</b> are very different beasts —— Transformer inference is two tasks in disguise! Compared to training, the KV cache is also a novel and significant source of complexity.</p> <h3 id="what-do-we-actually-want-to-optimize">What do we actually want to optimize?</h3> <p>Before we proceed further, it’s worth highlighting one aspect of inference that’s totally new: latency. While during training we only care about throughput (total tokens processed per second), during inference we have to worry about how fast we’re producing tokens (both the <strong>Time To First Token (TTFT)</strong> and the <strong>per-token latency</strong>). For example:</p> <ul> <li> <strong>Offline batch inference</strong> for evals and data generation only cares about bulk cost of inference and is blind to the latency of individual samples.</li> <li> <strong>Chat interfaces/streaming tasks</strong> need to run cheaply at scale while having low TTFT and generating tokens fast enough to exceed human reading speed.</li> <li> <strong>Edge inference</strong> (e.g. <code class="language-plaintext highlighter-rouge">llama.cpp</code> on your laptop) only needs to service one user at a time at the lowest possible latency, potentially with heavy hardware constraints.</li> </ul> <p>Maximizing hardware utilization is still critical and helps with cost and TTFT, but unlike training, it does not <em>necessarily</em> translate to better experience for individual users in all contexts. Many optimizations at the accelerator, systems and model architectural level make tradeoffs between latency, throughput, context length and even model quality.</p> <h3 id="a-more-granular-view-of-the-transformer">A more granular view of the Transformer</h3> <p>So far we’ve mostly treated a Transformer as a stack of feedforward blocks. While this is often reasonable from a FLOPs and memory standpoint, it’s not sufficient to properly model inference.<d-footnote>One thing you'll notice throughout this section is that inference is much less forgiving than training. We typically have far fewer FLOPs, less opportunity for batching, and a much greater sensitivity to latency. KV caches dramatically complicate inference as well.</d-footnote> As we saw in <a href="../transformers">Part 4</a>, the major components of a Transformer forward pass are:</p> <ol> <li> <strong>A bunch of linear operations</strong>, including the MLP ($W_{in}$, $W_{out}$) and the attention QKV projections and output projections ($W_Q$, $W_K$, $W_V$, and $W_O$). These all involve reading parameters and a batch of activations from HBM, doing some FLOPs, and writing the result back to HBM.</li> <li> <strong>Dot-product attention</strong>. We need to read a batch of key-value projections and a batch of query activations from HBM, do a few inner products and some softmax operations, and write the attention result back to HBM.</li> <li> <strong>Everything else</strong>, including applying layer norms, activation functions, tokens sampling, updating KV caches, and positional embeddings. These do take some FLOPs, but are dominated by, or fused into, the above.</li> </ol> <p>For the next couple of sections, we’re going to look at each of these in the context of prefill and generation and ask what is likely to bottleneck our performance. Within a single accelerator, are we compute-bound or memory-bound? We want to emphasize how different the answers will be for prefill versus generation.</p> <h3 id="linear-operations-what-bottlenecks-us">Linear operations: what bottlenecks us?</h3> <p>All our linear operations are conceptually the same, whether they live in the MLP block or attention. Their arithmetic intensity depends on the batch size. We did this math in <a href="../roofline">Section 1</a> but it’s worth repeating. Let’s look at a single matrix multiply of a $\text{bf16[B, D]}$ batch by a $\text{bf16[D, F]}$ matrix. This could be the big MLP block or one of the smaller attention projections ($W_Q$, $W_K$, $W_V$, $W_O$). To do this matrix multiplication, we need to load both of these arrays from HBM into the MXU, do the multiplicaton, then write the result back to HBM. As before, we have:</p> \[T_\text{math} = \frac{\text{Total FLOPs}}{\text{TPU FLOPs/s}} = \frac{2BDF}{\text{TPU FLOPs/s}}\] \[T_\text{comms} = \frac{\text{Total Bytes}}{\text{HBM Bandwidth}} = \frac{2BD + 2FD + 2BF}{\text{HBM Bandwidth}}\] <p>The TPU can overlap these by loading as it does the compute, so to be compute-bound, we need \(T_\text{math} \geq T_\text{comms}\), or:</p> \[\frac{2BDF}{2BD + 2DF + 2BF} \geq \frac{\text{TPU FLOPs/s}}{\text{HBM Bandwidth}} = \frac{1.97E+14}{8.20E+11} = 240\] <p>where the RHS is the arithmetic intensity of our hardware. Now let’s assume $D$ and $F$ are very large compared to $B$ (usually our batches are at most 500 and $D$ and $F &gt; 10k$), we can simplify the denominator by using the fact that $\small{2BD + 2DF + 2BF \approxeq 2DF}$ which gives us</p> \[\begin{align*} \frac{2BDF}{2BD + 2DF + BF} \approxeq \frac{2BDF}{2DF} \geq \frac{\text{TPU FLOPs/s}}{\text{HBM Bandwidth}} \\ = \frac{1.97E+14}{8.20E+11} \implies B \geq 240 = B_{\text{crit}} \end{align*}\] <p class="takeaway"><strong>Takeaway:</strong> to be compute-bound on any matrix multiplication, our total token batch size must be greater than $B_\text{crit}$, which depends on the hardware and quantization. For bf16 activations on TPU v5e, this is 240 tokens. This applies to any simple matmul in our Transformer (e.g. the MLP block or the attention projections).</p> <p>During training, we’ll have a high intensity during all our matrix multiplications because we reuse the same weights over a very large batch. <strong>That high arithmetic intensity carries over to prefill, since user prompts are typically hundreds if not thousands of tokens long.</strong> As we saw before, the hardware arithmetic intensity of a TPUv5e is 240, so if a sequence longer than 240 tokens is fed into a dense model running on this hardware at bf16, we would expect to be compute-bound and all is well. Prompts shorter than this can technically be batched together to achieve higher utilization, but this is typically not necessary.</p> <p class="takeaway"><strong>Takeaway:</strong> during prefill, all matrix multiplications are basically always compute-bound. Therefore, simply maximizing hardware utilization or MFU (Model FLOPs Utilization) is enough to maximize throughput-per-chip (cost) and latency (in the form of TTFT). Unless prompts are extremely short, batching at a per-prompt level only adds latency for a small improvements in prefill throughput.</p> <p>However, during generation, for each request, we can only do our forward passes one token at a time since there’s a sequential dependency between steps! Thus we can only (easily) achieve good utilization by batching multiple requests together, parallelizing over the batch dimension. We’ll talk about this more later, but actually batching many concurrent requests together without affecting latency is hard. For that reason, <strong>it is much harder to saturate the hardware FLOPs with generation.</strong></p> <p class="takeaway"><strong>Takeaway:</strong> our total token batch size must be greater than \(B_{\text{crit}}\) for generation to be compute-bound on the linear/feed-forward operations (240 for bf16 params on TPU v5e). Because generation happens serially, token-by-token, this requires us to batch multiple requests together, which is hard!</p> <p><em>It’s worth noting just how large this is!</em> Generate batch size of 240 means 240 concurrent requests generating at once, and 240 separate KV caches for dense models. That means this is difficult to achieve in practice, except in some bulk inference settings. In contrast, pushing more than 240 tokens through during a prefill is pretty routine, though some care is necessary as sparsity increases.</p> <p><strong>Note that this exact number will differ on the kind of quantization and hardware.</strong> Accelerators often can supply more arithmetic in lower precision. For example, if we have int8 parameters but do our computation in bf16, the critical batch size drops to 120. With int8 activations and int8 params, it jumps back up to 240 since the TPUv5e can supply 400 TOPs/s of int8 x int8.</p> <h3 id="what-about-attention">What about attention?</h3> <p>Things get more complicated when we look at the dot-product attention operation, especially since we have to account for KV caches. Let’s look at just one attention head with pure multi-headed attention. In a single Flash Attention fusion, we<d-footnote>We're simplifying a fair bit here by ignoring the non-matmul FLOPs in applying the softmax, masks etc. They should be overlapped with computation or HBM reads, but it can be non-trivial to do on certain TPU generations. Whese details don't change the main message, which is that KV caches are usually memory bound.</d-footnote>:</p> <ol> <li>Read the $Q$ activations of shape $\text{bf16[B, T, D]}$ from HBM.</li> <li>Read the $KV$ cache, which is a pair of $\text{bf16[B, S, D]}$ tensors from HBM.</li> <li>Perform $2BSTD$ FLOPs in the \(QK\) matmul. With Flash Attention, we don’t need to write the $\text{bf16[B, S, T]}$ attention matrix back into HBM.</li> <li>Perform $2BSTD$ in the attention \(AV\) matmul.</li> <li>Write the resulting $\text{bf16[B, T, D]}$ tensor back into HBM.</li> </ol> <p>Putting it all together, we get:</p> \[\text{Multiheaded Attention Arithmetic Intensity} = \frac{4BSTD}{4BSD + 4BTD} = \frac{ST}{S+T}\] <p>For prefill, $S=T$ since we’re doing self-attention, so this simplifies to $T^2 / 2T = T / 2$. This is great because it means <strong>the arithmetic intensity of attention during prefill is $\Theta(T)$</strong>. That means it’s quite easy to be compute-bound for attention. As long as our batch size <em>and sequence length</em> are both fairly large, we’ll be fine!</p> <p>But since generation has a trivial sequence dim, and the $B$ and $D$ dims cancel, we can make the approximation:</p> \[S \gg T = 1 \implies \frac{ST}{S+T} \approx 1\] <p>This is bad, since it means we cannot do anything to improve the arithmetic intensity of attention during generation. We’re doing a tiny amount of FLOPs while loading a massive KV cache. <strong>So we’re basically always memory bandwidth-bound during attention!</strong></p> <p class="takeaway"><strong>Takeaway:</strong> during prefill, attention is usually compute bound for any reasonable sequence length (roughly $\gt 480$ tokens) while during generation our arithmetic intensity is low and constant, so we are always memory bandwidth-bound.</p> <p><em>Why is this, conceptually?</em> Mainly, we’re compute-bound in linear portions of the model because the parameters (the memory bandwidth-heavy components) are reused for many batch items. However, every batch item has its own KV cache, so a bigger batch size means more KV caches. We will almost <em>always</em> be memory bound here unless the architecture is adjusted aggressively.</p> <p>This also means you will get diminishing returns on throughput from increasing batch size once params memory becomes comparable to KV cache memory. The degree to which the diminishing returns hurt you depends on the ratio of parameter to KV cache bytes for a single sequence, i.e. roughly the ratio $2DF / SHK$. Since $HK\approx D$, this roughly depends on the ratio of $F$ to $S$, the sequence length. This also depends on architectural modifications that make the KV cache smaller (we’ll say more in a moment).</p> <h3 id="theoretical-estimates-for-llm-latency-and-throughput">Theoretical estimates for LLM latency and throughput</h3> <p>From this math, we can get pretty good bounds on the step time we should aim for when optimizing. <strong>(Note: if there is one thing we want to the reader to take away from this entire chapter, it’s the following).</strong> For small batch sizes during generation (which is common), we can lower-bound our per-step latency by assuming we’re memory bandwidth bound in both the attention and MLP blocks:</p> \[\begin{equation*} \text{Theoretical Min Step Time} = \frac{\text{Batch Size} \times \text{KV Cache Size} + \text{Parameter Size}}{\text{Total Memory Bandwidth}} \end{equation*}\] <p>Similarly, for throughput:</p> \[\begin{equation*} \text{Theoretical Max Tokens/s} = \frac{\text{Batch Size} \times \text{Total Memory Bandwidth}}{\text{Batch Size} \times \text{KV Cache Size} + \text{Parameter Size}} \end{equation*}\] <p>Eventually, as our batch size grows, FLOPs begin to dominate parameter loading, so in practice we have the more general equation:</p> \[\begin{align} \tiny \text{Theoretical Step Time (General)} = \underbrace{\frac{\text{Batch Size} \times \text{KV Cache Size}}{\tiny \text{Total Memory Bandwidth}}}_{\text{Attention (always bandwidth-bound)}} + \underbrace{\max\left(\frac{2 \times \text{Batch Size} \times \text{Parameter Count}}{\text{Total FLOPs/s}}, \frac{\text{Parameter Size}}{\text{Total Memory Bandwidth}}\right)}_{\tiny \text{MLP (can be compute-bound)}} \end{align}\] <p>where the attention component (left) is never compute-bound, and thus doesn’t need a FLOPs roofline. These are fairly useful for back-of-the-envelope calculations, e.g.</p> <p><b style="color: #57cf57;">Pop Quiz:</b> Assume we want to take a generate step with a batch size of 4 tokens from a 30B parameter dense model on TPU v5e 4x4 slice in int8 with bf16 FLOPs, 8192 context and 100 kB / token KV caches. What is a reasonable lower bound on the latency of this operation? What if we wanted to sample a batch of 256 tokens?</p> <details><summary>Click here for the answer.</summary> <p><strong>Answer:</strong> in int8, our parameters will use 30e9 bytes and with the given specs our KV caches will use <code class="language-plaintext highlighter-rouge">100e3 * 8192 = 819MB</code> each. We have 16 chips, each with <code class="language-plaintext highlighter-rouge">8.1e11</code> bytes/s of bandwidth and <code class="language-plaintext highlighter-rouge">1.97e14</code> bf16 FLOPs/s. From the above equations, since we have a small batch size, we expect our step time to be at least <code class="language-plaintext highlighter-rouge">(4 * 819e6 + 30e9) / (16 * 8.1e11) = 2.5 ms</code>. At 256 tokens, we’ll be well into the compute-bound regime for our MLP blocks, so we have a step time of roughly <code class="language-plaintext highlighter-rouge">(256 * 819e6) / (16 * 8.1e11) + (2 * 256 * 30e9) / (16 * 1.97e14) = 21ms</code>.</p> </details> <p>As you can see, there’s a clear tradeoff between throughput and latency here. Small batches are fast but don’t utilize the hardware well. Big batches are slow but efficient. Here’s the latency-throughput Pareto frontier calculated for some older PaLM models (from the <a href="https://arxiv.org/pdf/2211.05102" rel="external nofollow noopener" target="_blank">ESTI paper</a><d-cite key="esti"></d-cite>):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/latency-cost-480.webp 480w,/scaling-book/assets/img/latency-cost-800.webp 800w,/scaling-book/assets/img/latency-cost-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/latency-cost.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure:</b> Pareto frontier of cost (read: throughput) versus latency for several PaLM models. Note how chip count (C) and batch size (B) moves you along the Pareto frontier, with the exception of the green dot (C:32 B:16 for PaLM 540B) where the available memory prevented the setup from supporting a good batch size and caused throughput to suffer. Note how throughput generally tends to flatten around after the batch size 240. int8 weights offers a better latency-throughput pareto optimal, but not a better max throughput.</figcaption> </figure> <p>Not only do we trade off latency and throughput with batch size as knob, we may also prefer a larger topology to a smaller one so we can fit larger batches if we find ourselves limited by HBM. The <a href="../applied-inference">next section</a> explores this in more detail.</p> <p class="takeaway"><strong>Takeaway:</strong> if you care about generation throughput, use the largest per-chip batch size possible. Any per-chip batch size above the TPU arithmetic intensity ($B_\text{crit}$, usually 120 or 240) will maximize throughput. You may need to increase your topology to achieve this. Smaller batch sizes will allow you to improve latency at the cost of throughput.</p> <details><summary>There are some caveats to this from a hardware standpoint. Click here for some nits.</summary> <p>This is all quite theoretical. In practice we often don’t quite see a sharp roofline for a few reasons:</p> <ul> <li>Our assumption that HBM reads will be perfectly overlapped with FLOPs is not realistic, since our compiler (XLA) is fallible.</li> <li>For sharded models, XLA also often fails to efficiently overlap the ICI communication of our model-sharded matrix multiples with the FLOPs themselves, so we often start taking a latency hit on linears over \(\text{BS}=32\).</li> <li>Batch sizes larger than the theoretical roofline will still see some improvement in throughput because of imperfect overlapping, but this is a good heuristic.</li> </ul> </details> <h3 id="what-about-memory">What about memory?</h3> <p>We’ve spent some time looking at bandwidth and FLOPs, but not at memory. The memory picture looks a lot different at inference time, thanks to our new data structure, the KV cache. For this section, let’s pick a real model (LLaMA 2-13B) to demonstrate how different things look:</p> <table> <thead> <tr> <th>hyperparam</th> <th>value</th> </tr> </thead> <tbody> <tr> <td>n_layers (L)</td> <td>40</td> </tr> <tr> <td>d_model (D)</td> <td>5,120</td> </tr> <tr> <td>ffw_multiplier (F // D)</td> <td>2.7</td> </tr> <tr> <td>n_heads (N)</td> <td>40</td> </tr> <tr> <td>n_kv_heads (K)</td> <td>40</td> </tr> <tr> <td>d_qkv (H)</td> <td>128</td> </tr> <tr> <td>n_embeddings (V)</td> <td>32,000</td> </tr> </tbody> </table> <p>What’s using memory during inference? Well, obviously, our parameters. Counting those, we have:</p> <table> <thead> <tr> <th>param</th> <th>formula</th> <th>size (in bytes)</th> </tr> </thead> <tbody> <tr> <td>FFW params</td> <td>d_model<sup>2</sup> x ffw_multiplier x 3 (for gelu + out-projection) x n_layers</td> <td>5,120 x 5,120 x 2.7 x 3 x 40 = <strong>8.5e9</strong> </td> </tr> <tr> <td>Vocab params</td> <td>2 (input and output embeddings) x n_embeddings x d_model</td> <td>2 x 32,000 x 5,120 = <strong>0.3e9</strong> </td> </tr> <tr> <td>Attention params</td> <td>[2 (<em>q and output</em>) x d_model x n_heads x d_qkv + 2 (<em>for k and v</em>) x d_model x n_kv_heads x d_qkv] x n_layers</td> <td>(2 x 5,120 x 40 x 128 + 2 x 5,120 x 40 x 128) x 40 = <strong>4.2e9</strong> </td> </tr> </tbody> </table> <p>Adding these parameters up, we get 8.5e9 + 4.2e9 + 0.3e9 = <strong>13e9 total parameters</strong>, just as expected. As we saw in the previous sections, during training we might store our parameters in bfloat16 with an optimizer state in float32. That may use around 100GB of memory. That pales in comparison to our gradient checkpoints, which can use several TBs.</p> <p><strong>How is inference different?</strong> During inference, we store one copy of our parameters, let’s say in bfloat16. That uses 26GB — and in practice we can often do much better than this with quantization. There’s no optimizer state or gradients to keep track of. Because we don’t checkpoint (keep activations around for the backwards pass), our activation footprint is negligible for both prefill<d-footnote>Particularly thanks to Flash Attention, which avoids materializing our attention matrix</d-footnote> and generate. If we prefill 8k tokens, a single activation only uses around <code class="language-plaintext highlighter-rouge">8,192 x 5,120 x 2 bytes = 80MB</code> of memory. Longer prefills can be broken down into many smaller forward passes, so it’s not a problem for longer contexts either. Generation use even fewer tokens than that, so activations are negligible.</p> <p><strong>The main difference is the KV cache</strong>. These are the keys and value projections for all past tokens, bounded in size only by the maximum allowed sequence length. The total size for \(T\) tokens is</p> \[\text{KV cache size} = 2 \cdot \text{bytes per float} \cdot H \cdot K \cdot L \cdot T\] <p>where \(H\) is the dimension of each head, \(K\) is the number of KV heads, \(L\) is the number of layers, and the 2 comes from storing both the keys and values.</p> <p><strong>This can get big very quickly</strong>, even with modest batch size and context lengths. For LLaMA-13B, a KV cache for a single 8192 sequence at bf16 is</p> \[8192\ (T) \times 40\ (K) \times 128\ (H) \times 40\ (L) \times 2\ (\text{bytes}) \times 2 = 6.7 \text{GB}\] <p><strong>Just 4 of these exceed the memory usage of our parameters!</strong> To be clear, LLaMA 2 was not optimized for KV cache size at longer contexts (it isn’t always this bad, since usually $K$ is much smaller, as in LLaMA-3), but this is still illustrative. We cannot neglect these in memory or latency estimates.</p> <h3 id="modeling-throughput-and-latency-for-llama-2-13b">Modeling throughput and latency for LLaMA 2-13B</h3> <p>Let’s see what happens if we try to perform generation perfectly efficiently at different batch sizes on 8xTPU v5es, up to the critical batch size (240) derived earlier for maximum theoretical throughput.</p> <table> <thead> <tr> <th style="text-align: left">Batch Size</th> <th style="text-align: right">1</th> <th style="text-align: right">8</th> <th style="text-align: right">16</th> <th style="text-align: right">32</th> <th style="text-align: right">64</th> <th style="text-align: right">240</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">KV Cache Memory (GiB)</td> <td style="text-align: right">6.7</td> <td style="text-align: right">53.6</td> <td style="text-align: right">107.2</td> <td style="text-align: right">214.4</td> <td style="text-align: right">428.8</td> <td style="text-align: right">1608</td> </tr> <tr> <td style="text-align: left">Total Memory (GiB)</td> <td style="text-align: right">32.7</td> <td style="text-align: right">79.6</td> <td style="text-align: right">133.2</td> <td style="text-align: right">240.4</td> <td style="text-align: right">454.8</td> <td style="text-align: right">1634</td> </tr> <tr> <td style="text-align: left">Theoretical Step Time (ms)</td> <td style="text-align: right">4.98</td> <td style="text-align: right">12.13</td> <td style="text-align: right">20.30</td> <td style="text-align: right">36.65</td> <td style="text-align: right">69.33</td> <td style="text-align: right">249.09</td> </tr> <tr> <td style="text-align: left">Theoretical Throughput (tokens/s)</td> <td style="text-align: right">200.61</td> <td style="text-align: right">659.30</td> <td style="text-align: right">787.99</td> <td style="text-align: right">873.21</td> <td style="text-align: right">923.13</td> <td style="text-align: right">963.53</td> </tr> </tbody> </table> <p>8x TPU v5es gives us 128GiB of HBM, 6.5TiB/s of HBM bandwidth (0.82TiB/s each) and 1600TF/s of compute.</p> <p>For this model, increasing the batch size does give us better throughput, but we suffer rapidly diminishing returns. We OOM beyond batch size 16, and need an order of magnitude more memory to go near 240. A bigger topology can improve the latency, but we’ve hit a wall on the per chip throughput.</p> <p>Let’s say we keep the total number of params the same, but magically make the KV cache 5x smaller (say, with 1:5 <a href="#tricks-for-improving-generation-throughput-and-latency">GMQA</a>, which means we have 8 KV heads shared over the 40 Q heads — see next section for more details).</p> <table> <thead> <tr> <th style="text-align: left">Batch Size</th> <th style="text-align: right">1</th> <th style="text-align: right">8</th> <th style="text-align: right">16</th> <th style="text-align: right">32</th> <th style="text-align: right">64</th> <th style="text-align: right">240</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">KV Cache Memory (GiB)</td> <td style="text-align: right">1.34</td> <td style="text-align: right">10.72</td> <td style="text-align: right">21.44</td> <td style="text-align: right">42.88</td> <td style="text-align: right">85.76</td> <td style="text-align: right">321.6</td> </tr> <tr> <td style="text-align: left">Total Memory (GiB)</td> <td style="text-align: right">27.34</td> <td style="text-align: right">36.72</td> <td style="text-align: right">47.44</td> <td style="text-align: right">68.88</td> <td style="text-align: right">111.76</td> <td style="text-align: right">347.6</td> </tr> <tr> <td style="text-align: left">Theoretical Step Time (ms)</td> <td style="text-align: right">4.17</td> <td style="text-align: right">5.60</td> <td style="text-align: right">7.23</td> <td style="text-align: right">10.50</td> <td style="text-align: right">17.04</td> <td style="text-align: right">52.99</td> </tr> <tr> <td style="text-align: left">Theoretical Throughput (tokens/s)</td> <td style="text-align: right">239.94</td> <td style="text-align: right">1,429.19</td> <td style="text-align: right">2,212.48</td> <td style="text-align: right">3,047.62</td> <td style="text-align: right">3,756.62</td> <td style="text-align: right">4,529.34</td> </tr> </tbody> </table> <p>With a smaller KV cache, we still have diminishing returns, but the theoretical throughput per chip continues to scale up to batch size 240. We can fit a much bigger batch of 64, and latency is also consistently better at all batch sizes. The latency, maximum throughput, and maximum batch size all improve dramatically! In fact, later LLaMA generations used this exact optimization — LLaMA-3 8B has 32 query heads and 8 KV heads (<a href="https://huggingface.co/MaziyarPanahi/Llama-3-13B-Instruct-v0.1/blob/dfdeb40bdb2c149dfa399ea2be0d56eb120f0831/config.json" rel="external nofollow noopener" target="_blank">source</a>).</p> <p class="takeaway"><strong>Takeaway:</strong> In addition to params, the size of KV cache has a lot of bearing over the ultimate inference performance of the model. We want to keep it under control with a combination of architectural decisions and runtime optimizations.</p> <h2 id="tricks-for-improving-generation-throughput-and-latency">Tricks for Improving Generation Throughput and Latency</h2> <p>Since the original <a href="https://arxiv.org/abs/1706.03762" rel="external nofollow noopener" target="_blank">Attention is All You Need paper</a>, many techniques have been developed to make the model more efficient, often targeting the KV cache specifically. Generally speaking, a smaller KV cache makes it easier to increase batch size and context length of the generation step without hurting latency, and makes life easier for the systems surrounding the Transformer (like request caching). Ignoring effects on quality, we may see:</p> <p><strong>Grouped multi-query attention (aka GMQA, GQA):</strong> We can reduce the number of KV heads, and share them with many Q heads in the attention mechanism. In the extreme case, it is possible to share a single KV head across all Q heads. This reduces the KV cache by a factor of the Q:KV ratio over pure MHA, and it has been observed that the performance of models is relatively insensitive to this change.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/gmqa-480.webp 480w,/scaling-book/assets/img/gmqa-800.webp 800w,/scaling-book/assets/img/gmqa-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/gmqa.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This also effectively increases the arithmetic intensity of the attention computation (see Question 4 in <a href="../transformers">Section 4</a>).</p> <p><strong>Mixing in some local attention layers:</strong> Local attention caps the context to a small to moderately sized max length. At training time and prefill time, this involves masking the attention matrix to a diagonal strip instead of a triangle. This effectively caps the size of the max length of the KV cache for the local layers. By mixing in some local layers into the model with some global layers, the KV cache is greatly reduced in size at contexts longer than the local window.</p> <p><strong>Sharing KVs across layers:</strong> The model can learn to share the same KV caches across layers in some pattern. Whilst this does reduce the KV cache size, and provide benefits in increasing batch size, caching, offline storage etc. shared KV caches may need to be read from HBM multiple times, <em>so it does not necessarily improve the step time.</em></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/kv-sharing-480.webp 480w,/scaling-book/assets/img/kv-sharing-800.webp 800w,/scaling-book/assets/img/kv-sharing-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/kv-sharing.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <b>Left:</b> Multiple layers of pure global attention. <b>Right:</b> An example of some global/local interleaving pattern with sharing with adjacent layers. Source: <a href="https://research.character.ai/optimizing-inference/?ref=blog.character.ai" rel="external nofollow noopener" target="_blank">Character.ai blog</a>.</figcaption> </figure> <p><strong>Quantization:</strong> Inference is usually less sensitive to the precision of parameters and KVs. By quantizing the parameters and KV cache (e.g. to int8, int4, <code class="language-plaintext highlighter-rouge">fp8</code> etc.), we can save on memory bandwidth on both, decrease the batch size required to reach the compute roofline and save memory to run at bigger batch sizes. Quantization has the added advantage that even if the model was not trained with quantization it can often be applied post training.</p> <p><strong>Using ragged HBM reads and Paged Attention:</strong> We allocated 8k of context for each KV cache in the calculations above but it is often not necessary to read the entire KV cache from memory — requests have a wide range of length distributions and don’t use the max context of the model, so we can often implement kernels (e.g. Flash Attention variants) that only read the non-padding part of the KV cache.</p> <p>Paged Attention<d-cite key="paged"></d-cite> is a refinement upon this that stores KV caches in OS-style page tables and mostly avoids padding the KV caches altogether. This adds a lot of complexity but means every batch only uses as much memory as it needs. This is a runtime optimization, so again it is indifferent to architecture.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/paged-attention-480.webp 480w,/scaling-book/assets/img/paged-attention-800.webp 800w,/scaling-book/assets/img/paged-attention-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/paged-attention.png" class="img-fluid img-small" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure:</b> during generation, a single token (forth) attends to multiple KV cache blocks/pages. By paging the KV cache, we avoid loading or storing more memory than we need to. Taken from the <a href="https://arxiv.org/pdf/2309.06180" rel="external nofollow noopener" target="_blank">PagedAttention paper</a>.</figcaption> </figure> <p class="takeaway"><strong>Big Picture:</strong> All told, these KV cache optimizations can reduce KV cache sizes by over an order of magnitude compared to a standard MHA Transformer. This can lead to an order-of-magnitude improvement in the overall cost of the Transformer.</p> <h2 id="distributing-inference-over-multiple-accelerators">Distributing Inference Over Multiple Accelerators</h2> <p>So far we’ve handwaved how we’re scaling beyond a single chip. Following <a href="../training">Section 5</a>, let’s explore the different strategies available to us and their tradeoffs. As always, we will look at prefill and generation separately.</p> <h3 id="prefill">Prefill</h3> <p>From a roofline standpoint, <strong>prefill is almost identical to training</strong> and almost all the same techniques and tradeoffs apply — model (Megatron) parallelism, sequence sharding (for sufficiently long context), pipelining, even FSDP are all viable! You just have to keep the KVs kicking around so you can do generation later. As in training, increasing the number of chips gives us access to more FLOPs/s (for potentially lower TTFT), but adds communication overhead (potentially reducing throughput per chip).</p> <p><strong>The general rule for sharding prefill:</strong> here’s a general set of rules for prefill. We’ll assume we’re doing prefill on a single sequence only (no batch dimension):</p> <ol> <li> <em>Model sharding:</em> We typically do some amount of model parallelism first, up to the point we become ICI-bound. As we saw in <a href="../training">Section 5</a>, this is around $F / 2550$ for 1 axis (usually around 4-8 way sharding).</li> <li> <em>Sequence parallelism:</em> Beyond this, we do sequence parallelism (like data parallelism but sharding across the sequence dimension). While sequence parallelism introduces some extra communication in attention, it is typically fairly small at longer contexts. As with training, we can overlap the communication and computation (using collective matmuls for Megatron and ring attention respectively).</li> </ol> <p class="takeaway"><strong>Takeaway:</strong> during prefill, almost any sharding that can work during training can work fine. Do model parallelism up to the ICI bound, then do sequence parallelism.</p> <h3 id="generation">Generation</h3> <p>Generation is a more complicated beast than prefill. For one thing, it is harder to get a large batch size because we need to batch many requests together. Latency targets are lower. Together, these mean we are typically more memory-bound and more sensitive to communication overhead, which restrict our sharding strategies:</p> <ol> <li> <p><strong>FSDP is impossible:</strong> since we are memory-bound in loading our parameters and KV caches from HBM to the MXU, we do not want to move them via ICI which is orders of magnitudes slower than HBM. <em>We want to move activations rather than weights.</em> This means methods similar to FSDP are usually completely unviable for generation.<d-footnote>Accidentally leaving it on after training is an easy and common way to have order of magnitude regressions</d-footnote></p> </li> <li> <p><strong>There is no reason to do data parallelism:</strong> pure data parallelism is unhelpful because it replicates our parameters and doesn’t help us load parameters faster. You’re better off spinning up multiple copies of the model instead.<d-footnote>By this we mean, spin up multiple servers with copies of the model at a smaller batch size. Data parallelism at the model level is strictly worse.</d-footnote></p> </li> <li> <p><strong>No sequence = no sequence sharding.</strong> Good luck sequence sharding.</p> </li> </ol> <p><em>This mostly leaves us with variants of model sharding for dense model generation</em>. As with prefill, the simplest thing we we can do is simple model parallelism (with activations fully replicated, weights fully sharded over hidden dimension for the MLP) up to 4-8 ways when we become ICI bound. However, since we are often memory bandwidth bound, we can actually go beyond this limit to improve latency!</p> <p><strong>Note on ICI bounds for generation:</strong> during training we want to be compute-bound, so our rooflines look at when our ICI comms take longer than our FLOPs. However, during generation, if we’re memory bandwidth bound by parameter loading, we can increase model sharding beyond this point and improve latency at a minimal throughput cost. More model sharding gives us more HBM to load our weights over, and our FLOPs don’t matter.<d-footnote>In the sense that FLOPs time isn't bottlenecking us, so the thing we need to worry about is ICI time exceeding parameter loading time.</d-footnote> Let’s look at how much model parallelism we can do before it becomes the bottleneck.</p> \[\begin{align*}T_\text{HBM comms} = \frac{2DF}{Y \cdot W_\text{hbm}} &amp;&amp; T_\text{ICI comms} = \frac{2BD}{W_\text{ici}}\end{align*}\] \[T_\text{ICI comms} &gt; T_\text{HBM comms} \rightarrow \frac{W_\text{hbm}}{W_\text{ici}} &gt; \frac{F}{Y \cdot B} \rightarrow Y &gt; F / (B \cdot \beta)\] <p>where $\beta = W_\text{hbm} / W_\text{ici}$. This number is usually around 8 for TPU v5e and TPU v6e. That means e.g. if $F$ is 16,384 and $B$ is 32, we can in theory do model parallelism up to <code class="language-plaintext highlighter-rouge">16384 / (32 * 8) = 64</code> ways without a meaningful hit in throughput. This assume we can fully shard our KV caches 64-ways which is difficult: we discuss this below.</p> <p>For the attention layer, we also model shard attention \(W_Q\) and \(W_O\) over heads Megatron style. The KV weights are quite small, and replicating them is often cheaper than sharding beyond $K$-way sharding.</p> <p class="takeaway"><strong>Takeaway:</strong> our only options during generation are variants of model parallelism. We aim to move activations instead of KV caches or parameters, which are larger. When our batch size is large, we do model parallelism up to the FLOPs-ICI bound ($F / \alpha$). When our batch size is smaller, we can improve latency by model sharding more (at a modest throughput cost). When we want to model shard more ways than we have KV heads, we can shard our KVs along the batch dimension as well.</p> <h3 id="sharding-the-kv-cache">Sharding the KV cache</h3> <p><strong>We also have an additional data structure that needs to be sharded — the KV cache.</strong> Again, we almost always prefer to avoid replicating the cache, since it is the primary source of attention latency. To do this, we first Megatron-shard the KVs along the head dimension. This is limited to $K$-way sharding, so for models with a small number of heads, we shard the head dimension as much as possible and then shard along the batch dimension, i.e. $\text{KV}[2, B_Z, S, K_Y, H]$. This means the KV cache is completely distributed.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/esta-figure-480.webp 480w,/scaling-book/assets/img/esta-figure-800.webp 800w,/scaling-book/assets/img/esta-figure-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/esta-figure.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure:</b> comparison of the attention mechanism with (a) Multi head attention with pure model sharding and (b) Multiquery attention with batch sharding of the KV cache. Notice how we need two extra AllToAlls to shift the activations from model sharding to batch sharding, so they can act on the KV caches.</figcaption> </figure> <p>The cost of this is two AllToAlls every attention layer — one to shift the Q activations to the batch sharding so we can compute attention with batch sharding, and one to shift the batch sharded attention output back to pure model sharded.</p> <details><summary>Here’s the full algorithm!</summary> <p>Here we’ll write out the full attention algorithm with model parallelism over both $Y$ and $Z$. I apologize for using $K$ for both the key tensor and the KV head dimension. Let $M=N/K$.</p> <div class="algorithm"> <ol> <li>X[B, D] = … (existing activations, unsharded from previous layer)</li> <li>K[B<sub>Z</sub>, S, K<sub>Y</sub>, H], V[B<sub>Z</sub>, S, K, H] = … (existing KV cache, batch sharded)</li> <li>Q[B, N<sub>YZ</sub>, H] = X[B, D] * W<sub>Q</sub>[D, N<sub>YZ</sub>, H]</li> <li>Q[B<sub>Z</sub>, N<sub>Y</sub>, H] = <strong>AllToAll</strong><sub>Z-&gt;B</sub>(Q[B, N<sub>YZ</sub>, H])</li> <li>Q[B<sub>Z</sub>, K<sub>Y</sub>, M, H] = <strong>Reshape</strong>(Q[B<sub>Z</sub>, N<sub>Y</sub>, H])</li> <li>O[B<sub>Z</sub>, S, K<sub>Y</sub>, M] = Q[B<sub>Z</sub>, K<sub>Y</sub>, M, H] *<sub>H</sub> K[B<sub>Z</sub>, S, K<sub>Y</sub>, H]</li> <li>O[B<sub>Z</sub>, S, K, M] = <strong>Softmax</strong><sub>S</sub>(O[B<sub>Z</sub>, S, K<sub>Y</sub>])</li> <li>O[B<sub>Z</sub>, K<sub>Y</sub>, M, H] = O[B<sub>Z</sub>, S, K, M] *<sub>S</sub> V[B<sub>Z</sub>, S, K<sub>Y</sub>, H]</li> <li>O[B, K<sub>Y</sub>, M<sub>Z</sub>, H] = <strong>AllToAll</strong><sub>Z-&gt;M</sub>(O[B<sub>Z</sub>, K<sub>Y</sub>, M, H])</li> <li>O[B, N<sub>YZ</sub>, H] = <strong>Reshape</strong>(O[B, K<sub>Y</sub>, M<sub>Z</sub>, H])</li> <li>X[B, D] {U<sub>YZ</sub>} = W<sub>O</sub>[N<sub>YZ</sub>, H, D] *<sub>N,H</sub> O[B, N<sub>YZ</sub>, H]</li> <li>X[B, D] = <strong>AllReduce</strong>(X[B, D] { U<sub>YZ</sub>})</li> </ol> <p>This is pretty complicated but you can see generally how it works. The new comms are modestly expensive since they operate on our small activations, while in return we save a huge amount of memory bandwidth loading the KVs (which are stationary).</p> </div> </details> <ul> <li> <strong>Sequence sharding:</strong> If the batch size is too small, or the context is long, we can sequence shard the KV cache. Again, we pay a collective cost in accumulating the attention across shards here. First we need to AllGather the Q activations, and then accumulate the KVs in a similar fashion to Flash Attention.</li> </ul> <h2 id="designing-an-effective-inference-engine">Designing an Effective Inference Engine</h2> <p>So far we’ve looked at how to optimize and shard the individual prefill and generate operations efficiently in isolation. To actually use them effectively, we need to design an inference engine which can feed these two operations at a point of our choosing on the latency/throughput Pareto frontier.</p> <p>The simplest method is simply to run a batch of prefill, then a batch of generations:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/batched-prefill-480.webp 480w,/scaling-book/assets/img/batched-prefill-800.webp 800w,/scaling-book/assets/img/batched-prefill-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/batched-prefill.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure:</b> in the simplest setup, requests are aggregated, and the server alternates between running a batch of prefills and calling the generate function until completion for all sequences.</figcaption> </figure> <p>This is easy to implement and is the first inference setup in most codebases, but it has multiple drawbacks:</p> <ol> <li> <strong>Latency is terrible.</strong> We couple the prefill and generate batch size. Time to first token (TTFT) is terrible at big prefill batch sizes — you need to finish all prefills before any users can see any tokens. Generate throughput is terrible at small batch sizes.</li> <li> <strong>We block shorter generations on longer ones.</strong> Many sequences will finish before others, leaving empty batch slots during generation, hurting generate throughput further. The problem exacerbates as batch size and generation length increases.</li> <li> <strong>Prefills are padded.</strong> Prefills are padded to the longest sequence and we waste a lot of compute. There are solutions for this, but historically XLA made it quite difficult to skip these FLOPs. Again this becomes worse the bigger the batch size and prefill sequence length.</li> <li> <strong>We’re forced to share a sharding between prefill and generation.</strong> Both prefill and generate live on the same slice, which means we use the same topology and shardings (unless you keep two copies of the weights) for both and is generally unhelpful for performance e.g. generate wants a lot more model sharding.</li> </ol> <p>Therefore this method is only recommended for edge applications (which usually only cares about serving a single user and using hardware with less FLOPs/byte) and rapid iteration early in the lifecycle of a Transformer codebase (due to its simplicity).</p> <p>A slightly better approach involves performing prefill at batch size 1 (where it is compute-bound but has reasonable latency) but batch multiple requests together during generation:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/interleaving-480.webp 480w,/scaling-book/assets/img/interleaving-800.webp 800w,/scaling-book/assets/img/interleaving-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/interleaving.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This will avoid wasted TTFT from batched prefill while keeping generation throughput high. We call this an <strong>interleaved</strong> configuration, since we “interleave” prefill and generation steps. This is very powerful for bulk generation applications like evaluations where throughput is the main goal. The orchestrator can be configured to prioritise prefill the moment any generation slots open up, ensuring high utilisation even for very large generation batch sizes. We can also avoid padding our prefill to the maximum length, since it isn’t batched with another request.</p> <p>The main disadvantage is that when the server is performing a prefill, the generation of all other requests pauses since all the compute resources will be consumed by the prefill. User A whose response is busy decoding will be blocked by user B whose prefill is occurring. This means even though TTFT has improved, the token generation will be jittery and slow on average, which is not a good user experience for many applications — other user’s prefills are on the critical path of the overall latency of a request.</p> <p>To get around this, we separate decode and prefill. While Transformer inference can be done on one server, it is often better from a latency standpoint to execute the two different tasks on two sets of TPUs/GPUs. Prefill servers generate KV caches that get sent across the network to the generate servers, which batch multiple caches together and generate tokens for each of them. We call this <strong>“disaggregated”</strong> serving.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/disaggregation-480.webp 480w,/scaling-book/assets/img/disaggregation-800.webp 800w,/scaling-book/assets/img/disaggregation-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/disaggregation.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This provides a few advantages:</p> <ol> <li> <p><strong>Low latency at scale</strong>: A user’s request never blocks on another user’s, except if there is insufficient prefill capacity. The request should be immediately prefilled, then sent to the generation server, then immediately slotted into the generation buffer. If we expect many concurrent requests to come in, we can scale the number of prefill servers independently from the number of generate servers so users are not left in the prefill queue for an extended period of time.</p> </li> <li> <p><strong>Specialization:</strong> Quite often, the latency-optimal parameter sharding strategy/hardware topology for prefill and generate is quite different (for instance, more model parallelism is useful for generate but not prefill). Constraining the two operations to use the same sharding hurts the performance of both, and having two sets of weights uses memory. Also, by moving prefill onto its own server, it doesn’t need to hold any KV caches except the one it’s currently processing. That means we have a lot more memory free for history caching (see the next section) or optimizing prefill latency.</p> </li> </ol> <p>One downside is that the KV cache now needs to be shifted across the network. This is typically acceptable but again provides a motivation for reducing KV cache size.</p> <p class="takeaway"><strong>Takeaway:</strong> for latency-sensitive, high-throughput serving, we typically have to separate prefill and generation into separate servers, with prefill operating at batch 1 and generation batching many concurrent requests together.</p> <h3 id="continuous-batching">Continuous Batching</h3> <p>Problem (2) above motivates the concept of <strong>continuous batching</strong>. We optimize and compile:</p> <ul> <li>A number of prefill functions with variable context lengths and inserts it into some KV buffer, some maximum batch size and context length/number of pages.</li> <li>A generate function which takes in the KV cache, and performs the generation step for all currently active requests.</li> </ul> <p>We then combine these functions with an orchestrator which queues the incoming requests, calls prefill and generate depending on the available generate slots, handles history caching (see next section) and streams the tokens out.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/continuous-batching-480.webp 480w,/scaling-book/assets/img/continuous-batching-800.webp 800w,/scaling-book/assets/img/continuous-batching-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/continuous-batching.gif" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="prefix-caching">Prefix Caching</h3> <p>Since prefill is expensive and compute-bound (giving us less headroom), one of the best ways to reduce its cost is to do less of it. Because LLMs are autoregressive, the queries [“I”, “like”, “dogs”] and [“I”, “like”, “cats”] produce KV caches that are identical in the first two tokens. What this means is that, in principle, if we compute the “I like dogs” cache first and then the “I like cats” cache, we only need to do 1 / 3 of the compute. We can save most of the work by reusing the cache. This is particularly powerful in a few specific cases:</p> <ol> <li> <strong>Chatbots</strong>: most chatbot conversations involve a back-and-forth dialog that strictly appends to itself. This means if we can save the KV caches from each dialog turn, we can skip computation for all but the newest tokens.</li> <li> <strong>Few-shot prompting</strong>: if we have any kind of few-shot prompt, this can be saved and reused for free. System instructions often have this form as well.</li> </ol> <p>The only reason this is hard to do is memory constraints. As we’ve seen, KV caches are big (often many GB), and for caching to be useful we need to keep them around until a follow-up query arrives. Typically, any unused HBM on the prefill servers can be used for a local caching system. Furthermore, accelerators usually have a lot of memory on their CPU hosts (e.g. a 8xTPUv5e server has 128GiB of HBM, but around 450GiB of Host DRAM). This memory is much slower than HBM — too slow to do generation steps usually — but is fast enough for a cache read. In practice:</p> <ul> <li>Because the KV cache is local to the set of TPUs that handled the initial request, we need some form of affinity routing to ensure follow-up queries arrive at the same replica. This can cause issues with load balancing.</li> <li>A smaller KV cache is helpful (again) — it enables us to save more KV caches in the same amount of space, and reduce read times.</li> <li>The KV cache and their lookups can be stored quite naturally in a tree or trie. Evictions can happen on an LRU basis.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/prefix-caching-trie-480.webp 480w,/scaling-book/assets/img/prefix-caching-trie-800.webp 800w,/scaling-book/assets/img/prefix-caching-trie-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/prefix-caching-trie.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure:</b> KV prefix cache implemented as an LRU trie. We can avoid duplicating KV memory by sharing prefixes. Source: <a href="https://research.character.ai/optimizing-inference/?ref=blog.character.ai" rel="external nofollow noopener" target="_blank">Character.ai blog</a>.</figcaption> </figure> <h3 id="lets-look-at-an-implementation-jetstream">Let’s look at an implementation: JetStream</h3> <p>Google has open-sourced a library that implements this logic called <a href="https://github.com/google/JetStream" rel="external nofollow noopener" target="_blank">JetStream</a>. The server has a set of “prefill engines” and “generate engines”, usually on different TPU slices, which are orchestrated by a single controller. Prefill happens in the “<a href="https://github.com/AI-Hypercomputer/JetStream/blob/c0f83127c16d7861cacc560303a28404c6cbb24c/jetstream/core/orchestrator.py#L499" rel="external nofollow noopener" target="_blank">prefill thread</a>”, while generation happens in the “<a href="https://github.com/AI-Hypercomputer/JetStream/blob/c0f83127c16d7861cacc560303a28404c6cbb24c/jetstream/core/orchestrator.py#L629" rel="external nofollow noopener" target="_blank">generate thread</a>”. We also have a “<a href="https://github.com/AI-Hypercomputer/JetStream/blob/c0f83127c16d7861cacc560303a28404c6cbb24c/jetstream/core/orchestrator.py#L592" rel="external nofollow noopener" target="_blank">transfer thread</a>” that orchestrates copying the KV caches from the prefill to generate slices.</p> <p>The Engine interface (implemented <a href="https://github.com/google/JetStream/blob/445f1aa8e857d0a09d72618e365daf80723bdf4c/jetstream/engine/engine_api.py#L138" rel="external nofollow noopener" target="_blank">here</a>) is a generic interface that any LLM must provide. The key methods are:</p> <ul> <li> <strong>prefill:</strong> takes a set of input tokens and generates a KV cache.</li> <li> <strong>insert:</strong> takes a KV cache and inserts it into the batch of KV caches that generate is generating from.</li> <li> <strong>generate:</strong> takes a set of batched KV caches and generates one token per batch entry, appending a single token’s KV cache to the decode state for each token.</li> </ul> <p>We also have a PyTorch version of JetStream available <a href="https://github.com/google/jetstream-pytorch" rel="external nofollow noopener" target="_blank">here</a>.</p> <h2 id="worked-problems">Worked Problems</h2> <p>I’m going to invent a new model based on LLaMA-2 13B for this section. Here are the details:</p> <table> <thead> <tr> <th>hyperparam</th> <th>value</th> </tr> </thead> <tbody> <tr> <td>n_layers (L)</td> <td>64</td> </tr> <tr> <td>d_model (D)</td> <td>4,096</td> </tr> <tr> <td>d_ff (F)</td> <td>16,384</td> </tr> <tr> <td>n_heads (N)</td> <td>32</td> </tr> <tr> <td>n_kv_heads (K)</td> <td>8</td> </tr> <tr> <td>d_qkv (H)</td> <td>256</td> </tr> <tr> <td>n_embeddings (V)</td> <td>32,128</td> </tr> </tbody> </table> <p><strong>Question 1:</strong> How many parameters does the above model have? How large are its KV caches per token? <em>You can assume we share the input and output projection matrices.</em></p> <details><summary>Click here for the answer.</summary> <p><strong>Parameter count:</strong></p> <ul> <li>MLP parameter count: $L * D * F * 3$</li> <li>Attention parameter count: $L * 2 * D * H * (N + K)$</li> <li>Vocabulary parameter: $D * V$ (since we share these matrices)</li> </ul> <p>Our total parameter count is thus $L * D * (3F + 2H * (N + K)) + D * V$. Plugging in the numbers above, we have <code class="language-plaintext highlighter-rouge">64 * 4096 * (3*16384 + 2 * 256 * (32 + 8)) + 4096 * 32128 = 18.4e9</code>. Thus, this model has about 18.4 billion parameters.</p> </details> <p><strong>Question 2:</strong> Let’s say we want to serve this model on a TPUv5e 4x4 slice and can fully shard our KV cache over this topology. What’s the largest batch size we can fit, assuming we use int8 for everything. What if we dropped the number of KV heads to 1?</p> <p><strong>Question 3:</strong> Let’s pretend we’re totally HBM bandwidth bound. How long does it take to load all the parameters into the MXU from HBM? <em>This is a good lower bound on the per-step latency.</em></p> <p><strong>Question 4:</strong> Let’s say we want to serve this model on a TPUv5e 4x4 slice. How would we shard it? <em>Hint: maybe answer these questions first:</em></p> <ol> <li>What’s the upper bound on tensor parallelism for this model over ICI?</li> <li>How can we shard the KV caches?</li> </ol> <p>For this sharding, what is the rough per-step latency for generation?</p> <p><strong>Question 5:</strong> Let’s pretend the above model is actually an MoE. An MoE model is effectively a dense model with E copies of the FFW block. Each token passes through k of the FFW blocks and these <code class="language-plaintext highlighter-rouge">k</code> are averaged to produce the output. Let’s use <code class="language-plaintext highlighter-rouge">E=16</code> and <code class="language-plaintext highlighter-rouge">k=2</code> with the above settings.</p> <ol> <li>How many parameters does it have?</li> <li>What batch size is needed to become FLOPs bound?</li> <li>How large are its KV caches per token (assume no local attention)?</li> <li>How many FLOPs are involved in a forward pass with T tokens?</li> </ol> <p><strong>Question 6:</strong> With MoEs, we can do “expert sharding”, where we split our experts across one axis of our mesh. In our standard notation, our first FFW weight has shape <code class="language-plaintext highlighter-rouge">[E, D, F]</code> and we shard it as [E<sub>Z</sub>, D<sub>X</sub>, F<sub>Y</sub>] where <code class="language-plaintext highlighter-rouge">X</code> is only used during training as our FSDP dimension. Let’s say we want to do inference on a TPU v5e:</p> <ol> <li>What’s the HBM weight loading time for the above model on a TPU v5e 8x16 slice with Y=8, Z=16? How much free HBM is available per TPU?</li> <li>What is the smallest slice we could fit our model on?</li> </ol> <p><strong>Question 7 [2D model sharding]:</strong> Here we’ll work through the math of what the <a href="https://arxiv.org/pdf/2211.05102" rel="external nofollow noopener" target="_blank">ESTI paper</a> calls 2D weight-stationary sharding. We describe this briefly in Appendix B, but try doing this problem first to see if you can work out the math. The basic idea of 2D weight stationary sharding is to shard our weights along both the $D$ and $F$ axes so that each chunk is roughly square. This reduces the comms load and allows us to scale slightly farther.</p> <p>Here’s the algorithm for 2D weight stationary:</p> <div class="algorithm"> <ol> <li>In[B, D<sub>X</sub>] = <strong>AllGather</strong><sub>YZ</sub>(In[B, D<sub>XYZ</sub>])</li> <li>Tmp[B, F<sub>YZ</sub>] {U.X} = In[B, D<sub>X</sub>] *<sub>D</sub> W<sub>in</sub>[D<sub>X</sub>, F<sub>YZ</sub>]</li> <li>Tmp[B, F<sub>YZ</sub>] = <strong>AllReduce</strong><sub>X</sub>(Tmp[B, F<sub>YZ</sub>] {U.X})</li> <li>Out[B, D<sub>X</sub>] {U.YZ} = Tmp[B, F<sub>YZ</sub>] *<sub>F</sub> W2[F<sub>YZ</sub>, D<sub>X</sub>]</li> <li>Out[B, D<sub>XYZ</sub>] = <strong>ReduceScatter</strong><sub>YZ</sub>(Out[B, D<sub>X</sub>] {U.YZ})</li> </ol> </div> <p>Your goal is to work out $T_\text{math}$ and $T_\text{comms}$ for this algorithm and find when it will outperform traditional 3D model sharding?</p> <details><summary>Click here for the answer!</summary> <p>Let’s work out $T_\text{math}$ and $T_\text{comms}$. All our FLOPs are fully sharded so as before we have $T_\text{math} = 4BDF / (N \cdot C)$ but our comms are now</p> \[\begin{align*} T_\text{2D comms} = \frac{2BD}{2X \cdot W_\text{ici}} + \frac{4BF}{YZ \cdot W_\text{ici}} + \frac{2BD}{2X \cdot W_\text{ici}} = \frac{2BD}{X \cdot W_\text{ici}} + \frac{4BF}{YZ \cdot W_\text{ici}} \end{align*}\] <p>where we note that the AllReduce is twice as expensive and we scale our comms by the number of axes over which each operation is performed. Assuming we have freedom to choose our topology and assuming $F=4D$ (as in LLaMA-2), we claim (by some basic calculus) that the optimal values for $X$, $Y$, and $Z$ are $X = \sqrt{N / 8}$, $YZ = \sqrt{8N}$ so the total communication is</p> \[T_\text{2D comms} = \frac{2B}{W_\text{ici}} \left(\frac{D}{X} + \frac{8D}{YZ}\right) = \frac{\sqrt{128} BD}{\sqrt{N} \cdot W_\text{ici}} \approx \frac{11.3 BD}{\sqrt{N} \cdot W_\text{ici}}\] <p>Firstly, copying from above, normal 1D model parallelism would have $T_\text{model parallel comms} = 4BD / (3 \cdot W_\text{ici})$, so when are the new comms smaller? We have</p> \[\begin{align*} T_\text{model parallel comms} &gt; T_\text{2D comms} \iff \frac{4BD}{3 \cdot W_\text{ici}} &gt; \frac{\sqrt{128} BD}{\sqrt{N} \cdot W_\text{ici}} \\ \iff N &gt; 128 \cdot \left(\frac{3}{4}\right)^2 = 81 \end{align*}\] <p>For a general $F$, we claim this condition is</p> \[N &gt; 32 \cdot \left(\frac{F}{D}\right) \cdot \left(\frac{3}{4}\right)^2\] <p>So that tells us if we have more than 81 chips, we’re better off using this new scheme. Now this is a slightly weird result because we’ve historically found ourselves ICI bound at around ~20 way tensor parallelism. But here, even if we’re communication-bound, our total communication continues to decrease with the number of total chips! What this tells us is that we can continuous to increase our chips, increase our batch size, do more parameter scaling, and see reduced latency.</p> </details> <h3 class="next-section">That’s all for Part 7! For Part 8, with a look at how we might serve LLaMA 3 on TPUs, click <a href="../applied-inference">here</a>.</h3> <h2 id="appendix">Appendix</h2> <h3 id="appendix-a-how-real-is-the-batch-size--240-rule">Appendix A: How real is the batch size &gt; 240 rule?</h3> <p>The simple rule we provided above, that our batch size must be greater than 240 tokens to be compute-bound, is roughly true but ignores some ability of the TPU to prefetch the weights while other operations are not using all available HBM, like when doing inter-device communication.</p> <p>Here’s an empirical plot of layer time (in microseconds) for a small Transformer with d<sub>model</sub> 8192, d<sub>ff</sub> 32768, and only 2 matmuls per layer. This comes from <a href="https://colab.sandbox.google.com/drive/1_6krERgtolH7hbUIo7ewAMLlbA4fqEF8?usp=sharing" rel="external nofollow noopener" target="_blank">this Colab notebook</a>. You’ll see that step time increases very slowly up until around batch 240, and then increases linearly.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/batch-scaling-latency-480.webp 480w,/scaling-book/assets/img/batch-scaling-latency-800.webp 800w,/scaling-book/assets/img/batch-scaling-latency-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/batch-scaling-latency.png" class="img-fluid img-small" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Here’s the actual throughput in tokens / us. This makes the argument fairly clearly. Since our layer is about 600M parameters sharded 4 ways here, we’d expect a latency of roughly 365us at minimum.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/batch-scaling-throughput-480.webp 480w,/scaling-book/assets/img/batch-scaling-throughput-800.webp 800w,/scaling-book/assets/img/batch-scaling-throughput-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/batch-scaling-throughput.png" class="img-fluid img-small" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>So at least in this model, we do in fact see throughput increase until about BS240 per data parallel shard.</p> <h3 id="appendix-b-2d-weight-stationary-sharding">Appendix B: 2D Weight Stationary sharding</h3> <p>As the topology grows, if we have access to higher dimensional meshes (like that of TPUs) it is possible to refine this further with “<strong>2D Weight Sharding”</strong>. By introducing a second sharding axis. We call this “<strong>2D Weight Stationary</strong>”, and was described in more detail in the <a href="https://arxiv.org/abs/2211.05102" rel="external nofollow noopener" target="_blank">Efficiently Scaling Transformer Inference paper</a>.</p> <p>Because we’re only sharding the hidden \(F\) dimension in Megatron, it can become significantly smaller than \(E\) (the \(d_\text{model}\) dimension) once the number of chips grows large with 1D sharding. This means at larger batch sizes, it can be more economical to perform a portion of the collectives over the hidden dimension after the first layer of the MLP is applied.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/2d-weight-stationary-480.webp 480w,/scaling-book/assets/img/2d-weight-stationary-800.webp 800w,/scaling-book/assets/img/2d-weight-stationary-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/2d-weight-stationary.png" class="img-fluid img-small" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This figure shows:</p> <ol> <li>1D weight-stationary sharding, a.k.a. Pure Megatron sharding, where activations are fully replicated after AllGather, and weights are fully sharded over the hidden F dimension.</li> <li>2D weight stationary sharding, where weights are sharded over both the hidden F and reduction E dimension, and activations are sharded over the E dimension. We perform an AllGather on the (yz) axis before the first layer, then ReduceScatter on the (x) axis.</li> </ol> <p>For the attention layer, Megatron style sharding is also relatively simple for smaller numbers of chips. However, Megatron happens over the \(n_\text{heads}\) dimension, which puts a limit on the amount of sharding that is possible. Modifying the 2D sharding with for (instead of sharding the hidden, we shard the \(n_\text{heads}\) dimension), we gain the ability to scale further.</p> <h3 id="appendix-c-latency-bound-communications">Appendix C: Latency bound communications</h3> <p>As a recap, in <a href="../sharding">Section 3</a> we derived the amount of time it takes to perform an AllGather into a tensor of size B on each TPU, over X chips on a 1D ring links of full duplex bandwidth of WICI and latency Tmin.</p> \[T_{total} = \max\left(\frac{T_{min} \cdot |X|}{2}, \frac{B}{W_{ICI}}\right)\] <p>For large B, the wall clock stays relatively constant because as you add more chips to the system, you simultaneously scale the amount of data movement necessary to perform the operation and the total bandwidth available.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/all-gather-480.webp 480w,/scaling-book/assets/img/all-gather-800.webp 800w,/scaling-book/assets/img/all-gather-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/all-gather.gif" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Because of the relatively low amounts of data being moved during latency optimized inference, collectives on activations are often bound by the latency term (especially for small batch sizes). One can visualise the latency quite easily, by counting the number of hops we need to complete before it is completed.</p> <p>On TPUs, if the tensor size-dependent part of communication is less than 1 microsecond per hop (a hop is communication between two adjacent devices) we can be bottlenecked by the fixed overhead of actually dispatching the collective. With <code class="language-plaintext highlighter-rouge">4.5e10</code> unidirectional ICI bandwidth, ICI communication becomes latency bound when: \((\text{bytes} / n_\text{shards}) / 4.5e10 &lt; 1e-6\). For 8-way Megatron sharding, this is when <code class="language-plaintext highlighter-rouge">buffer_size &lt; 360kB</code>. <strong>This actually is not that small during inference:</strong> with <code class="language-plaintext highlighter-rouge">BS=16</code> and <code class="language-plaintext highlighter-rouge">D=8192</code> in int8, our activations will use <code class="language-plaintext highlighter-rouge">16*8192=131kB</code>, so we’re already latency bound.</p> <p class="takeaway"><strong>Takeaway:</strong> our comms become latency bound when \(\text{total bytes} &lt; W_{ICI} \times 1e-6\). For instance, with model parallelism over \(Y\), we become bound in int8 when \(Y &gt; BD / 45,000\).</p> <p>There’s a parallel to be drawn here with the compute roofline — we are incurring the fixed cost of some small operations (latency for comms, memory bandwidth for matmuls).</p> <h3 id="appendix-d-speculative-sampling">Appendix D: Speculative Sampling</h3> <p>When we <em>really</em> care about end to end latency, there is one extra trick we can employ called speculative sampling<d-cite key="spec1"></d-cite><d-cite key="spec2"></d-cite>. As a recap, we usually generate tokens from a large Transformer one by one:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/spec-sampling1-480.webp 480w,/scaling-book/assets/img/spec-sampling1-800.webp 800w,/scaling-book/assets/img/spec-sampling1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/spec-sampling1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>With speculative sampling, we use a smaller, cheaper model to generate tokens and then check the result with the big model. This is easiest to understand with <em>greedy decoding</em>:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/spec-sampling2-480.webp 480w,/scaling-book/assets/img/spec-sampling2-800.webp 800w,/scaling-book/assets/img/spec-sampling2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/spec-sampling2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li>We sample greedily from some smaller, cheaper model. Ideally we use a model trained to match the larger model, e.g. by distillation, but it could be as simple as simply using n-grams or token matching a small corpus of text.</li> <li>After we’ve generated K tokens, we use the big model to compute the next-token logits for all the tokens we’ve generated so far.</li> <li>Since we’re decoding greedily, we can just check if the token generated by the smaller model has the highest probability of all possible tokens. If one of the tokens is wrong, we take the longest correct prefix and replace the first wrong token with the correct token, then go back to (1). If all the tokens are correct, we can use the last correct logit to sample an extra token before going back to (1).</li> </ol> <p><strong>Why is this a latency win?</strong> This scheme still requires us to do the FLOPs-equivalent of one forward pass through the big model for every token, but because we can batch a bunch of tokens together, we can do all these FLOPs in one forward pass and take advantage of the fact that we’re <em>not</em> <em>compute-bound</em> to score more tokens for free.</p> <p>Every accepted token becomes more expensive in terms of FLOPs on average (since some will be rejected, and we have to call a draft model), but we wring more FLOPs out of the hardware, and the small model is cheap, so we win overall. Since everything has been checked by the big model, we don’t change the sampling distribution at all (though the exact trajectory will differ for non-greedy).</p> <p>For normal autoregressive sampling the token/s is the same as the step time. We are still beholden to the theoretical minimum step time according to the Arithmetic Intensity section here (in fact, Speculative Sampling step times are usually quite a bit slower than normal autoregressive sampling, but because we get more than 1 token out per step on average we can get much better tokens/s).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/spec-sampling3-480.webp 480w,/scaling-book/assets/img/spec-sampling3-800.webp 800w,/scaling-book/assets/img/spec-sampling3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/spec-sampling3.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure:</b> this figure shows the per-step latency and speculation success rate for Chinchilla (a 70B model from DeepMind) with a 4B parameter drafter (small model). For XSum (a natural language dataset), the ideal amount of speculation is about 3-4 tokens ahead, while HumanEval (a coding dataset) is more predictable and sees wins from more aggressive speculation.</figcaption> </figure> <p><strong>How does this work for non-greedy decoding?</strong> This is a bit more complicated, but essentially boils down to a Metropolis-Hastings inspired algorithm where have \(P_{\text{draft model}}(\text{chosen token})\) and \(P_{\text{target model}}(\text{chosen token})\) derived from the logits, and reject the chosen token probabilistically if the ratio of these probabilities is smaller than some threshold.</p> <p>These <a href="https://arxiv.org/abs/2211.17192" rel="external nofollow noopener" target="_blank">two</a> <a href="https://arxiv.org/abs/2302.01318" rel="external nofollow noopener" target="_blank">papers</a> derived this concurrently and have good examples of how this works in practice.</p> <p class="takeaway"><strong>Takeaway:</strong> Speculative sampling is yet another powerful lever for trading throughput for better per token latency. However, in the scenario where batch size is limited (e.g. small hardware footprint or large KV caches), it becomes a win-win.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> <div class="base-grid appendix-entry"> <h3 style="grid-column: 0;">Miscellaneous</h3> <p class="author-footnote" style="grid-column: text;"><sup>*</sup>Work done at Google DeepMind, now at MatX.</p> </div> <div class="base-grid appendix-entry"> <h3 style="grid-column: 0;">Citation</h3> <p class="author-footnote">For attribution in academic contexts, please cite this work as:</p> <div class="author-footnote"> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c">Austin et al., "How to Scale Your Model", Google DeepMind, online, 2025.</span>
</code></pre></div></div> </div> <p class="author-footnote">or as a BibTeX entry:</p> <div class="author-footnote"> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="nc">@article</span><span class="p">{</span><span class="nl">scaling-book</span><span class="p">,</span>
      <span class="na">title</span> <span class="p">=</span> <span class="s">{How to Scale Your Model}</span><span class="p">,</span>
      <span class="na">author</span> <span class="p">=</span> <span class="s">{Austin, Jacob and Douglas, Sholto and Frostig, Roy and Levskaya, Anselm and Chen, Charlie and Vikram, Sharad
      and Lebron, Federico and Choy, Peter and Ramasesh, Vinay and Webson, Albert and Pope, Reiner}</span><span class="p">,</span>
      <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Google DeepMind}</span><span class="p">,</span>
      <span class="na">howpublished</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
      <span class="na">note</span> <span class="p">=</span> <span class="s">{Retrieved from https://jax-ml.github.io/scaling-book/}</span><span class="p">,</span>
      <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
    <span class="p">}</span>
</code></pre></div></div> </div> </div> </d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/main.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'jax-ml/scaling-book',
        'data-repo-id': '',
        'data-category': 'General',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '0',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-loading': '1',
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/scaling-book/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/scaling-book/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/scaling-book/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/scaling-book/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/scaling-book/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/scaling-book/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/scaling-book/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/scaling-book/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/scaling-book/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>