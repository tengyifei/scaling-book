<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> How to Profile TPU Programs | How To Scale Your Model </title> <meta name="author" content=" "> <meta name="description" content="So far this series has been entirely theoretical: back-of-the-envelope calculations based on hardware rooflines. That understanding gets you far but a lot of optimization comes down to practical details: how the XLA compiler works and how to use profiling tools like the JAX/Tensorboard Profiler to figure out what to do when it fails. We discuss this here."> <meta name="keywords" content="scaling, jax, llms, transformers, tpus, google, deepmind, parallelism, pallas"> <link rel="stylesheet" href="/scaling-book/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/scaling-book/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/scaling-book/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/scaling-book/assets/css/jekyll-pygments-themes-vs.css?4ee1a2facd1a8a76347f4bd43a740500" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/scaling-book/assets/img/favicon.png?fddbd8c2ec231ba2060e67c85de32a55"> <link rel="stylesheet" href="/scaling-book/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jax-ml.github.io/scaling-book/profiling/"> <script src="/scaling-book/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/scaling-book/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/scaling-book/assets/js/distillpub/template.v2.js"></script> <script src="/scaling-book/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">{{page._styles}}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "How to Profile TPU Programs",
            "description": "So far this series has been entirely theoretical: back-of-the-envelope calculations based on hardware rooflines. That understanding gets you far but a lot of optimization comes down to practical details: how the XLA compiler works and how to use profiling tools like the JAX/Tensorboard Profiler to figure out what to do when it fails. We discuss this here.",
            "published": "February 04, 2025",
            "authors": [
              
              {
                "author": "Jacob Austin",
                "authorURL": "https://www.jacobaustin.org/",
                "affiliations": [
                  {
                    "name": "Google DeepMind",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Sholto Douglas",
                "authorURL": "https://x.com/_sholtodouglas",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Roy Frostig",
                "authorURL": "https://cs.stanford.edu/~rfrostig/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Anselm Levskaya",
                "authorURL": "https://anselmlevskaya.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Charlie Chen",
                "authorURL": "https://x.com/charliexychen",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Sharad Vikram",
                "authorURL": "https://sharadvikram.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Federico Lebron",
                "authorURL": "https://fedelebron.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Peter Choy",
                "authorURL": "https://x.com/pchoy95",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Vinay Ramasesh",
                "authorURL": "https://x.com/vinayramasesh",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Albert Webson",
                "authorURL": "https://representation.ai/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Reiner Pope<sup>*</sup>",
                "authorURL": "https://x.com/reinerpope",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <script>
    function goToTop() {
      document.body.scrollTop = 0; // For Safari
      document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
    }

    // When the user scrolls down 20px from the top of the document, show the button
    window.onscroll = function() {scrollFunction()};

    function scrollFunction() {
      // Get the button:
      let mybutton = document.getElementById("top-button");

      if (document.body.scrollTop > 40 || document.documentElement.scrollTop > 40) {
        mybutton.style.display = "block";
      } else {
        mybutton.style.display = "none";
      }
  }
  </script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/scaling-book"> How To Scale Your Model </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="left-button section-button"><a href="../applied-inference"><svg viewbox="-78.5 0 512 512"><path d="M257 64L291 98 128 262 291 426 257 460 61 262 257 64Z"></path></svg></a></div> <div class="right-button section-button"><a href="../jax-stuff"><svg viewbox="-78.5 0 512 512"><path d="M98 460L64 426 227 262 64 98 98 64 294 262 98 460Z"></path></svg></a></div> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/scaling-book/"> </a> </li> <li class="nav-item nav-hidden"><a class="nav-link" onclick="goToTop()" id="top-button" style="display: none;">Back to Top</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../applied-inference">Previous Part</a></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../jax-stuff">Next Part</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Sections </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/scaling-book/index">Part 0. Introduction</a> <a class="dropdown-item " href="/scaling-book/roofline">Part 1. Intro to Rooflines</a> <a class="dropdown-item " href="/scaling-book/tpus">Part 2. All About TPUs</a> <a class="dropdown-item " href="/scaling-book/sharding">Part 3. Sharded Matmuls</a> <a class="dropdown-item " href="/scaling-book/transformers">Part 4. Transformers</a> <a class="dropdown-item " href="/scaling-book/training">Part 5. Training</a> <a class="dropdown-item " href="/scaling-book/applied-training">Part 6. Training LLaMA</a> <a class="dropdown-item " href="/scaling-book/inference">Part 7. Inference</a> <a class="dropdown-item " href="/scaling-book/applied-inference">Part 8. Serving LLaMA</a> <a class="dropdown-item " href="/scaling-book/profiling">Part 9. Profiling</a> <a class="dropdown-item " href="/scaling-book/jax-stuff">Part 10. All About JAX</a> <a class="dropdown-item " href="/scaling-book/conclusion">Part 11. Conclusions</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>How to Profile TPU Programs</h1> <p>Part 9 of <a href="/scaling-book">How To Scale Your Model</a> (<a href="../applied-inference">Part 8: Serving LLaMA</a> | <a href="../jax-stuff">Part 10: JAX</a>)</p> <p>So far this series has been entirely theoretical: back-of-the-envelope calculations based on hardware rooflines. That understanding gets you far but a lot of optimization comes down to practical details: how the XLA compiler works and how to use profiling tools like the JAX/Tensorboard Profiler to figure out what to do when it fails. We discuss this here.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#a-thousand-foot-view-of-the-tpu-software-stack">A Thousand-Foot View of the TPU Software Stack</a> </div> <div> <a href="#the-tensorboard-profiler-a-multi-purpose-tpu-profiler">The TensorBoard Profiler: A Multi-Purpose TPU Profiler</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#trace-viewer">Trace Viewer</a> </li> <li> <a href="#how-to-read-an-xla-op">How to read an XLA op</a> </li> <li> <a href="#graph-viewer">Graph Viewer</a> </li> <li> <a href="#looking-at-a-real-ish-example-profile">Looking at a real(ish) example profile</a> </li> <li> <a href="#memory-profile">Memory Profile</a> </li> </ul> <div> <a href="#worked-problems">Worked Problems</a> </div> </nav> </d-contents> <h2 id="a-thousand-foot-view-of-the-tpu-software-stack">A Thousand-Foot View of the TPU Software Stack</h2> <p>Google exposes a bunch of APIs for programming TPUs, from high level JAX code to low level Pallas or HLO. Most programmers write JAX code exclusively, which lets you write abstract NumPy-style linear algebra programs that are compiled automatically to run efficiently on TPUs.</p> <p>Here’s a simple example, a JAX program that multiplies two matrices together:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>

<span class="k">def</span> <span class="nf">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bf,fd-&gt;db</span><span class="sh">'</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">jit</span><span class="p">(</span><span class="n">multiply</span><span class="p">)(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">)),</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">))</span>
</code></pre></div></div> <p>By calling <code class="language-plaintext highlighter-rouge">jax.jit</code>, we tell JAX to trace this function and emit a lower-level IR called <a href="https://openxla.org/stablehlo" rel="external nofollow noopener" target="_blank">StableHLO</a>, a platform-agnostic IR for ML computation, which is in turn lowered to HLO by the XLA compiler. The compiler runs many passes to determine fusions, layouts, and other factors that result in the HLO that is observable in a JAX profile. This HLO represents all the core linear algebra operations in the JAX code (matmuls, pointwise ops, convolutions, etc) in an LLVM-style graph view. For instance, here is an abridged version of the above program as HLO<d-footnote>To get this HLO, you can run `jax.jit(f).lower(*args, **kwargs).compile().as_text()`.</d-footnote>:</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ENTRY</span> <span class="o">%</span><span class="n">main</span><span class="p">.</span><span class="mi">5</span> <span class="p">(</span><span class="n">Arg_0</span><span class="p">.</span><span class="mi">1</span><span class="o">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span><span class="mi">256</span><span class="p">],</span> <span class="n">Arg_1</span><span class="p">.</span><span class="mi">2</span><span class="o">:</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">256</span><span class="p">,</span><span class="mi">16</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">f32</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="mi">128</span><span class="p">]</span> <span class="p">{</span>
  <span class="o">%</span><span class="n">Arg_1</span><span class="p">.</span><span class="mi">2</span> <span class="o">=</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">256</span><span class="p">,</span><span class="mi">16</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">parameter</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s">"y"</span><span class="p">}</span>
  <span class="o">%</span><span class="n">convert</span><span class="p">.</span><span class="mi">3</span> <span class="o">=</span> <span class="n">f32</span><span class="p">[</span><span class="mi">256</span><span class="p">,</span><span class="mi">16</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">convert</span><span class="p">(</span><span class="n">bf16</span><span class="p">[</span><span class="mi">256</span><span class="p">,</span><span class="mi">16</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">Arg_1</span><span class="p">.</span><span class="mi">2</span><span class="p">),</span>
  <span class="o">%</span><span class="n">Arg_0</span><span class="p">.</span><span class="mi">1</span> <span class="o">=</span> <span class="n">f32</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span><span class="mi">256</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">parameter</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s">"x"</span><span class="p">}</span>
  <span class="n">ROOT</span> <span class="o">%</span><span class="n">dot</span><span class="p">.</span><span class="mi">4</span> <span class="o">=</span> <span class="n">f32</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="mi">128</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">dot</span><span class="p">(</span><span class="n">f32</span><span class="p">[</span><span class="mi">256</span><span class="p">,</span><span class="mi">16</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">convert</span><span class="p">.</span><span class="mi">3</span><span class="p">,</span> <span class="n">f32</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span><span class="mi">256</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">Arg_0</span><span class="p">.</span><span class="mi">1</span><span class="p">),</span> <span class="n">lhs_contracting_dims</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">},</span> <span class="n">rhs_contracting_dims</span><span class="o">=</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span>
<span class="p">}</span>
</code></pre></div></div> <p>We’ll explain the syntax of HLO in just a second, but for now just note that it actually matches the JAX code above fairly well. For instance,</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ROOT</span> <span class="o">%</span><span class="n">dot</span><span class="p">.</span><span class="mi">4</span> <span class="o">=</span> <span class="n">f32</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="mi">128</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">dot</span><span class="p">(</span><span class="n">f32</span><span class="p">[</span><span class="mi">256</span><span class="p">,</span><span class="mi">16</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">convert</span><span class="p">.</span><span class="mi">3</span><span class="p">,</span> <span class="n">f32</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span><span class="mi">256</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">Arg_0</span><span class="p">.</span><span class="mi">1</span><span class="p">),</span> <span class="n">lhs_contracting_dims</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">},</span> <span class="n">rhs_contracting_dims</span><span class="o">=</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span>
</code></pre></div></div> <p>is the actual matmul above that multiplies two f32 matrices along the 0 and 1 dimension, respectively.</p> <p><strong>To transform this HLO to code that can be executed on the TPU, the XLA compiler first lowers it to LLO</strong> (low-level optimizer) IR. LLO programs the TPU directly, scheduling copies between memories, pushing arrays onto the systolic array, etc. LLO code contains primitives that push buffers into the systolic array, pull results off, and schedule DMAs that communicate between different pieces of TPU memory. Once this has been lowered to LLO, it is then compiled to machine code that is loaded into the TPU IMEM and executed.</p> <p>When a program is running slower than we’d like, we primarily work with the JAX level to improve performance. Doing so, however, often requires us to understand some of the semantics of HLO and how the code is actually running on the TPU. When something goes wrong at a lower level, we pull yet another escape hatch and write custom kernels in <a href="https://jax.readthedocs.io/en/latest/pallas/tpu/details.html" rel="external nofollow noopener" target="_blank">Pallas</a>. To view the HLO of a program and its runtime statistics, we use the JAX profiler.</p> <h2 id="the-jax-profiler-a-multi-purpose-tpu-profiler">The JAX Profiler: A Multi-Purpose TPU Profiler</h2> <p>JAX provides a multi-purpose TPU profiler with a bunch of useful tools for understanding what’s happening on the TPU when a program is run. You can using the <code class="language-plaintext highlighter-rouge">jax.profiler</code> module to trace a program as it’s running and record everything from the duration of each subcomponent, the HLO of each program, memory usage, and more. For example, this code will dump a trace to a file in <code class="language-plaintext highlighter-rouge">/tmp/tensorboard</code> that can be viewed in TensorBoard (<a href="https://jax.readthedocs.io/en/latest/../profiling#tensorboard-profiling" rel="external nofollow noopener" target="_blank">here</a> is a step-by-step guide).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="k">with</span> <span class="n">jax</span><span class="p">.</span><span class="n">profiler</span><span class="p">.</span><span class="nf">trace</span><span class="p">(</span><span class="sh">"</span><span class="s">/tmp/tensorboard</span><span class="sh">"</span><span class="p">):</span>
  <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">))</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">x</span>
  <span class="n">y</span><span class="p">.</span><span class="nf">block_until_ready</span><span class="p">()</span>

<span class="c1"># Now you can load TensorBoard in a Google Colab with
#
# !pip install tensorboard-plugin-profile
# %load_ext tensorboard
# %tensorboard --logdir=/tmp/tensorboard
#
# or externally with
#
# &gt; tensorboard --logdir=/tmp/tensorboard
#
</span></code></pre></div></div> <p>Here’s an overview of what you can do in the profiler:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/xprof-overview-480.webp 480w,/scaling-book/assets/img/xprof-overview-800.webp 800w,/scaling-book/assets/img/xprof-overview-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/xprof-overview.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Once in TensorBoard, the profiler has a few key tabs that help you understand your program:</p> <ol> <li> <strong>Trace Viewer</strong> shows a detailed timeline of what’s actually happening on the TPU as a timeline.</li> <li> <strong>Graph Viewer</strong> shows the HLO graph, letting you see what parts of the program feed into each other and how things are sharded.</li> <li> <strong>Memory Profile and Memory Viewer:</strong> these show how much memory your program is using.</li> </ol> <p>While it’s slightly difficult to share profiles, <a href="https://ui.perfetto.dev/#!/?s=fa9f13b487bde622707c1a503f9227c34594760a" rel="external nofollow noopener" target="_blank">here</a> is a Perfetto link that contains at least the Trace Viewer component for a simple Transformer. <a href="https://colab.research.google.com/drive/1_6krERgtolH7hbUIo7ewAMLlbA4fqEF8?usp=sharing" rel="external nofollow noopener" target="_blank">This Colab</a> lets you generate the full JAX/TensorBoard trace and play around with it.</p> <h3 id="trace-viewer">Trace Viewer</h3> <p><strong>The Trace Viewer is probably the most useful part of the profiler.</strong> The example below shows a simple Transformer with pieces annotated. Names come from labels provided in the code.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/trace-viewer-480.webp 480w,/scaling-book/assets/img/trace-viewer-800.webp 800w,/scaling-book/assets/img/trace-viewer-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/trace-viewer.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The Trace Viewer shows a chronological timeline of all the actions on each TPU core. We’re only looking at TPU:0 here, since typically all TPUs execute the same instructions. A few key notes:</p> <ol> <li>The top row (XLA Ops) shows the actual TPU operations (the names are HLO names). Everything else is an approximate trace based on <code class="language-plaintext highlighter-rouge">jax.named_scope</code>, <code class="language-plaintext highlighter-rouge">jax.named_call</code>, and the Python stack trace.</li> <li>Noting the repeated blocks, we can isolate a single layer here. We can also see (from looking at the code/understanding how a Transformer works) what parts are attention and what parts are MLPs.</li> <li>By clicking on an XLA op, we can view where in the code it comes from (useful for understanding the trace) and see links to the Graph viewer.</li> </ol> <p class="takeaway"><strong>Tip:</strong> you can navigate the Trace Viewer using “video game” style controls, with A/D panning left and right, and W/S zooming in and out. These controls make navigating a lot easier.</p> <h3 id="how-to-read-an-xla-op">How to read an XLA op</h3> <p>HLO isn’t actually very hard to read, and it’s very helpful for understanding what a given part of the trace above corresponds to. Here’s an example op called fusion.3.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">fusion</span><span class="p">.</span><span class="mi">3</span> <span class="o">=</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">4096</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="nc">T</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">128</span><span class="p">)(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="nc">S</span><span class="p">(</span><span class="mi">1</span><span class="p">)}</span> <span class="nf">fusion</span><span class="p">(</span><span class="n">bf16</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">8192</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="nc">T</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">128</span><span class="p">)(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="nc">S</span><span class="p">(</span><span class="mi">1</span><span class="p">)}</span> <span class="o">%</span><span class="n">fusion</span><span class="p">.</span><span class="mi">32</span><span class="p">),</span> <span class="n">kind</span><span class="o">=</span><span class="n">kCustom</span><span class="p">,</span> <span class="n">calls</span><span class="o">=%</span><span class="nb">all</span><span class="o">-</span><span class="nb">reduce</span><span class="o">-</span><span class="n">scatter</span><span class="p">.</span><span class="mi">3</span>
</code></pre></div></div> <p>Let’s break this down into its pieces.</p> <ul> <li> <strong>Op Name</strong>: fusion.3 <ul> <li>A dot or fusion op is a set of operations containing at most 1 matrix multiplication and possibly a bunch of related pointwise VPU-ops.</li> </ul> </li> <li> <strong>Shape/layout</strong>: <code class="language-plaintext highlighter-rouge">bf16[32,32,4096]</code> <ul> <li>This is the output shape of the op. We can see the dtype is bf16 (2 bytes per parameter) and <code class="language-plaintext highlighter-rouge">[32,32,4096</code>] is the shape.</li> </ul> </li> <li> <strong>Layout:</strong> <code class="language-plaintext highlighter-rouge">{2,1,0:T(8,128)(2,1)}</code> <ul> <li> <code class="language-plaintext highlighter-rouge">{2,1,0:T(8,128)(2,1)}</code> tells us the order of the axes in memory (column major, row major, etc.) and the array padding. More below.</li> </ul> </li> <li> <strong>Memory location:</strong> S(1) <ul> <li>S(1) tells us this array lives in VMEM. S(0) (sometimes omitted) is HBM. S(2) and S(3) are other memory spaces.</li> </ul> </li> <li> <strong>Arguments</strong>: <code class="language-plaintext highlighter-rouge">bf16[32,32,8192]{2,1,0:T(8,128)(2,1)S(1)} %fusion.32</code> <ul> <li>This op has one input, a bf16 array called fusion.32 with a particular shape. This tells us what function feeds into this one.</li> </ul> </li> </ul> <p>Let’s try to understand this notation a little more. Let’s take this as a simple example:</p> <p><code class="language-plaintext highlighter-rouge">f32[3,5]{1,0:T(2,2)}</code></p> <p>which again tells us that this Op returns a float32 array of shape <code class="language-plaintext highlighter-rouge">[3, 5]</code> with a particular tiling <code class="language-plaintext highlighter-rouge">{1,0:T(2,2)}</code>. While tilings don’t matter <em>too</em> much, briefly, tilings tell us how an N-dimensional array is laid out sequentially in memory. Here’s a diagram showing how this array is laid out:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/tiling-480.webp 480w,/scaling-book/assets/img/tiling-800.webp 800w,/scaling-book/assets/img/tiling-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/tiling.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>T(2,2) tells us that the array is tiled in chunks of <code class="language-plaintext highlighter-rouge">(2, 2)</code> where within each chunk, the array has rows first (<strong>row-major</strong>), then columns, i.e. <code class="language-plaintext highlighter-rouge">(0, 0)</code> is followed by <code class="language-plaintext highlighter-rouge">(0, 1)</code>, then <code class="language-plaintext highlighter-rouge">(1, 0)</code> and <code class="language-plaintext highlighter-rouge">(1,1)</code>. Because of the <code class="language-plaintext highlighter-rouge">T(2, 2)</code> tiling, the array is padded to <code class="language-plaintext highlighter-rouge">[4, 6]</code>, expanding its memory usage by about 1.6x. For the big bf16 array given above, <code class="language-plaintext highlighter-rouge">bf16[32,32,8192]{2,1,0:T(8,128)(2,1)S(1)}</code>, we do <code class="language-plaintext highlighter-rouge">T(8,128)(2,1)</code> which tells us the array has two levels of tiling, an outer <code class="language-plaintext highlighter-rouge">(8, 128)</code> tiling and an inner <code class="language-plaintext highlighter-rouge">(2, 1)</code> tiling within that unit (used for bf16 so our loads are always multiples of 4 bytes). For example, here’s <code class="language-plaintext highlighter-rouge">bf16[4,8]{1,0,T(2,4)(2,1)}</code> (colors are (2,4) tiles, red boxes are (2,1) tiles):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/tiling2-480.webp 480w,/scaling-book/assets/img/tiling2-800.webp 800w,/scaling-book/assets/img/tiling2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/tiling2.png" class="img-fluid img-small" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Tiling can affect how efficiently chunks of tensors can be loaded into VMEM and XLA will sometimes introduce copies that “retile” or “re-layout” a tensor inside a program, sometimes at non-trivial overhead.<d-footnote>JAX provides an experimental feature to work around this issue, by allowing XLA to compute its "preferred" layout for inputs to a program. When you "just-in-time" compile a program with `jax.jit`, you typically pass in "mock" inputs that tell JAX what shape and dtype to expect. These typically also carry tiling information that may not be optimal. Instead, you can specify the input layouts as AUTO, and `jax.jit` will return a layout that the jitted program prefers. You can then explicitly load the tensor in that layout to avoid inducing copies within the program.</d-footnote></p> <h3 id="graph-viewer">Graph Viewer</h3> <p>While some of the fusions above can seem complicated, the XLA Graph Viewer makes them easier to parse. For example here’s the view of a fairly complicated fusion:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/graph-viewer-480.webp 480w,/scaling-book/assets/img/graph-viewer-800.webp 800w,/scaling-book/assets/img/graph-viewer-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/graph-viewer.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>It’s really helpful to stare at a bunch of HLO graphs and try to map HLO ops onto the code you’re profiling. By hovering over a box you’ll often see the line of code where the function was defined.</p> <h3 id="looking-at-a-realish-example-profile">Looking at a real(ish) example profile</h3> <p><a href="https://colab.research.google.com/drive/1_6krERgtolH7hbUIo7ewAMLlbA4fqEF8?usp=sharing" rel="external nofollow noopener" target="_blank">This Colab</a> has an example profile for a fake Transformer. <a href="https://ui.perfetto.dev/#!/?s=fa9f13b487bde622707c1a503f9227c34594760a" rel="external nofollow noopener" target="_blank">Here’s</a> a Perfetto link to at least see the Trace Viewer if you’re in a hurry. I’ve gone to more effort than usual to annotate the trace with <code class="language-plaintext highlighter-rouge">jax.named_scope</code> calls so you can identify what’s going on.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/transformer-xprof-480.webp 480w,/scaling-book/assets/img/transformer-xprof-800.webp 800w,/scaling-book/assets/img/transformer-xprof-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/transformer-xprof.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Take a look at the profile and try to really understand what each part is doing. Let’s break it down a bit, starting with the FFW block:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/transformer-ffw-480.webp 480w,/scaling-book/assets/img/transformer-ffw-800.webp 800w,/scaling-book/assets/img/transformer-ffw-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/transformer-ffw.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Here we’ve zoomed into the FFW block. You’ll see the up-projection Op is a fusion (matmul) with inputs <code class="language-plaintext highlighter-rouge">bf16[8, 1024, 8192]</code> and <code class="language-plaintext highlighter-rouge">bf16[8192, 16384]</code> and output <code class="language-plaintext highlighter-rouge">bf16[32, 1024, 16384]</code>. I know (because I wrote this code) that this is a local view of a 4-way DP, 2-way MP sharded matmul, so we’re actually doing</p> <p><strong>X:</strong> <code class="language-plaintext highlighter-rouge">bf16[32, 1024, 8192]</code> * <strong>W<sub>in</sub></strong>: <code class="language-plaintext highlighter-rouge">bf16[8192, 32768]</code> -&gt; <strong>Tmp</strong>: <code class="language-plaintext highlighter-rouge">bf16[32, 1024, 32768]</code></p> <p><strong>How long do we expect this to take?</strong> First of all, our batch size per data parallel shard is <code class="language-plaintext highlighter-rouge">8 * 1024 = 8192</code>, so we should be solidly compute-bound. This is on 8 TPUv2 cores (freely available on Google Colab), so we expect it to take about <code class="language-plaintext highlighter-rouge">2 * 32 * 1024 * 8192 * 32768 / (23e12 * 8) = 95.6ms</code> which is pretty much exactly how long it takes (96ms). That’s great! That means we’re getting fantastic FLOPs utilization!</p> <p><strong>What about communication?</strong> You’ll notice the little fusion hidden at the end of the second matmul. If we click on it, you’ll see</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">fusion</span><span class="p">.</span><span class="mi">1</span> <span class="o">=</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span><span class="mi">1024</span><span class="p">,</span><span class="mi">4096</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="nc">T</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">128</span><span class="p">)(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)}</span> <span class="nf">fusion</span><span class="p">(</span><span class="n">bf16</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span><span class="mi">1024</span><span class="p">,</span><span class="mi">8192</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="nc">T</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">128</span><span class="p">)(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)}</span> <span class="o">%</span><span class="n">fusion</span><span class="p">.</span><span class="mi">31</span><span class="p">),</span> <span class="n">kind</span><span class="o">=</span><span class="n">kCustom</span><span class="p">,</span> <span class="n">calls</span><span class="o">=%</span><span class="nb">all</span><span class="o">-</span><span class="nb">reduce</span><span class="o">-</span><span class="n">scatter</span><span class="p">.</span><span class="mi">1</span>
</code></pre></div></div> <p>which is basically a little ReduceScatter (here’s the GraphViewer);</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/reduce-scatter-xprof-480.webp 480w,/scaling-book/assets/img/reduce-scatter-xprof-800.webp 800w,/scaling-book/assets/img/reduce-scatter-xprof-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/reduce-scatter-xprof.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>How long do we expect this to take? Well, we’re doing a ReduceScatter on a TPUv2 4x2, which should require only one hop on 1.2e11 bidirectional bandwidth. The array has size <code class="language-plaintext highlighter-rouge">2*32*1024*8192</code> with the batch axis sharded 4 ways, so each shard is <code class="language-plaintext highlighter-rouge">2*8*1024*8192=134MB</code>. So this should take roughly 1.1ms. <strong>How long does it actually take?</strong> 1.13ms reported in the profile. So we’re really close to the roofline!</p> <p><strong>Let’s look at attention too!</strong> Here’s a profile of the attention component:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/attn-xprof-480.webp 480w,/scaling-book/assets/img/attn-xprof-800.webp 800w,/scaling-book/assets/img/attn-xprof-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/attn-xprof.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>I’ve clicked on the Q projection op, which uses a matrix \(W_Q\) of shape [d<sub>model</sub> = 8192, n<sub>heads</sub> = 32, d<sub>qkv</sub> = 256]. We’re Megatron sharding along the head dimension. Try to do the same exercise of calculating how long these should take.</p> <h3 id="memory-profile">Memory Profile</h3> <p>The Memory Profile makes it easy to see the program memory as a function of time. This is helpful for debugging OOMs. You can see here about 7.5GB allocated to model parameters and about 10GB free. So we can fit a lot more into memory.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/memory-viewer-480.webp 480w,/scaling-book/assets/img/memory-viewer-800.webp 800w,/scaling-book/assets/img/memory-viewer-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/memory-viewer.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="worked-problems">Worked Problems</h2> <p><strong>Question 1</strong>: take a look at <a href="https://colab.research.google.com/drive/1LfLO3OTr-_MWFPxUN36KJ3cqH0BcAoli?usp=sharing" rel="external nofollow noopener" target="_blank">this</a> Colab/profile and figure out what looks suspicious and what’s going on here. Can you tell me exactly what computations are happening and what each operation is doing? What are the true shapes of each matrix involved and how are they sharded? <em>Try looking at the profile first without reading the code.</em></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/all-reduce-profile-480.webp 480w,/scaling-book/assets/img/all-reduce-profile-800.webp 800w,/scaling-book/assets/img/all-reduce-profile-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/all-reduce-profile.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <details><summary>Click here for the answer.</summary> <p>This is two matrix multiplications, i.e. specifically this:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">wf,bf-&gt;bw</span><span class="sh">'</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">fw,bw-&gt;bf</span><span class="sh">'</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
</code></pre></div></div> <p>You can see a reduce, two big fusions, and an all-reduce. The first big fusion is:</p> <p><code class="language-plaintext highlighter-rouge">%fusion.1 = bf16[4096]{0:T(1024)(128)(2,1)} fusion(bf16[4096,8192]{1,0:T(8,128)(2,1)} %param.1, bf16[8192]{0:T(1024)(128)(2,1)} %reduce.6), kind=kLoop, calls=%fused_computation.1</code></p> <p>which tells us the per-shard shape is <code class="language-plaintext highlighter-rouge">bf16[8192] * bf16[4096, 8192] -&gt; bf16[4096]</code> (over the 8192 dimension). By observing the final AllReduce with <code class="language-plaintext highlighter-rouge">replica_groups=\{\{0,16,32,48,64,80,96,112\}, ...\}</code>, we can tell we’re doing 8-way model parallelism, so the true shapes are <code class="language-plaintext highlighter-rouge">[8, 8192] * bf16[32,768, 8192] -&gt; bf16[8, 32,768]</code>.</p> </details> <p><strong>Question 2:</strong> <a href="https://colab.research.google.com/drive/1_6krERgtolH7hbUIo7ewAMLlbA4fqEF8?usp=sharing" rel="external nofollow noopener" target="_blank">The Transformer Colab from earlier</a> implements a simple mock Transformer. Follow the instructions in the Colab and get a benchmark of the naive Transformer with GSPMD partitioning. How long does each part take? How long should it take? What sharding is being used. Try fixing the sharding! <em>Hint: use <code class="language-plaintext highlighter-rouge">jax.lax.with_sharding_constraints</code> to constrain the behavior. With this fix, what’s the best MXU you can get?</em></p> <p>For reference, the initial version gets roughly 184ms / layer and the optimized profile gets 67 ms / layer. Once you’ve done this, try staring at the profile and see if you can answer these questions purely from the profile:</p> <ul> <li>What sharding strategy is this?</li> <li>What is the batch size, \(d_\text{model}\), \(d_\text{ff}\)?</li> <li>What fraction of time is spent on attention vs. the MLP block?</li> <li>What fraction of time should be spent on each op at the roofline?</li> </ul> <h3 class="next-section">That’s all for Part 9. For Part 10, with a deep dive into JAX parallelism, click <a href="../jax-stuff">here</a>.</h3> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> <div class="base-grid appendix-entry"> <h3 style="grid-column: 0;">Miscellaneous</h3> <p class="author-footnote" style="grid-column: text;"><sup>*</sup>Work done at Google DeepMind, now at MatX.</p> </div> <div class="base-grid appendix-entry"> <h3 style="grid-column: 0;">Citation</h3> <p class="author-footnote">For attribution in academic contexts, please cite this work as:</p> <div class="author-footnote"> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c">Austin et al., "How to Scale Your Model", Google DeepMind, online, 2025.</span>
</code></pre></div></div> </div> <p class="author-footnote">or as a BibTeX entry:</p> <div class="author-footnote"> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="nc">@article</span><span class="p">{</span><span class="nl">scaling-book</span><span class="p">,</span>
      <span class="na">title</span> <span class="p">=</span> <span class="s">{How to Scale Your Model}</span><span class="p">,</span>
      <span class="na">author</span> <span class="p">=</span> <span class="s">{Austin, Jacob and Douglas, Sholto and Frostig, Roy and Levskaya, Anselm and Chen, Charlie and Vikram, Sharad
      and Lebron, Federico and Choy, Peter and Ramasesh, Vinay and Webson, Albert and Pope, Reiner}</span><span class="p">,</span>
      <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Google DeepMind}</span><span class="p">,</span>
      <span class="na">howpublished</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
      <span class="na">note</span> <span class="p">=</span> <span class="s">{Retrieved from https://jax-ml.github.io/scaling-book/}</span><span class="p">,</span>
      <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
    <span class="p">}</span>
</code></pre></div></div> </div> </div> </d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'jax-ml/scaling-book',
        'data-repo-id': '',
        'data-category': 'General',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '0',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-loading': '1',
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/scaling-book/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/scaling-book/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/scaling-book/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/scaling-book/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/scaling-book/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/scaling-book/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/scaling-book/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/scaling-book/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/scaling-book/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>