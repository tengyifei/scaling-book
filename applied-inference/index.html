<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Serving LLaMA 3-70B on TPUs | How To Scale Your Model </title> <meta name="author" content=" "> <meta name="description" content="Let's take a close look at how we'd serve LLaMA 3-70B models on TPU v5e. How expensive are different models to serve at roofline? How large are their KV caches? What batch sizes should we use? How are the parameters and activations sharded during inference? Let's work through some back-of-the-envelope estimates for latency and throughput in production."> <meta name="keywords" content="scaling, jax, llms, transformers, tpus, google, deepmind, parallelism, pallas"> <link rel="stylesheet" href="/scaling-book/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/scaling-book/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/scaling-book/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/scaling-book/assets/css/jekyll-pygments-themes-vs.css?4ee1a2facd1a8a76347f4bd43a740500" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/scaling-book/assets/img/favicon.png?fddbd8c2ec231ba2060e67c85de32a55"> <link rel="stylesheet" href="/scaling-book/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jax-ml.github.io/scaling-book/applied-inference/"> <script src="/scaling-book/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/scaling-book/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/scaling-book/assets/js/distillpub/template.v2.js"></script> <script src="/scaling-book/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">{{page._styles}}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Serving LLaMA 3-70B on TPUs",
            "description": "Let's take a close look at how we'd serve LLaMA 3-70B models on TPU v5e. How expensive are different models to serve at roofline? How large are their KV caches? What batch sizes should we use? How are the parameters and activations sharded during inference? Let's work through some back-of-the-envelope estimates for latency and throughput in production.",
            "published": "February 04, 2025",
            "authors": [
              
              {
                "author": "Jacob Austin",
                "authorURL": "https://www.jacobaustin.org/",
                "affiliations": [
                  {
                    "name": "Google DeepMind",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Sholto Douglas",
                "authorURL": "https://x.com/_sholtodouglas",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Roy Frostig",
                "authorURL": "https://cs.stanford.edu/~rfrostig/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Anselm Levskaya",
                "authorURL": "https://anselmlevskaya.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Charlie Chen",
                "authorURL": "https://x.com/charliexychen",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Sharad Vikram",
                "authorURL": "https://sharadvikram.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Federico Lebron",
                "authorURL": "https://fedelebron.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Peter Choy",
                "authorURL": "https://x.com/pchoy95",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Vinay Ramasesh",
                "authorURL": "https://x.com/vinayramasesh",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Albert Webson",
                "authorURL": "https://representation.ai/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Reiner Pope<sup>*</sup>",
                "authorURL": "https://x.com/reinerpope",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <script>
    function goToTop() {
      document.body.scrollTop = 0; // For Safari
      document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
    }

    // When the user scrolls down 20px from the top of the document, show the button
    window.onscroll = function() {scrollFunction()};

    function scrollFunction() {
      // Get the button:
      let mybutton = document.getElementById("top-button");

      if (document.body.scrollTop > 40 || document.documentElement.scrollTop > 40) {
        mybutton.style.display = "block";
      } else {
        mybutton.style.display = "none";
      }
  }
  </script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/scaling-book"> How To Scale Your Model </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="left-button section-button"><a href="../inference"><svg viewbox="-78.5 0 512 512"><path d="M257 64L291 98 128 262 291 426 257 460 61 262 257 64Z"></path></svg></a></div> <div class="right-button section-button"><a href="../profiling"><svg viewbox="-78.5 0 512 512"><path d="M98 460L64 426 227 262 64 98 98 64 294 262 98 460Z"></path></svg></a></div> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/scaling-book/"> </a> </li> <li class="nav-item nav-hidden"><a class="nav-link" onclick="goToTop()" id="top-button" style="display: none;">Back to Top</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../inference">Previous Part</a></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../profiling">Next Part</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Sections </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/scaling-book/index">Part 0. Introduction</a> <a class="dropdown-item " href="/scaling-book/roofline">Part 1. Intro to Rooflines</a> <a class="dropdown-item " href="/scaling-book/tpus">Part 2. All About TPUs</a> <a class="dropdown-item " href="/scaling-book/sharding">Part 3. Sharded Matmuls</a> <a class="dropdown-item " href="/scaling-book/transformers">Part 4. Transformers</a> <a class="dropdown-item " href="/scaling-book/training">Part 5. Training</a> <a class="dropdown-item " href="/scaling-book/applied-training">Part 6. Training LLaMA</a> <a class="dropdown-item " href="/scaling-book/inference">Part 7. Inference</a> <a class="dropdown-item " href="/scaling-book/applied-inference">Part 8. Serving LLaMA</a> <a class="dropdown-item " href="/scaling-book/profiling">Part 9. Profiling</a> <a class="dropdown-item " href="/scaling-book/jax-stuff">Part 10. All About JAX</a> <a class="dropdown-item " href="/scaling-book/conclusion">Part 11. Conclusions</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Serving LLaMA 3-70B on TPUs</h1> <p>Part 8 of <a href="/scaling-book">How To Scale Your Model</a> (<a href="../inference">Part 7: Inference</a> | <a href="../profiling">Part 9: Profiling</a>)</p> <p>Let's take a close look at how we'd serve LLaMA 3-70B models on TPU v5e. How expensive are different models to serve at roofline? How large are their KV caches? What batch sizes should we use? How are the parameters and activations sharded during inference? Let's work through some back-of-the-envelope estimates for latency and throughput in production.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#what-s-the-llama-serving-story">What's the LLaMA Serving Story?</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#thinking-about-throughput">Thinking about throughput</a> </li> <li> <a href="#what-about-prefill">What about prefill?</a> </li> </ul> <div> <a href="#visualizing-the-latency-throughput-tradeoff">Visualizing the Latency Throughput Tradeoff</a> </div> <div> <a href="#worked-problems">Worked Problems</a> </div> </nav> </d-contents> <p><em>This section will look at what it takes to serve LLaMA-3 and how efficiently it can be done. As in the previous “applied” section, try to work out the answers on your with a pen and paper before looking them up!</em></p> <h2 id="whats-the-llama-serving-story">What’s the LLaMA Serving Story?</h2> <p>Let’s remind ourselves what LLaMA 3-70B looks like (see <a href="../applied-training">Section 6</a> for reference):</p> <table> <thead> <tr> <th><strong>hyperparam</strong></th> <th style="text-align: center"><strong>value</strong></th> </tr> </thead> <tbody> <tr> <td>\(n_\text{layers}\) (L)</td> <td style="text-align: center">80</td> </tr> <tr> <td>\(d_\text{model}\) (D)</td> <td style="text-align: center">8,192</td> </tr> <tr> <td>\(d_{ff}\) (F)</td> <td style="text-align: center">28,672</td> </tr> <tr> <td>\(n_\text{heads}\) (N)</td> <td style="text-align: center">64</td> </tr> <tr> <td>\(n_\text{kv heads}\) (K)</td> <td style="text-align: center">8</td> </tr> <tr> <td>\(d_\text{qkv}\) (H)</td> <td style="text-align: center">128</td> </tr> <tr> <td>\(n_\text{embeddings}\) (V)</td> <td style="text-align: center">128,256</td> </tr> </tbody> </table> <p>Let’s start with a simple question: <strong>what hardware should we serve on?</strong> The answer is basically, whichever is cheapest in FLOPs / dollar.<d-footnote>This isn't always true, sometimes more HBM or ICI bandwidth is critical rather than FLOPs, but this is a good heuristic.</d-footnote> For this reason, we typically want to serve on TPU v5e, our current dedicated inference chip (cost comes from <a href="https://cloud.google.com/tpu/pricing" rel="external nofollow noopener" target="_blank">Google Cloud pricing</a> as of February 2025):</p> <table> <thead> <tr> <th><strong>TPU type</strong></th> <th style="text-align: center"><strong>bfloat16 FLOPs/s</strong></th> <th style="text-align: center"><strong>Google Cloud USD / hour</strong></th> <th style="text-align: center"><strong>FLOPs / $</strong></th> </tr> </thead> <tbody> <tr> <td>H100</td> <td style="text-align: center">9.9e14</td> <td style="text-align: center">$10.8</td> <td style="text-align: center">9.1e13</td> </tr> <tr> <td>v5p</td> <td style="text-align: center">4.59e14</td> <td style="text-align: center">$4.2</td> <td style="text-align: center">1.09e14</td> </tr> <tr> <td>v5e</td> <td style="text-align: center">1.97e14</td> <td style="text-align: center">$1.2</td> <td style="text-align: center"><strong>1.64e14</strong></td> </tr> </tbody> </table> <p>Each TPU v5e has 16GB of HBM which will require us to shard our model fairly aggressively. Let’s start by thinking about some basic quantities that might matter for us:</p> <p><strong>Question:</strong> How large are LLaMA 3-70B’s KV caches per token? <em>You can assume we store them in int8. This determines how large our batch size can be on a given topology.</em></p> <details><summary>Click here once you’ve thought it through!</summary> <p>LLaMA 3-70B has 8 KV heads, so the size per token is <code class="language-plaintext highlighter-rouge">2 * K * H * L = 2 * 8 * 128 * 80 = 160kB</code>.</p> <p><strong>Note just how big this is!</strong> If we have a sequence length of 32k tokens (as is common), this uses <code class="language-plaintext highlighter-rouge">162e3 * 32,768 = 5.3GB / sequence</code>. For BS=240, this is 1.3TB! Since TPU v5e only have 16GB a piece, we would need about <code class="language-plaintext highlighter-rouge">(70e9 + 1.3e12) / 16e9 = 86</code> TPU v5e chips to even fit this much memory. Also note how large this is compared to the 70GB of model parameters.</p> </details> <p><strong>Question:</strong> Let’s say we want to serve L3 70B at batch size 32 and 8192 sequence length with everything (params and KVs) in int8. How much total memory will this use? What’s the smallest slice we could serve this on?</p> <details><summary>Answer</summary> <p>Since our KVs are <code class="language-plaintext highlighter-rouge">160e3</code> bytes in int8, our total KV memory is <code class="language-plaintext highlighter-rouge">160e3 * 8192 * 32 = 41.9e9</code> bytes. Our parameters are <code class="language-plaintext highlighter-rouge">70e9</code> bytes, since we have 1 byte per parameter. Thus, our total memory usage is <code class="language-plaintext highlighter-rouge">41.9e9 + 70e9 = 112GB</code>.</p> <p>The smallest slice we could use would have <code class="language-plaintext highlighter-rouge">112e9 / 16e9 = 7</code> TPUs, or (rounding to an even size), TPU v5e <code class="language-plaintext highlighter-rouge">4x2</code>. This will be a tight fit and we might not be able to quite fit this accounting for other overhead, so we might need a <code class="language-plaintext highlighter-rouge">4x4</code> at minimum (or to drop the batch size).</p> </details> <p><strong>Question:</strong> At this batch size and quantization on a TPU v5e <code class="language-plaintext highlighter-rouge">4x2</code>, roughly what latency would we expect per decode step? What throughput (tokens / sec / chip). What about a <code class="language-plaintext highlighter-rouge">4x4</code>? <em>Assume we perform our FLOPs in bfloat16 and everything is fully sharded.</em></p> <details><summary>Answer</summary> <p>We can invoke the formula from the previous section that</p> \[\begin{align*} \tiny \text{Theoretical Step Time (General)} = \underbrace{\frac{\text{Batch Size} \times \text{KV Cache Size}}{\tiny \text{Total Memory Bandwidth}}}_{\text{Attention (always bandwidth-bound)}} + \underbrace{\max\left(\frac{2 \times \text{Batch Size} \times \text{Parameter Count}}{\text{Total FLOPs/s}}, \frac{\text{Parameter Size}}{\text{Total Memory Bandwidth}}\right)}_{\tiny \text{MLP (can be compute-bound)}} \end{align*}\] <p>Here our critical batch size will be about 120 since our parameters are in int8 but our FLOPs are in bfloat16. We could also manually calculate the RHS maximum, but that’s basically a calculation we’ve already done several times. <strong>So we’re well into the memory-bound regime for both our matmul and our FLOPs.</strong></p> <p>Strictly looking at memory bandwidth then, our step time is basically <code class="language-plaintext highlighter-rouge">(KV size + param size) / (8 * HBM bandwidth) = 112e9 / (8 * 8.1e11) = 17ms</code>. <strong>So theoretically our step time is about 17ms.</strong> Our throughput would be <code class="language-plaintext highlighter-rouge">32 / .017 = 1882 tokens / sec</code>, or <code class="language-plaintext highlighter-rouge">1882 / 8 = 235 tokens / sec / chip</code>.</p> <p>There’s one caveat here which is to check if we might be ICI bound on our matmuls. We could dedicate 2 axes to it here, so we’re ICI bound in theory when $Y &gt; 2 * F / 2550 = 2 * 28672 / 2550 = 22$, so we’re golden!</p> <p>If we were to run on a <code class="language-plaintext highlighter-rouge">4x4</code>, we’d still be fine ICI-wise, so our latency would drop to <code class="language-plaintext highlighter-rouge">17 / 2 = 8.5ms</code>, but our throughput would remain the same.</p> </details> <h3 id="thinking-about-throughput">Thinking about throughput</h3> <p>Let’s spend a little time thinking purely about throughput. When we optimize for throughput, we want to be compute bound, meaning we come close to utilizing all the TPU MXU capacity. Typically that means we want the batch size to be as large as possible, so we are doing as much work as possible.</p> <p><strong>Question:</strong> On TPU v5e, using bfloat16 weights and activations, how large do our batch sizes need to be for us to be compute-bound in our matmuls? What if we do int8 weights but perform our FLOPs in bfloat16? What about int8 weights with int8 FLOPs?</p> <details><summary>Answer</summary> <p>As discussed in Section 7, for any bfloat16 matmul for which $B \ll D, F$ we have</p> \[\begin{equation*} T_\text{math} &gt; T_\text{comms} \leftrightarrow \frac{2BDF}{2DF} \geq \frac{\text{TPU bfloat16 FLOPs/s}}{\text{HBM bandwidth}} = 240 \end{equation*}\] <p>When our weights are in int8, we lose a factor of 2 in the denominator, so we have $2BDF / DF = 2B &gt; 240$, or equally $B &gt; 120$, half the critical batch size from before. That’s really helpful for us! When we do int8 weights and int8 FLOPs, we have to use the int8 value for TPU FLOPs/s, which goes from 1.97e14 for bfloat16 to 3.94e14, nearly double. That means we’re back where we started at about $B &gt; 240$.</p> <p>The case of int8 weights and bfloat16 FLOPs is quite common, since quantizing parameters losslessly is often easier than doing low-precision arithmetic. There, we have</p> </details> <p><strong>Question:</strong> What is the smallest TPU v5e topology we could serve LLaMA 3-70B on using bfloat16, int8, and int4 (both KVs and parameters) with 8k context? <em>You can think of KV caches as negligibly small for this one.</em></p> <details><summary>Answer</summary> <p>This is easy! If we’re OK with a tiny batch size then the only limit is fitting parameter memory in HBM, i.e. it is just <code class="language-plaintext highlighter-rouge">ceil(num_params * sizeof(dtype) / HBM per TPU</code>, or <code class="language-plaintext highlighter-rouge">ceil(70e9 * sizeof(dtype) / 16e9)</code> rounded to the nearest reasonable topology (some multiple of 2):</p> <table> <thead> <tr> <th style="text-align: center">dtype</th> <th style="text-align: center">param size</th> <th style="text-align: center">KV size / token (bytes)</th> <th style="text-align: center">min TPU v5es</th> <th style="text-align: center">actual min slice</th> <th style="text-align: center">remaining HBM for KV caches</th> <th style="text-align: center">num KV caches @ 8k</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">bf16</td> <td style="text-align: center">140GB</td> <td style="text-align: center">324kB</td> <td style="text-align: center">8.75</td> <td style="text-align: center">4x4 = 16 chips</td> <td style="text-align: center">116</td> <td style="text-align: center">43</td> </tr> <tr> <td style="text-align: center">int8</td> <td style="text-align: center">70GB</td> <td style="text-align: center">162kB</td> <td style="text-align: center">4.38</td> <td style="text-align: center">4x2 = 8 chips</td> <td style="text-align: center">68</td> <td style="text-align: center">52</td> </tr> <tr> <td style="text-align: center">int4</td> <td style="text-align: center">45GB</td> <td style="text-align: center">81kB</td> <td style="text-align: center">2.81</td> <td style="text-align: center">2x2 = 4 chips</td> <td style="text-align: center">19</td> <td style="text-align: center">67</td> </tr> </tbody> </table> <p>That’s pretty cool! It tells us we could fit LLaMA 70B on a TPU v5e 2x2 if we wanted to. Except you’ll notice the number of KV caches is very small. That’s our batch size! That means we’ll be getting terrible FLOPs utilization. We’d be very happy to use a larger topology in order to push our batch size up to 240.</p> </details> <p><strong>Question:</strong> Assume we use the largest batch size that fits on these topologies, what latency we could expect for each generate step?</p> <details><summary>Answer</summary> <p>This is also easy, since we’re picking our batch size to fill up all our HBM! This is just a question of how long it takes to load a full TPU v5e’s worth of bytes into the MXU. This is just <code class="language-plaintext highlighter-rouge">v5e HBM / v5e HBM memory bandwidth = 16GB / 8.2e11 = 19ms</code>, so this is <strong>19ms / step</strong>. Assuming our generations have a median length of 512 tokens, that is about 9s for each decode. Note that we could get marginally better latency with a smaller batch size, for instance if we only looked at model parameters in int4 our minimum latency is about 10ms / step, since HBM is no longer full.</p> </details> <p class="takeaway"><strong>Takeaway</strong>: we can always lower bound decode latency by asking how long it takes to load all the model’s parameters from HBM into the MXU. When our KV caches are small, you can think about each layer as just loading the weights chunk-by-chunk and then discarding them. Unless we’re using large batch sizes or lots of inter-device comms, this is often a reasonable bound (within 1.5x). When our batch size is bigger, we need to model the KV cache loading as well, since that dominates the parameters.</p> <p>Likewise, in the FLOPs-bound regime (e.g. training or big-batch inference), we can use the \(\text{Total FLOPs} / (N \cdot C) = 2 \cdot \text{param count} \cdot B / (N \cdot C)\) lower bound, which assumes no communication.</p> <p><strong>Question:</strong> For each of these, what throughput per chip does this give us (in terms of queries / chip)? <em>You can assume our median decode length is 512 tokens.</em></p> <details><summary>Answer</summary> <p>This is an important question because it’s exactly correlated with cost / token.</p> <p>With our assumption about median decode length, our throughput is just \(B / (\text{per-step latency} \cdot \text{median steps} \cdot N) \approxeq 43 / (0.019 * 512 * N)\). This gives us roughly \((4.42 / N)\) QPS, so plugging in \(N\) we get:</p> <table> <thead> <tr> <th style="text-align: center">dtype</th> <th style="text-align: center">QPS / chip</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">bfloat16</td> <td style="text-align: center">0.27</td> </tr> <tr> <td style="text-align: center">int8</td> <td style="text-align: center">0.66</td> </tr> <tr> <td style="text-align: center">int4</td> <td style="text-align: center">1.72</td> </tr> </tbody> </table> <p>Note that this is rather optimistic since it totally ignores the working memory of the forward pass (memory allocated to activations and attention). This is not ridiculous with Flash Attention, but it is also not realistic. The real numbers are likely maybe 1/2 of this. For absolutely maximum throughput we would probably want to more than double the number of chips and increase the batch size significantly as well.</p> </details> <p><strong>Question:</strong> How would our peak throughput change if we doubled our topology for each of the above examples?</p> <details><summary>Answer</summary> <p>If we used a 4x8 slice in bfloat16, we would have 186GB remaining for KV caches, which would let us up our batch size to 161. Then since our step time would remaining the same, we would have a throughput of <code class="language-plaintext highlighter-rouge">16.54 / num_chips</code>, or</p> <table> <thead> <tr> <th style="text-align: center">dtype</th> <th style="text-align: center">QPS / chip</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">bfloat16 (on 4x8)</td> <td style="text-align: center">0.51</td> </tr> <tr> <td style="text-align: center">int8 (on 4x4)</td> <td style="text-align: center">1.03</td> </tr> <tr> <td style="text-align: center">int4 (on 2x4)</td> <td style="text-align: center">2.06</td> </tr> </tbody> </table> <p>A further increase would give an even bigger win! The big takeaway is that <strong>the smallest topology is not the most performance topology</strong> in all cases, if we’re limited by KV cache size.</p> </details> <p><strong>Question:</strong> Now let’s dig into the question of sharding. Let’s say we wanted to serve in bfloat16 on a TPU v5e 4x8. What sharding would we use for our model on a TPU v5e 4x8 during generation? Can we avoid being communication bound?</p> <details><summary>Answer</summary> <p>As discussed in the previous section, we only really have one option for sharding during generation: model parallelism. How much can we do before we become communication bound? As we’ve discussed in the previous section, our models become communication bound roughly when</p> \[Y &gt; \frac{F \cdot n_\text{axes}}{2550}\] <p>For LLaMA 3-70B we have <code class="language-plaintext highlighter-rouge">F = 28,672</code>, so if we do 2 axes of model sharding this gives us roughly \(Y = 28672 \cdot 2 / 2550 = 22\), so in general we could scale up to about 16 chips without being communication bound, which lets us use a <code class="language-plaintext highlighter-rouge">4x4</code> but not a <code class="language-plaintext highlighter-rouge">4x8</code>. Generally, since we do not perfectly overlap computation, even this estimate is overly optimistic.</p> <p><strong>Takeaway: we cannot actually serve on a 4x8 with pure model parallelism.</strong> The best we can do here is a 4x2 or <em>maybe</em> a 4x4.</p> <p>However, as we’ve discussed, when our batch size is small we can often do more model parallelism without significantly hurting throughput, since our model is memory-bandwidth-bound and not FLOPs bound. We said before that this value is roughly $Y=F / (8\cdot B)$, so if we did batch size 64, we could in theory go up to <code class="language-plaintext highlighter-rouge">Y = 28,672 / (8 * 64) = 56</code> way model parallelism before we become ICI-bound. To sanity check this, we can look at $T_\text{ici comms}$, $T_\text{hbm comms}$, and $T_\text{math}$ for a single matmul. We clearly have:</p> \[\begin{align*}T_\text{ici comms} = \frac{2BD}{W_\text{ici}} &amp;&amp; T_\text{hbm comms} = \frac{2DF}{Y \cdot W_\text{hbm}} &amp;&amp; T_\text{math} = \frac{2BDF}{Y \cdot C}\end{align*}\] <p>For a <code class="language-plaintext highlighter-rouge">4x8</code>, this would give us $T_\text{ici comms}$ = <code class="language-plaintext highlighter-rouge">(2 * 64 * 8192) / 9e10 = 11us</code>, $T_\text{hbm comms}$ = <code class="language-plaintext highlighter-rouge">(2 * 8192 * 28,672) / (32 * 8.1e11) = 18us</code>, and $T_\text{math}$ = <code class="language-plaintext highlighter-rouge">(2 * 64 * 8192 * 28,672) / (32 * 1.97e14) = 4us</code>, so in theory we’re still HBM bandwidth bound, which is great! *Note that scaling up from a <code class="language-plaintext highlighter-rouge">4x4</code> to a <code class="language-plaintext highlighter-rouge">4x8</code> probably isn’t helpful from a throughput standpoint, but it’ll reduce our latency!</p> <p>If we look at the int8 and int4 configs, we <em>can</em> do those with pure model parallelism. So we’ve hit a point at which quantization actually gives us a meaningful advantage beyond faster FLOPs: it lets us use a larger batch size before we become comms-bound. <em>*So the end of this story is that we can’t achieve peak throughput on a 4x8, but for the int8 and int4 configs we could do pure model parallelism</em>.</p> </details> <p class="takeaway"><strong>Tip</strong>: the maximum amount of useful model parallelism depends on \(d_{ff}\) and the number of axes over which you’re sharding your model. The maximum value usually ranges between 8 and 32 depending on the model size. You can scale beyond this limit to improve latency at some throughput cost.</p> <h3 id="what-about-prefill">What about prefill?</h3> <p>We’ve mostly ignored prefill here because it’s much simpler. Let’s put a couple of concepts together and think about the end-to-end picture.</p> <p><strong>Question:</strong> Assume we achieve a 40% FLOPs utilization during prefill. How long will a prefill of length 8192 take on 16 TPU v5e chips?</p> <details><summary>Answer</summary> <p>At 8k tokens, we are solidly compute bound, so we just need to reason about FLOPs. We know our model has <code class="language-plaintext highlighter-rouge">70e9</code> parameters so each forward pass uses <code class="language-plaintext highlighter-rouge">2 * 70e9 * B</code> FLOPs. Assuming 40% MFU (FLOPs utilization), this gives us a runtime of about <code class="language-plaintext highlighter-rouge">2 * 70e9 * 8192 / (16 * 1.97e14 * 0.4) = 0.91s</code>. Compared to the numbers we’ve been looking at before, that’s actually quite a lot!</p> </details> <p><strong>Question:</strong> Assume we have a median prefill length of 8192 tokens and a median decode length of 4096 tokens. Say we have a generate batch size of 32. On average how many sequences finish decoding per step? On average how many tokens are evicted from our KV cache each step?</p> <details><summary>Answer</summary> <p>This is kind of straightforward. Since we have a median decode length of 4096 tokens, a sequence will finish roughly every 1 / 4096 tokens. Given a batch size of 32, this means we have <code class="language-plaintext highlighter-rouge">32 / 4096</code> sequences evicted per step. Since our KV cache length is roughly <code class="language-plaintext highlighter-rouge">8192 + 4096</code>, this is <code class="language-plaintext highlighter-rouge">32 * (8192 + 4096) / 4096 = 96</code> tokens evicted per step. The general formula is $B * (P + G) / G$ where $P$ and $G$ are the prefill and generate lengths.</p> </details> <p><strong>Question:</strong> Assume we do disaggregated serving with a median prefill length of 8192 and a median decode length of 512. Assume the prefill and generate latencies calculated above in bfloat16. What ratio of prefill:generate servers will you need to keep both fully saturated.</p> <details><summary>Answer</summary> <p>This is kind of a fun question. Let $P$ be the number of prefill servers and $G$ be the number of generate servers. So generally speaking, this is a pipeline problem where we feed sequences in at a rate of <code class="language-plaintext highlighter-rouge">P / prefill_latency</code> and consume them at a rate of <code class="language-plaintext highlighter-rouge">B * G / (generate_latency * median_decode_length)</code>. We had calculated <code class="language-plaintext highlighter-rouge">910ms</code> per prefill step and <code class="language-plaintext highlighter-rouge">19ms</code> per decode step at batch size 43 (let’s call that 32). Therefore we need <code class="language-plaintext highlighter-rouge">P / 0.91 = 32 * G / (0.019 * 512)</code> or <code class="language-plaintext highlighter-rouge">P = 3G</code>, i.e. we need about 3 times more prefill servers than generation servers!</p> </details> <h2 id="visualizing-the-latency-throughput-tradeoff">Visualizing the Latency Throughput Tradeoff</h2> <p>Sticking with LLaMA 70B for a second, let’s actually look at the latency and throughput for different batch sizes during generation. As we showed in the previous section for PaLM models, this gives us a Pareto frontier for throughput/latency. Let’s assume 16-way tensor parallelism since that’s a reasonable bound on what we can use while staying compute-bound in the MLP blocks. We’ll use a TPU v5e 4x4 topology here. <strong>The slider controls the sequence length so you can see the effect of larger KV caches.</strong></p> <div class="l-page"> <iframe src="/scaling-book/assets/plotly/pareto.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> </div> <ul> <li> <strong>See how dramatic the trandeoff is between cost and latency.</strong> At the cost of doubling per-token latency, we can achieve a roughly 100x reduction in per-token cost. Also, our latency can range anywhere from 5.5ms with low batch size to 20 ms with very large batches.</li> <li>Note how at 2k context the throughput effectively plateaus at around 1 token / ms / chip when it hits the BS 120 roofline (120 here because we do int8 weights but bf16 FLOPs). As the sequence length increases, however, we can no longer fit this batch size in memory, so we never hit the point of full saturation.</li> <li>Note how much higher the latency is at large batch sizes for the same throughput, since KV loading becomes dominant (instead of parameter loading).</li> </ul> <p>We can understand this better by breaking down the sources of cost and latency into param loading time, KV loading time, and FLOPs time. The red sector is the region in which we expect to be compute-bound in our MLP blocks.</p> <div class="l-page"> <iframe src="/scaling-book/assets/plotly/latency_breakdown_log.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> </div> <p>This tells quite a story. You can see that initially, parameter loading represents the vast majority of the latency, until the batch size becomes large enough that FLOPs and KV loading become more significant. Notably, at all sequence lengths greater than 2048, we spend more time on KV cache loading than we do on FLOPs! <strong>So while we can improve our hardware utilization by increasing batch size, at long context lengths KV loading always dominates the total step time.</strong></p> <p class="takeaway"><strong>Takeaway:</strong> for LLaMA 3-70B, we are strongly KV cache memory bandwidth-bound (and HBM-bound) in almost all of these configurations, highlighting just how important reducing KV cache size is for generation throughput. Also note just how dramatic the latency/throughput tradeoff remains here.</p> <details><summary>The code for this is quite simple.</summary> <p>Here’s the code for computing these rooflines:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">num_chips</span> <span class="o">=</span> <span class="mi">16</span>  <span class="c1"># we fix 16 as the amount of total model parallelism we do
</span><span class="n">param_size</span> <span class="o">=</span> <span class="mf">70e9</span>  <span class="c1"># int8 means 1 byte per param
</span><span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">8192</span>  <span class="c1"># can vary this
</span>
<span class="n">hbm_bandwidth</span> <span class="o">=</span> <span class="mf">8.20E+11</span>  <span class="c1"># v5e
</span><span class="n">flops</span> <span class="o">=</span> <span class="mf">1.97E+14</span>  <span class="c1"># v5e
</span>
<span class="n">param_size</span> <span class="o">=</span> <span class="n">bytes_per_param</span> <span class="o">*</span> <span class="n">param_count</span>

<span class="k">def</span> <span class="nf">kv_cache_size</span><span class="p">(</span><span class="n">bs</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">bs</span> <span class="o">*</span> <span class="mi">128</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">*</span> <span class="mi">80</span>
    
<span class="k">def</span> <span class="nf">min_topology</span><span class="p">(</span><span class="nb">bytes</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">np</span><span class="p">.</span><span class="nf">ceil</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">log2</span><span class="p">(</span><span class="nb">bytes</span> <span class="o">/</span> <span class="mf">16e9</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">get_max_batch_size</span><span class="p">(</span><span class="n">max_num_chips</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">):</span>
  <span class="c1"># for num_chips in topo_sizes:
</span>  <span class="n">batch_sizes</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
  <span class="n">kv_sizes</span> <span class="o">=</span> <span class="nf">kv_cache_size</span><span class="p">(</span><span class="n">sequence_length</span> <span class="o">*</span> <span class="n">batch_sizes</span><span class="p">)</span>
  <span class="n">num_chips</span> <span class="o">=</span> <span class="nf">min_topology</span><span class="p">(</span><span class="n">kv_sizes</span> <span class="o">+</span> <span class="n">param_size</span><span class="p">)</span>
  <span class="n">max_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">num_chips</span> <span class="o">&lt;=</span> <span class="n">max_num_chips</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">max_idx</span>

<span class="n">max_idx</span> <span class="o">=</span> <span class="nf">get_max_batch_size</span><span class="p">(</span><span class="n">num_chips</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">param_size</span><span class="p">)</span>  <span class="c1"># get the largest batch size that can fit
</span><span class="n">batch_sizes</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[:</span><span class="n">max_idx</span><span class="p">]</span>
<span class="n">kv_sizes</span> <span class="o">=</span> <span class="nf">kv_cache_size</span><span class="p">(</span><span class="n">sequence_length</span> <span class="o">*</span> <span class="n">batch_sizes</span><span class="p">)</span>

<span class="n">kv_comms_time</span> <span class="o">=</span> <span class="n">kv_sizes</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_chips</span> <span class="o">*</span> <span class="n">hbm_bandwidth</span><span class="p">)</span>

<span class="n">param_comms_time</span> <span class="o">=</span> <span class="n">param_size</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_chips</span> <span class="o">*</span> <span class="n">hbm_bandwidth</span><span class="p">)</span>
<span class="n">param_comms_time</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">([</span><span class="n">param_comms_time</span><span class="p">]</span> <span class="o">*</span> <span class="n">batch_sizes</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">flops_time</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">param_count</span> <span class="o">*</span> <span class="n">batch_sizes</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_chips</span> <span class="o">*</span> <span class="n">flops</span><span class="p">)</span>  <span class="c1"># roughly true in a 2ND sense
</span>
<span class="n">mlp_time</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">flops_time</span><span class="p">,</span> <span class="n">param_comms_time</span><span class="p">)</span>
<span class="n">attn_time</span> <span class="o">=</span> <span class="n">kv_comms_time</span>  <span class="c1"># always bandwidth-bound for generate
</span>
<span class="n">latency</span> <span class="o">=</span> <span class="mi">1000</span> <span class="o">*</span> <span class="p">(</span><span class="n">mlp_time</span> <span class="o">+</span> <span class="n">attn_time</span><span class="p">)</span>
<span class="n">throughput</span> <span class="o">=</span> <span class="n">batch_sizes</span> <span class="o">/</span> <span class="p">(</span><span class="n">latency</span> <span class="o">*</span> <span class="n">num_chips</span><span class="p">)</span>
</code></pre></div></div> <p>Note how we very explicitly break out latency into two sources: KV loading and param loading, and how the latency is either bound by FLOPs or comms, whichever is bigger.</p> </details> <h2 id="worked-problems">Worked Problems</h2> <p>Here are a few worked problems. Some of these repeat things that are worked above, but might be pedagogically useful.</p> <p><strong>Question 1:</strong> How many FLOPs does each forward pass for LLaMA 3-405B use per-token? Assuming we’re FLOPs bound, what is a lower bound on a single forward pass on N chips on TPU v5e? What if we’re comms bound? <em>Ignore the fact that the model does not fit on a single chip.</em></p> <p><strong>Question 2:</strong> Assume we want to serve LLaMA 3-8B with BS240 using int8 weights and int8 KV caches. How many bytes are used by (a) model parameters (b) KV caches and (c) peak working activations (roughly)? What’s the smallest topology we can run this on?</p> <p><strong>Question 3:</strong> How would you serve LLaMA 3-405B on TPU v5e? Assume int8 weights and bfloat16 FLOPs. Let’s say we have a firm limit of 15ms / token, what’s the highest throughput configuration we could achieve? What is the theoretical minimum step time?</p> <h3 class="next-section">That’s all for Part 8! For Part 9, with a deep dive into XLA and TPU profiling, click <a href="../profiling">here</a>.</h3> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> <div class="base-grid appendix-entry"> <h3 style="grid-column: 0;">Miscellaneous</h3> <p class="author-footnote" style="grid-column: text;"><sup>*</sup>Work done at Google DeepMind, now at MatX.</p> </div> <div class="base-grid appendix-entry"> <h3 style="grid-column: 0;">Citation</h3> <p class="author-footnote">For attribution in academic contexts, please cite this work as:</p> <div class="author-footnote"> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c">Austin et al., "How to Scale Your Model", Google DeepMind, online, 2025.</span>
</code></pre></div></div> </div> <p class="author-footnote">or as a BibTeX entry:</p> <div class="author-footnote"> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="nc">@article</span><span class="p">{</span><span class="nl">scaling-book</span><span class="p">,</span>
      <span class="na">title</span> <span class="p">=</span> <span class="s">{How to Scale Your Model}</span><span class="p">,</span>
      <span class="na">author</span> <span class="p">=</span> <span class="s">{Austin, Jacob and Douglas, Sholto and Frostig, Roy and Levskaya, Anselm and Chen, Charlie and Vikram, Sharad
      and Lebron, Federico and Choy, Peter and Ramasesh, Vinay and Webson, Albert and Pope, Reiner}</span><span class="p">,</span>
      <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Google DeepMind}</span><span class="p">,</span>
      <span class="na">howpublished</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
      <span class="na">note</span> <span class="p">=</span> <span class="s">{Retrieved from https://jax-ml.github.io/scaling-book/}</span><span class="p">,</span>
      <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
    <span class="p">}</span>
</code></pre></div></div> </div> </div> </d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'jax-ml/scaling-book',
        'data-repo-id': '',
        'data-category': 'General',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '0',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-loading': '1',
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/scaling-book/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/scaling-book/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/scaling-book/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/scaling-book/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/scaling-book/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/scaling-book/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/scaling-book/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/scaling-book/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/scaling-book/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>